{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bdf3686",
   "metadata": {},
   "source": [
    "## Pip Install Requirment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee43e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting backoff\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: backoff\n",
      "Successfully installed backoff-2.2.1\n"
     ]
    }
   ],
   "source": [
    "# Requirments:\n",
    "# pip install requests\n",
    "!pip install torch\n",
    "!pip install sacrebleu\n",
    "!pip install accelerate\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install scipy torchvision\n",
    "!pip install peft\n",
    "!pip install backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487ecb3e",
   "metadata": {},
   "source": [
    "## Load Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca5eeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "finetune Phi-4-multimodal-instruct on an speech task\n",
    "\n",
    "scipy==1.15.1\n",
    "peft==0.13.2\n",
    "backoff==2.2.1\n",
    "transformers==4.46.1\n",
    "accelerate==1.3.0\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import sacrebleu\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    BatchFeature,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd604c3",
   "metadata": {},
   "source": [
    "## Original from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40b495cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelechi/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.43it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.29 GiB. GPU 0 has a total capacity of 23.65 GiB of which 2.02 GiB is free. Including non-PyTorch memory, this process has 21.62 GiB memory in use. Of the allocated memory 20.82 GiB is allocated by PyTorch, and 382.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 438\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLEU Score after finetuning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 438\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 314\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39mlocal_main_process_first():\n\u001b[1;32m    310\u001b[0m     processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    311\u001b[0m         args\u001b[38;5;241m.\u001b[39mmodel_name_or_path,\n\u001b[1;32m    312\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    313\u001b[0m     )\n\u001b[0;32m--> 314\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_flash_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_flash_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m model\u001b[38;5;241m.\u001b[39mset_lora_adapter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeech\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    322\u001b[0m rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRANK\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "Cell \u001b[0;32mIn[3], line 190\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name_or_path, use_flash_attention)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_model\u001b[39m(model_name_or_path, use_flash_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 190\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_flash_attention\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_attn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflash_attention_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_flash_attention\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msdpa\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/transformers/modeling_utils.py:3698\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3694\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3695\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3696\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3697\u001b[0m         )\n\u001b[0;32m-> 3698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.29 GiB. GPU 0 has a total capacity of 23.65 GiB of which 2.02 GiB is free. Including non-PyTorch memory, this process has 21.62 GiB memory in use. Of the allocated memory 20.82 GiB is allocated by PyTorch, and 382.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "INSTSRUCTION = {\n",
    "    \"en_zh-CN\": \"Translate the audio to Mandarin.\",\n",
    "    \"en_id\": \"Translate the audio to Indonesian.\",\n",
    "    \"en_sl\": \"Translate the audio to Slovenian.\",\n",
    "}\n",
    "TOKENIZER = {\n",
    "    \"en_zh-CN\": \"zh\",\n",
    "    \"en_ja\": \"ja-mecab\",\n",
    "}\n",
    "ANSWER_SUFFIX = \"<|end|><|endoftext|>\"\n",
    "_IGNORE_INDEX = -100\n",
    "_TRAIN_SIZE = 50000\n",
    "_EVAL_SIZE = 200\n",
    "\n",
    "class MultipleTokenBatchStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"Stopping criteria capable of receiving multiple stop-tokens and handling batched inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, stop_tokens: torch.LongTensor, batch_size: int = 1) -> None:\n",
    "        \"\"\"Initialize the multiple token batch stopping criteria.\n",
    "\n",
    "        Args:\n",
    "            stop_tokens: Stop-tokens.\n",
    "            batch_size: Batch size.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.stop_tokens = stop_tokens\n",
    "        self.max_stop_tokens = stop_tokens.shape[-1]\n",
    "        self.stop_tokens_idx = torch.zeros(batch_size, dtype=torch.long, device=stop_tokens.device)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # Only gather the maximum number of inputs compatible with stop tokens\n",
    "        # and checks whether generated inputs are equal to `stop_tokens`\n",
    "        generated_inputs = torch.eq(input_ids[:, -self.max_stop_tokens :].unsqueeze(1), self.stop_tokens)\n",
    "        equal_generated_inputs = torch.all(generated_inputs, dim=2)\n",
    "\n",
    "        # Mark the position where a stop token has been produced for each input in the batch,\n",
    "        # but only if the corresponding entry is not already set\n",
    "        sequence_idx = torch.any(equal_generated_inputs, dim=1)\n",
    "        sequence_set_mask = self.stop_tokens_idx == 0\n",
    "        self.stop_tokens_idx[sequence_idx & sequence_set_mask] = input_ids.shape[-1]\n",
    "\n",
    "        return torch.all(self.stop_tokens_idx)\n",
    "\n",
    "class CoVoSTDataset(Dataset):\n",
    "    def __init__(self, processor, data_dir, split, \n",
    "                 lang=\"en_zh-CN\", rank=0, world_size=1):\n",
    "\n",
    "        # Automatically download the dataset from Hugging Face\n",
    "        self.data = load_dataset(\n",
    "            \"facebook/covost2\",  # CoVoST2 dataset\n",
    "            lang,                # e.g., 'en_zh-CN'\n",
    "            data_dir=data_dir, \n",
    "            split=split,\n",
    "            trust_remote_code=True  # Make sure to trust the code from the repository\n",
    "        )\n",
    "        \n",
    "        self.training = \"train\" in split\n",
    "        self.processor = processor\n",
    "        self.instruction = INSTSRUCTION[lang]\n",
    "        \n",
    "        # For distributed training, shard the dataset if needed\n",
    "        if world_size > 1:\n",
    "            self.data = self.data.shard(world_size, rank) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        user_message = {\n",
    "            'role': 'user',\n",
    "            'content': '<|audio_1|>\\n' + self.instruction,\n",
    "        }\n",
    "        prompt = self.processor.tokenizer.apply_chat_template(\n",
    "            [user_message], tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = self.processor(text=prompt, audios=[(data[\"audio\"][\"array\"], data[\"audio\"][\"sampling_rate\"])], return_tensors='pt')\n",
    "        \n",
    "        answer = f\"{data['translation']}{ANSWER_SUFFIX}\"\n",
    "        answer_ids = self.processor.tokenizer(answer, return_tensors='pt').input_ids\n",
    "        if  self.training:\n",
    "            input_ids = torch.cat([inputs.input_ids, answer_ids], dim=1)\n",
    "            labels = torch.full_like(input_ids, _IGNORE_INDEX)\n",
    "            labels[:, -answer_ids.shape[1] :] = answer_ids\n",
    "        else:\n",
    "            input_ids = inputs.input_ids\n",
    "            labels = answer_ids\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'input_audio_embeds': inputs.input_audio_embeds,\n",
    "            'audio_embed_sizes': inputs.audio_embed_sizes,\n",
    "        }\n",
    "\n",
    "def pad_sequence(sequences, padding_side='right', padding_value=0):\n",
    "    \"\"\"\n",
    "    Pad a list of sequences to the same length.\n",
    "    sequences: list of tensors in [seq_len, *] shape\n",
    "    \"\"\"\n",
    "    assert padding_side in ['right', 'left']\n",
    "    max_size = sequences[0].size()\n",
    "    trailing_dims = max_size[1:]\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    batch_size = len(sequences)\n",
    "    output = sequences[0].new_full((batch_size, max_len) + trailing_dims, padding_value)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = seq.size(0)\n",
    "        if padding_side == 'right':\n",
    "            output.data[i, :length] = seq\n",
    "        else:\n",
    "            output.data[i, -length:] = seq\n",
    "    return output\n",
    "\n",
    "\n",
    "def cat_with_pad(tensors, dim, padding_value=0):\n",
    "    \"\"\"\n",
    "    cat along dim, while pad to max for all other dims\n",
    "    \"\"\"\n",
    "    ndim = tensors[0].dim()\n",
    "    assert all(\n",
    "        t.dim() == ndim for t in tensors[1:]\n",
    "    ), 'All tensors must have the same number of dimensions'\n",
    "\n",
    "    out_size = [max(t.shape[i] for t in tensors) for i in range(ndim)]\n",
    "    out_size[dim] = sum(t.shape[dim] for t in tensors)\n",
    "    output = tensors[0].new_full(out_size, padding_value)\n",
    "\n",
    "    index = 0\n",
    "    for t in tensors:\n",
    "        # Create a slice list where every dimension except dim is full slice\n",
    "        slices = [slice(0, t.shape[d]) for d in range(ndim)]\n",
    "        # Update only the concat dimension slice\n",
    "        slices[dim] = slice(index, index + t.shape[dim])\n",
    "\n",
    "        output[slices] = t\n",
    "        index += t.shape[dim]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def covost_collate_fn(batch):\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    input_audio_embeds_list = []\n",
    "    audio_embed_sizes_list = []\n",
    "    audio_attention_mask_list = []\n",
    "    for inputs in batch:\n",
    "        input_ids_list.append(inputs['input_ids'][0])\n",
    "        labels_list.append(inputs['labels'][0])\n",
    "        input_audio_embeds_list.append(inputs['input_audio_embeds'])\n",
    "        audio_embed_sizes_list.append(inputs['audio_embed_sizes'])\n",
    "        audio_attention_mask_list.append(\n",
    "            inputs['input_audio_embeds'].new_full((inputs['input_audio_embeds'].size(1),), True, dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        input_ids = pad_sequence(input_ids_list, padding_side='left', padding_value=0)\n",
    "        labels = pad_sequence(labels_list, padding_side='left', padding_value=0)\n",
    "        audio_attention_mask = (\n",
    "            pad_sequence(audio_attention_mask_list, padding_side='right', padding_value=False)\n",
    "            if len(audio_attention_mask_list) > 1\n",
    "            else None\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(input_ids_list)\n",
    "        print(labels_list)\n",
    "        raise\n",
    "    attention_mask = (input_ids != 0).long()\n",
    "    input_audio_embeds = cat_with_pad(input_audio_embeds_list, dim=0)\n",
    "    audio_embed_sizes = torch.cat(audio_embed_sizes_list)\n",
    "\n",
    "    return BatchFeature(\n",
    "        {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "            'input_audio_embeds': input_audio_embeds,\n",
    "            'audio_embed_sizes': audio_embed_sizes,\n",
    "            'audio_attention_mask': audio_attention_mask,\n",
    "            'input_mode': 2,  # speech mode\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def create_model(model_name_or_path, use_flash_attention=False):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        torch_dtype=torch.bfloat16 if use_flash_attention else torch.float32,\n",
    "        _attn_implementation='flash_attention_2' if use_flash_attention else 'sdpa',\n",
    "        trust_remote_code=True,\n",
    "    ).to('cuda')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model, processor, eval_dataset, save_path=None, disable_tqdm=False, eval_batch_size=1\n",
    "):\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "\n",
    "    model.eval()\n",
    "    all_generated_texts = []\n",
    "    all_labels = []\n",
    "\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=eval_batch_size,\n",
    "        collate_fn=covost_collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=8,\n",
    "        prefetch_factor=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    stop_tokens = [\"<|end|>\", processor.tokenizer.eos_token]\n",
    "    stop_tokens_ids = processor.tokenizer(stop_tokens, add_special_tokens=False, padding=\"longest\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "    stop_tokens_ids = stop_tokens_ids.to(f'cuda:{local_rank}')\n",
    "\n",
    "    for inputs in tqdm(\n",
    "        eval_dataloader, disable=(rank != 0) or disable_tqdm, desc='running eval'\n",
    "    ):\n",
    "        stopping_criteria=StoppingCriteriaList([MultipleTokenBatchStoppingCriteria(stop_tokens_ids, batch_size=inputs.input_ids.size(0))])\n",
    "        inputs = inputs.to(f'cuda:{local_rank}')\n",
    "        generated_ids = model.generate(\n",
    "            **inputs, eos_token_id=processor.tokenizer.eos_token_id, max_new_tokens=64,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "        )\n",
    "\n",
    "        stop_tokens_idx = stopping_criteria[0].stop_tokens_idx.reshape(inputs.input_ids.size(0), -1)[:, 0]\n",
    "\n",
    "        stop_tokens_idx = torch.where(\n",
    "            stop_tokens_idx > 0,\n",
    "            stop_tokens_idx - stop_tokens_ids.shape[-1],\n",
    "            generated_ids.shape[-1],\n",
    "        )\n",
    "        generated_text = [\n",
    "            processor.decode(_pred_ids[inputs[\"input_ids\"].shape[1] : _stop_tokens_idx], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "            for _pred_ids, _stop_tokens_idx in zip(generated_ids, stop_tokens_idx)\n",
    "        ]\n",
    "        all_generated_texts.extend(generated_text)\n",
    "        labels = [processor.decode(_label_ids[_label_ids != 0]).removesuffix(ANSWER_SUFFIX) for _label_ids in inputs[\"labels\"]]\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    all_generated_texts = gather_object(all_generated_texts)\n",
    "    all_labels = gather_object(all_labels)\n",
    "    \n",
    "    if rank == 0:\n",
    "        assert len(all_generated_texts) == len(all_labels)\n",
    "        bleu = sacrebleu.corpus_bleu(all_generated_texts, [all_labels])\n",
    "        print(bleu)\n",
    "        if save_path:\n",
    "            with open(save_path, 'w') as f:\n",
    "                save_dict = {\n",
    "                    'all_generated_texts': all_generated_texts,\n",
    "                    'all_labels': all_labels,\n",
    "                    'score': bleu.score,\n",
    "                }\n",
    "                json.dump(save_dict, f)\n",
    "\n",
    "        return bleu.score\n",
    "    return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--model_name_or_path',\n",
    "        type=str,\n",
    "        default='microsoft/Phi-4-multimodal-instruct',\n",
    "        help='Model name or path to load from',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--common_voice_dir\",\n",
    "        type=str,\n",
    "        default=\"CommonVoice/EN\",\n",
    "        help=\"Unzipped Common Voice Audio dataset directory, refer to https://commonvoice.mozilla.org/en/datasets, version 4.0\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lang\",\n",
    "        type=str,\n",
    "        default=\"en_sl\",\n",
    "        help=\"Language pair for translation.\",\n",
    "    )\n",
    "    parser.add_argument('--use_flash_attention', action='store_true', help='Use Flash Attention')\n",
    "    parser.add_argument('--output_dir', type=str, default='./output/', help='Output directory')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')\n",
    "    parser.add_argument(\n",
    "        '--batch_size_per_gpu',\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help='Batch size per GPU (adjust this to fit in GPU memory)',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_train_epochs', type=int, default=1, help='Number of training epochs'\n",
    "    )\n",
    "    parser.add_argument('--learning_rate', type=float, default=4.0e-5, help='Learning rate')\n",
    "    parser.add_argument('--wd', type=float, default=0.01, help='Weight decay')\n",
    "    parser.add_argument('--no-tqdm', dest='tqdm', action='store_false', help='Disable tqdm')\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    with accelerator.local_main_process_first():\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model = create_model(\n",
    "            args.model_name_or_path,\n",
    "            use_flash_attention=args.use_flash_attention,\n",
    "        )\n",
    "\n",
    "    model.set_lora_adapter('speech')\n",
    "\n",
    "\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
    "\n",
    "    eval_dataset = CoVoSTDataset(processor,\n",
    "                                 data_dir=args.common_voice_dir,\n",
    "                                 split=f'test[:{_EVAL_SIZE}]',\n",
    "                                 lang=args.lang,\n",
    "                                 rank=rank,\n",
    "                                 world_size=world_size)\n",
    "    \n",
    "    train_dataset = CoVoSTDataset(processor,\n",
    "                                  data_dir=args.common_voice_dir,\n",
    "                                  split=f'train[:{_TRAIN_SIZE}]',\n",
    "                                  lang=args.lang)\n",
    "\n",
    "    num_gpus = accelerator.num_processes\n",
    "    print(f'training on {num_gpus} GPUs')\n",
    "    assert (\n",
    "        args.batch_size % (num_gpus * args.batch_size_per_gpu) == 0\n",
    "    ), 'Batch size must be divisible by the number of GPUs'\n",
    "    gradient_accumulation_steps = args.batch_size // (num_gpus * args.batch_size_per_gpu)\n",
    "\n",
    "    if args.use_flash_attention:\n",
    "        fp16 = False\n",
    "        bf16 = True\n",
    "    else:\n",
    "        fp16 = True\n",
    "        bf16 = False\n",
    "\n",
    "    # hard coded training args\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        per_device_train_batch_size=args.batch_size_per_gpu,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        optim='adamw_torch',\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.95,\n",
    "        adam_epsilon=1e-7,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.wd,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type='linear',\n",
    "        warmup_steps=50,\n",
    "        logging_steps=10,\n",
    "        output_dir=args.output_dir,\n",
    "        save_strategy='no',\n",
    "        save_total_limit=10,\n",
    "        save_only_model=True,\n",
    "        bf16=bf16,\n",
    "        fp16=fp16,\n",
    "        remove_unused_columns=False,\n",
    "        report_to='none',\n",
    "        deepspeed=None,\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        dataloader_num_workers=4,\n",
    "        ddp_find_unused_parameters=True,  # for unused SigLIP layers\n",
    "    )\n",
    "\n",
    "    # eval before fine-tuning\n",
    "    out_path = Path(training_args.output_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        save_path=out_path / 'eval_before.json',\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        eval_batch_size=args.batch_size_per_gpu,\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        print(f'BLEU Score before finetuning: {score}')\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=covost_collate_fn,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    if accelerator.is_main_process:\n",
    "        processor.save_pretrained(training_args.output_dir)\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    # eval after fine-tuning (load saved checkpoint)\n",
    "    # first try to clear GPU memory\n",
    "    del model\n",
    "    del trainer\n",
    "    __import__('gc').collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # reload the model for inference\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        training_args.output_dir,\n",
    "        torch_dtype=torch.bfloat16 if args.use_flash_attention else torch.float32,\n",
    "        trust_remote_code=True,\n",
    "        _attn_implementation='flash_attention_2' if args.use_flash_attention else 'sdpa',\n",
    "    ).to('cuda')\n",
    "\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        save_path=out_path / 'eval_after.json',\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        eval_batch_size=args.batch_size_per_gpu,\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        print(f'BLEU Score after finetuning: {score}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a3cb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading dataset: __init__() got an unexpected keyword argument 'lang'\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def test_huggingface_dataset_access():\n",
    "    try:\n",
    "        # Try to load the CoVoST2 dataset with a specific language pair (en_zh-CN)\n",
    "        dataset = load_dataset(\"facebook/covost2\", data_dir=\"CommonVoice/EN\", lang=\"en_zh-CN\", split=\"train[:10]\", trust_remote_code=True)\n",
    "        # Print the first example in the training split to verify access\n",
    "        print(\"Successfully loaded the dataset!\")\n",
    "        print(f\"First example: {dataset[0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "if __name__ == \"__main__\":\n",
    "    test_huggingface_dataset_access()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5e2b2a",
   "metadata": {},
   "source": [
    "## Modified Original Pipeline Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download tar file to the current directory\n",
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "import shutil\n",
    "def download_and_extract_tar(url, extract_to='.'):\n",
    "    # Download the tar file\n",
    "    response = requests.get(url, stream=True)\n",
    "    tar_file_path = 'dataset.tar.gz'\n",
    "    \n",
    "    with open(tar_file_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    # Extract the tar file\n",
    "    with tarfile.open(tar_file_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=extract_to)\n",
    "\n",
    "    # Remove the tar file after extraction\n",
    "    os.remove(tar_file_path)\n",
    "# Example usage\n",
    "#url = 'https://storage.googleapis.com/common-voice-prod-prod-datasets/cv-corpus-1/en.tar.gz?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gke-prod%40moz-fx-common-voice-prod.iam.gserviceaccount.com%2F20250423%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250423T151115Z&X-Goog-Expires=43200&X-Goog-SignedHeaders=host&X-Goog-Signature=a0a8c9465f6d3f72bdd9febcf8e47fbe7f286685e76a10126a1394c3e4e881d621931fc6c6fd2b482f71d2f17792c0d209841531db972aac2e6158fbee5a6e119e3d9d95a4a7f08bb5fd207647112ed202fd79f4e82b1ef5d60e565189678e9a50e9acbbc2d41db4bb5c1aa53eaccfa1134d2e2024be37c52b8f36b7d13332915d285d3ff9d246a8c8a6772f6284ebac05bdd4d1d691df8f9a8c843604a28a00fffd556f4373f96d29bbae007a086740f5d442e4d7b5f6991fe4589dd0a69e97e38133c90adf63ca114323a388fa081ded336a1922181f17f8252294f3419bb0c7efec202867bbdc882ab490cb3a150235c24b88dfa942d2ec5e7de014246e6a'\n",
    "url = 'https://huggingface.co/datasets/facebook/covost2/resolve/main/en_zh-CN.tar.gz'\n",
    "\n",
    "download_and_extract_tar(url, extract_to='CommonVoice/EN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eba66ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/kelechi/bio_ramp_project\n"
     ]
    }
   ],
   "source": [
    "# print the current working directory\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5650c94",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StoppingCriteria' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m _TRAIN_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50000\u001b[39m\n\u001b[1;32m     13\u001b[0m _EVAL_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMultipleTokenBatchStoppingCriteria\u001b[39;00m(\u001b[43mStoppingCriteria\u001b[49m):\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Stopping criteria capable of receiving multiple stop-tokens and handling batched inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, stop_tokens: torch\u001b[38;5;241m.\u001b[39mLongTensor, batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StoppingCriteria' is not defined"
     ]
    }
   ],
   "source": [
    "INSTSRUCTION = {\n",
    "    \"en_zh-CN\": \"Translate the audio to Mandarin.\",\n",
    "    \"en_id\": \"Translate the audio to Indonesian.\",\n",
    "    \"en_sl\": \"Translate the audio to Slovenian.\",\n",
    "}\n",
    "TOKENIZER = {\n",
    "    \"en_zh-CN\": \"zh\",\n",
    "    \"en_ja\": \"ja-mecab\",\n",
    "}\n",
    "ANSWER_SUFFIX = \"<|end|><|endoftext|>\"\n",
    "_IGNORE_INDEX = -100\n",
    "_TRAIN_SIZE = 50000\n",
    "_EVAL_SIZE = 200\n",
    "\n",
    "class MultipleTokenBatchStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"Stopping criteria capable of receiving multiple stop-tokens and handling batched inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, stop_tokens: torch.LongTensor, batch_size: int = 1) -> None:\n",
    "        \"\"\"Initialize the multiple token batch stopping criteria.\n",
    "\n",
    "        Args:\n",
    "            stop_tokens: Stop-tokens.\n",
    "            batch_size: Batch size.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.stop_tokens = stop_tokens\n",
    "        self.max_stop_tokens = stop_tokens.shape[-1]\n",
    "        self.stop_tokens_idx = torch.zeros(batch_size, dtype=torch.long, device=stop_tokens.device)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # Only gather the maximum number of inputs compatible with stop tokens\n",
    "        # and checks whether generated inputs are equal to `stop_tokens`\n",
    "        generated_inputs = torch.eq(input_ids[:, -self.max_stop_tokens :].unsqueeze(1), self.stop_tokens)\n",
    "        equal_generated_inputs = torch.all(generated_inputs, dim=2)\n",
    "\n",
    "        # Mark the position where a stop token has been produced for each input in the batch,\n",
    "        # but only if the corresponding entry is not already set\n",
    "        sequence_idx = torch.any(equal_generated_inputs, dim=1)\n",
    "        sequence_set_mask = self.stop_tokens_idx == 0\n",
    "        self.stop_tokens_idx[sequence_idx & sequence_set_mask] = input_ids.shape[-1]\n",
    "\n",
    "        return torch.all(self.stop_tokens_idx)\n",
    "\n",
    "class CoVoSTDataset(Dataset):\n",
    "    def __init__(self, processor, data_dir, split=\"validated\", lang=\"en_zh-CN\", rank=0, world_size=1):\n",
    "        try:\n",
    "            print(f\"Loading dataset with data_dir={data_dir}, split={split}\")\n",
    "            self.data = load_dataset(\n",
    "                \"csv\",\n",
    "                lang,\n",
    "                data_dir=data_dir,\n",
    "                split=split,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(f\"Loaded {split} split with {len(self.data)} examples.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {split} split: {e}\")\n",
    "            self.data = []\n",
    "\n",
    "        self.training = \"validated\" in split\n",
    "        self.processor = processor\n",
    "        self.instruction = INSTSRUCTION[lang]\n",
    "\n",
    "        if world_size > 1:\n",
    "            self.data = self.data.shard(world_size, rank)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        {'client_id': '0013037a1d45cc33460806cc3f8ecee9d536c45639ba4cbbf1564f1c051f53ff3c9f89ef2f1bf04badf55b3a2e7654c086f903681a7b6299616cff6f67598eff',\n",
    "        'file': '{data_dir}/clips/common_voice_en_699711.mp3',\n",
    "        'audio': {'path': '{data_dir}/clips/common_voice_en_699711.mp3',\n",
    "        'array': array([-1.28056854e-09, -1.74622983e-09, -1.16415322e-10, ...,\n",
    "                3.92560651e-10,  6.62794264e-10, -3.89536581e-09]),\n",
    "        'sampling_rate': 16000},\n",
    "        'sentence': '\"She\\'ll be all right.\"',\n",
    "        'translation': '她会没事的。',\n",
    "        'id': 'common_voice_en_699711'}\n",
    "        \"\"\"\n",
    "        data = self.data[idx]\n",
    "        user_message = {\n",
    "            'role': 'user',\n",
    "            'content': '<|audio_1|>\\n' + self.instruction,\n",
    "        }\n",
    "        prompt = self.processor.tokenizer.apply_chat_template(\n",
    "            [user_message], tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = self.processor(text=prompt, audios=[(data[\"audio\"][\"array\"], data[\"audio\"][\"sampling_rate\"])], return_tensors='pt')\n",
    "        \n",
    "        answer = f\"{data['translation']}{ANSWER_SUFFIX}\"\n",
    "        answer_ids = self.processor.tokenizer(answer, return_tensors='pt').input_ids\n",
    "        if  self.training:\n",
    "            input_ids = torch.cat([inputs.input_ids, answer_ids], dim=1)\n",
    "            labels = torch.full_like(input_ids, _IGNORE_INDEX)\n",
    "            labels[:, -answer_ids.shape[1] :] = answer_ids\n",
    "        else:\n",
    "            input_ids = inputs.input_ids\n",
    "            labels = answer_ids\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'input_audio_embeds': inputs.input_audio_embeds,\n",
    "            'audio_embed_sizes': inputs.audio_embed_sizes,\n",
    "        }\n",
    "\n",
    "def pad_sequence(sequences, padding_side='right', padding_value=0):\n",
    "    \"\"\"\n",
    "    Pad a list of sequences to the same length.\n",
    "    sequences: list of tensors in [seq_len, *] shape\n",
    "    \"\"\"\n",
    "    assert padding_side in ['right', 'left']\n",
    "    max_size = sequences[0].size()\n",
    "    trailing_dims = max_size[1:]\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    batch_size = len(sequences)\n",
    "    output = sequences[0].new_full((batch_size, max_len) + trailing_dims, padding_value)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = seq.size(0)\n",
    "        if padding_side == 'right':\n",
    "            output.data[i, :length] = seq\n",
    "        else:\n",
    "            output.data[i, -length:] = seq\n",
    "    return output\n",
    "\n",
    "\n",
    "def cat_with_pad(tensors, dim, padding_value=0):\n",
    "    \"\"\"\n",
    "    cat along dim, while pad to max for all other dims\n",
    "    \"\"\"\n",
    "    ndim = tensors[0].dim()\n",
    "    assert all(\n",
    "        t.dim() == ndim for t in tensors[1:]\n",
    "    ), 'All tensors must have the same number of dimensions'\n",
    "\n",
    "    out_size = [max(t.shape[i] for t in tensors) for i in range(ndim)]\n",
    "    out_size[dim] = sum(t.shape[dim] for t in tensors)\n",
    "    output = tensors[0].new_full(out_size, padding_value)\n",
    "\n",
    "    index = 0\n",
    "    for t in tensors:\n",
    "        # Create a slice list where every dimension except dim is full slice\n",
    "        slices = [slice(0, t.shape[d]) for d in range(ndim)]\n",
    "        # Update only the concat dimension slice\n",
    "        slices[dim] = slice(index, index + t.shape[dim])\n",
    "\n",
    "        output[slices] = t\n",
    "        index += t.shape[dim]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def covost_collate_fn(batch):\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    input_audio_embeds_list = []\n",
    "    audio_embed_sizes_list = []\n",
    "    audio_attention_mask_list = []\n",
    "    for inputs in batch:\n",
    "        input_ids_list.append(inputs['input_ids'][0])\n",
    "        labels_list.append(inputs['labels'][0])\n",
    "        input_audio_embeds_list.append(inputs['input_audio_embeds'])\n",
    "        audio_embed_sizes_list.append(inputs['audio_embed_sizes'])\n",
    "        audio_attention_mask_list.append(\n",
    "            inputs['input_audio_embeds'].new_full((inputs['input_audio_embeds'].size(1),), True, dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        input_ids = pad_sequence(input_ids_list, padding_side='left', padding_value=0)\n",
    "        labels = pad_sequence(labels_list, padding_side='left', padding_value=0)\n",
    "        audio_attention_mask = (\n",
    "            pad_sequence(audio_attention_mask_list, padding_side='right', padding_value=False)\n",
    "            if len(audio_attention_mask_list) > 1\n",
    "            else None\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(input_ids_list)\n",
    "        print(labels_list)\n",
    "        raise\n",
    "    attention_mask = (input_ids != 0).long()\n",
    "    input_audio_embeds = cat_with_pad(input_audio_embeds_list, dim=0)\n",
    "    audio_embed_sizes = torch.cat(audio_embed_sizes_list)\n",
    "\n",
    "    return BatchFeature(\n",
    "        {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "            'input_audio_embeds': input_audio_embeds,\n",
    "            'audio_embed_sizes': audio_embed_sizes,\n",
    "            'audio_attention_mask': audio_attention_mask,\n",
    "            'input_mode': 2,  # speech mode\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def create_model(model_name_or_path, use_flash_attention=False):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        torch_dtype=torch.bfloat16 if use_flash_attention else torch.float32,\n",
    "        _attn_implementation='flash_attention_2' if use_flash_attention else 'sdpa',\n",
    "        trust_remote_code=True,\n",
    "    ).to('cuda')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model, processor, eval_dataset, save_path=None, disable_tqdm=False, eval_batch_size=1\n",
    "):\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "\n",
    "    model.eval()\n",
    "    all_generated_texts = []\n",
    "    all_labels = []\n",
    "\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=eval_batch_size,\n",
    "        collate_fn=covost_collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=8,\n",
    "        prefetch_factor=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    stop_tokens = [\"<|end|>\", processor.tokenizer.eos_token]\n",
    "    stop_tokens_ids = processor.tokenizer(stop_tokens, add_special_tokens=False, padding=\"longest\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "    stop_tokens_ids = stop_tokens_ids.to(f'cuda:{local_rank}')\n",
    "\n",
    "    for inputs in tqdm(\n",
    "        eval_dataloader, disable=(rank != 0) or disable_tqdm, desc='running eval'\n",
    "    ):\n",
    "        stopping_criteria=StoppingCriteriaList([MultipleTokenBatchStoppingCriteria(stop_tokens_ids, batch_size=inputs.input_ids.size(0))])\n",
    "        inputs = inputs.to(f'cuda:{local_rank}')\n",
    "        generated_ids = model.generate(\n",
    "            **inputs, eos_token_id=processor.tokenizer.eos_token_id, max_new_tokens=16,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "        )\n",
    "\n",
    "        stop_tokens_idx = stopping_criteria[0].stop_tokens_idx.reshape(inputs.input_ids.size(0), -1)[:, 0]\n",
    "\n",
    "        stop_tokens_idx = torch.where(\n",
    "            stop_tokens_idx > 0,\n",
    "            stop_tokens_idx - stop_tokens_ids.shape[-1],\n",
    "            generated_ids.shape[-1],\n",
    "        )\n",
    "        generated_text = [\n",
    "            processor.decode(_pred_ids[inputs[\"input_ids\"].shape[1] : _stop_tokens_idx], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "            for _pred_ids, _stop_tokens_idx in zip(generated_ids, stop_tokens_idx)\n",
    "        ]\n",
    "        all_generated_texts.extend(generated_text)\n",
    "        labels = [processor.decode(_label_ids[_label_ids != 0]).removesuffix(ANSWER_SUFFIX) for _label_ids in inputs[\"labels\"]]\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    all_generated_texts = gather_object(all_generated_texts)\n",
    "    all_labels = gather_object(all_labels)\n",
    "    \n",
    "    if rank == 0:\n",
    "        assert len(all_generated_texts) == len(all_labels)\n",
    "        bleu = sacrebleu.corpus_bleu(all_generated_texts, [all_labels])\n",
    "        print(bleu)\n",
    "        if save_path:\n",
    "            with open(save_path, 'w') as f:\n",
    "                save_dict = {\n",
    "                    'all_generated_texts': all_generated_texts,\n",
    "                    'all_labels': all_labels,\n",
    "                    'score': bleu.score,\n",
    "                }\n",
    "                json.dump(save_dict, f)\n",
    "\n",
    "        return bleu.score\n",
    "    return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--model_name_or_path',\n",
    "        type=str,\n",
    "        default='microsoft/Phi-4-multimodal-instruct',\n",
    "        help='Model name or path to load from',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--common_voice_dir\",\n",
    "        type=str,\n",
    "        default=\"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en\",\n",
    "        help=\"Unzipped Common Voice Audio dataset directory, refer to https://commonvoice.mozilla.org/en/datasets, version 4.0\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lang\",\n",
    "        type=str,\n",
    "        default=\"en_sl\",\n",
    "        help=\"Language pair for translation.\",\n",
    "    )\n",
    "    parser.add_argument('--use_flash_attention', action='store_true', help='Use Flash Attention')\n",
    "    parser.add_argument('--output_dir', type=str, default='./output/', help='Output directory')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')\n",
    "    parser.add_argument(\n",
    "        '--batch_size_per_gpu',\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help='Batch size per GPU (adjust this to fit in GPU memory)',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_train_epochs', type=int, default=1, help='Number of training epochs'\n",
    "    )\n",
    "    parser.add_argument('--learning_rate', type=float, default=4.0e-5, help='Learning rate')\n",
    "    parser.add_argument('--wd', type=float, default=0.01, help='Weight decay')\n",
    "    parser.add_argument('--no-tqdm', dest='tqdm', action='store_false', help='Disable tqdm')\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    with accelerator.local_main_process_first():\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model = create_model(\n",
    "            args.model_name_or_path,\n",
    "            use_flash_attention=args.use_flash_attention,\n",
    "        )\n",
    "\n",
    "    model.set_lora_adapter('speech')\n",
    "\n",
    "\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
    "\n",
    "    eval_dataset = CoVoSTDataset(processor,\n",
    "                                 data_dir=\"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en\",\n",
    "                                 split=\"train[:100]\",\n",
    "                                 lang=args.lang,\n",
    "                                 rank=rank,\n",
    "                                 world_size=world_size)\n",
    "    \n",
    "    train_dataset = CoVoSTDataset(processor,\n",
    "                                  data_dir=\"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en\",\n",
    "                                  split=f'validated[:{_TRAIN_SIZE}]',\n",
    "                                  lang=args.lang)\n",
    "\n",
    "    num_gpus = accelerator.num_processes\n",
    "    print(f'training on {num_gpus} GPUs')\n",
    "    assert (\n",
    "        args.batch_size % (num_gpus * args.batch_size_per_gpu) == 0\n",
    "    ), 'Batch size must be divisible by the number of GPUs'\n",
    "    gradient_accumulation_steps = args.batch_size // (num_gpus * args.batch_size_per_gpu)\n",
    "\n",
    "    if args.use_flash_attention:\n",
    "        fp16 = False\n",
    "        bf16 = True\n",
    "    else:\n",
    "        fp16 = True\n",
    "        bf16 = False\n",
    "\n",
    "    # hard coded training args\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        per_device_train_batch_size=args.batch_size_per_gpu,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        optim='adamw_torch',\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.95,\n",
    "        adam_epsilon=1e-7,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.wd,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type='linear',\n",
    "        warmup_steps=50,\n",
    "        logging_steps=10,\n",
    "        output_dir=args.output_dir,\n",
    "        save_strategy='no',\n",
    "        save_total_limit=10,\n",
    "        save_only_model=True,\n",
    "        bf16=bf16,\n",
    "        fp16=fp16,\n",
    "        remove_unused_columns=False,\n",
    "        report_to='none',\n",
    "        deepspeed=None,\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        dataloader_num_workers=4,\n",
    "        ddp_find_unused_parameters=True,  # for unused SigLIP layers\n",
    "    )\n",
    "\n",
    "    # eval before fine-tuning\n",
    "    out_path = Path(training_args.output_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        save_path=out_path / 'eval_before.json',\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        eval_batch_size=args.batch_size_per_gpu,\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        print(f'BLEU Score before finetuning: {score}')\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=covost_collate_fn,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    if accelerator.is_main_process:\n",
    "        processor.save_pretrained(training_args.output_dir)\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    # eval after fine-tuning (load saved checkpoint)\n",
    "    # first try to clear GPU memory\n",
    "    del model\n",
    "    del trainer\n",
    "    __import__('gc').collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # reload the model for inference\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        training_args.output_dir,\n",
    "        torch_dtype=torch.bfloat16 if args.use_flash_attention else torch.float32,\n",
    "        trust_remote_code=True,\n",
    "        _attn_implementation='flash_attention_2' if args.use_flash_attention else 'sdpa',\n",
    "    ).to('cuda')\n",
    "\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        save_path=out_path / 'eval_after.json',\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        eval_batch_size=args.batch_size_per_gpu,\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        print(f'BLEU Score after finetuning: {score}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d68160c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting TSV file at: /home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/validated.tsv\n",
      "First few rows of the TSV file:\n",
      "                                           client_id  \\\n",
      "0  116398939d6be70fc5fb532924a130c0adf286ac283499...   \n",
      "1  24a4da2e8f053a45a0715849c222a40a4b0da9872efb2e...   \n",
      "2  30849595699bc853c3810a78448acede46888b4e2d0809...   \n",
      "3  42d53f34c1bc50f7a7c4ed1765a8d1ffeaf5cd441513cc...   \n",
      "4  436b9e1f9da710d74eb01209f8f269bee70e93cadf2053...   \n",
      "\n",
      "                           path  \\\n",
      "0  common_voice_en_41923025.mp3   \n",
      "1  common_voice_en_42356358.mp3   \n",
      "2  common_voice_en_42165090.mp3   \n",
      "3  common_voice_en_41921729.mp3   \n",
      "4  common_voice_en_42528393.mp3   \n",
      "\n",
      "                                         sentence_id  \\\n",
      "0  f5a2a431746c5229ab696ba0e1a518fe7b26e208ff3b84...   \n",
      "1  f6f009587d8812c147af1cc05079e1fcd8120c8a98cdf8...   \n",
      "2  f69afa5e77812e8be0085c874d2a9767323c78ffb43ba6...   \n",
      "3  f5739acbefdbd3aac990792966fac4d40dcb39eb8dfa21...   \n",
      "4  f7d35c60d76f025c45a9495757d1ee0e2b7c206317a288...   \n",
      "\n",
      "                                            sentence sentence_domain  \\\n",
      "0  He was born at Wichenford, in Worcestershire, ...             NaN   \n",
      "1  The Portuguese division was overrun and withdr...             NaN   \n",
      "2            Her health by this stage was also poor.             NaN   \n",
      "3  His sporting interests outside of cricket incl...             NaN   \n",
      "4  The following year he was elected to be part o...             NaN   \n",
      "\n",
      "   up_votes  down_votes       age gender                accents  variant  \\\n",
      "0         2           0  thirties    NaN  United States English      NaN   \n",
      "1         2           0     teens    NaN  United States English      NaN   \n",
      "2         2           0       NaN    NaN                    NaN      NaN   \n",
      "3         2           0  nineties    NaN        England English      NaN   \n",
      "4         2           0     teens    NaN  United States English      NaN   \n",
      "\n",
      "  locale  segment  \n",
      "0     en      NaN  \n",
      "1     en      NaN  \n",
      "2     en      NaN  \n",
      "3     en      NaN  \n",
      "4     en      NaN  \n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "client_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentence_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentence",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentence_domain",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "up_votes",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "down_votes",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "age",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "gender",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "accents",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "variant",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "locale",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "segment",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "b64ae07d-ba2c-4d23-bc6b-783af5437770",
       "rows": [
        [
         "0",
         "116398939d6be70fc5fb532924a130c0adf286ac283499da82ac8940d5cd1435a250bba61eb7fcff20f35087275f7f9d5c5d8bafd1a76dd37794a756b4a46851",
         "common_voice_en_41923025.mp3",
         "f5a2a431746c5229ab696ba0e1a518fe7b26e208ff3b84b164a95f06dd0e02f7",
         "He was born at Wichenford, in Worcestershire, and educated at Balliol College, Oxford.",
         null,
         "2",
         "0",
         "thirties",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "1",
         "24a4da2e8f053a45a0715849c222a40a4b0da9872efb2ed89e6721c4c1139c4e0a4514a3a5be51eef8de345add706d9296d7eb2d10518b45b6684ee43627bb8d",
         "common_voice_en_42356358.mp3",
         "f6f009587d8812c147af1cc05079e1fcd8120c8a98cdf86973552e35bdab48e1",
         "The Portuguese division was overrun and withdrew towards Estaires.",
         null,
         "2",
         "0",
         "teens",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "2",
         "30849595699bc853c3810a78448acede46888b4e2d08097f665cad511ec2b0eba5a46746cc222f24d936dfbbbbdc9e24db13df074d2b9d453c1121c06170b15a",
         "common_voice_en_42165090.mp3",
         "f69afa5e77812e8be0085c874d2a9767323c78ffb43ba65ce58705a9cb8fbf27",
         "Her health by this stage was also poor.",
         null,
         "2",
         "0",
         null,
         null,
         null,
         null,
         "en",
         null
        ],
        [
         "3",
         "42d53f34c1bc50f7a7c4ed1765a8d1ffeaf5cd441513cc0e6c72a4af7b841345903a04d3d5b8de8877de3d8b270a8d38d38a7a2157b845c0da695f53eec413bb",
         "common_voice_en_41921729.mp3",
         "f5739acbefdbd3aac990792966fac4d40dcb39eb8dfa21e1b316e5be00ba0388",
         "His sporting interests outside of cricket included golf.",
         null,
         "2",
         "0",
         "nineties",
         null,
         "England English",
         null,
         "en",
         null
        ],
        [
         "4",
         "436b9e1f9da710d74eb01209f8f269bee70e93cadf20539cb66b5c414da0f8897e019acb4ddf0fbffee8e39ddde52b88cc458c575aa52e34118d2f14070c9ae1",
         "common_voice_en_42528393.mp3",
         "f7d35c60d76f025c45a9495757d1ee0e2b7c206317a2880b9bda6f94e4403a3e",
         "The following year he was elected to be part of the London Designer collections.",
         null,
         "2",
         "0",
         "teens",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "5",
         "47e310af6db6b62e9820ec86ab728d5b5a374e609c86eb3f831c1978f06ee69ede927331d4beb3d1c126bd362cc3a657b3ce2dedde4a6bb377cec1b3312ae10f",
         "common_voice_en_42006138.mp3",
         "09366b701c5430420b021acc7d6e5f9a70c7ddf1504ad1fa026dceff7f2ee59e",
         "A healthy diet combined with lots of exercise can help you keep fit.",
         null,
         "2",
         "0",
         null,
         null,
         "Australian English,Canadian English",
         null,
         "en",
         null
        ],
        [
         "6",
         "55158ab09dacdb9f29a4a9dbe649970d7d5e3c4f634bb9d98568f60c0d85796dadac07493eecf9f7b536d7870c6f8aae226c23067f8f4240fa46b36df45a9c34",
         "common_voice_en_42555516.mp3",
         "f7adddb67a702c8dcb6804f28e1491ec76efbad6cb0c3b5fa4ce0fb84764ca05",
         "Safronov is the nearest rural locality.",
         null,
         "4",
         "0",
         "twenties",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "7",
         "5744531c533d5d1717e7bfdfc6b54a4a4e385e4415900a186f9cf41b5e737eaa0be36bac08fed90e86c55b7fde4d95a44dc9d3994623298bdd808a708455f0ae",
         "common_voice_en_41951792.mp3",
         "f605b1dfdb89d9699c3e08bdf8c3a43895e509ca84e37105fb7af87d2f522b59",
         "Contemporary fellow ministers in the Southern Baptist Convention praised his preaching abilities.",
         null,
         "2",
         "0",
         "thirties",
         null,
         "Scottish English",
         null,
         "en",
         null
        ],
        [
         "8",
         "63249207b46877a627fa51558278f322c17c459c56789e094f91cb6ef3c412b5af2c1f6677460316d3442c91d741e2f3a729aeb8408f3f82bb26a0986bc5f9bc",
         "common_voice_en_42446785.mp3",
         "f73d6aeb53eacc74834b9392149a751408bbf45d5991969573247d3e406d102b",
         "Bucknell tied for third in the Colonial League.",
         null,
         "2",
         "0",
         "fifties",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "9",
         "6ed6b67b63bf01ddb7e8bf6b75bdeb83c7a7e4c42ac226f3224254eb64d7319a220f4568239a4f0d141ce2002f0ea53666c10bae3af29d5fefff217489c987e2",
         "common_voice_en_41974998.mp3",
         "f5d431517eb0c1cc0f3c8fdf2470fa53e060cbc67216ceccac37221a299409bc",
         "The ghettoization was completed within a week.",
         null,
         "2",
         "0",
         "twenties",
         null,
         "England English",
         null,
         "en",
         null
        ],
        [
         "10",
         "75af00ad24068fa1a9aef0ffb9e972914db959cf9ac7cca9d6a527566a1485f0f79b977b1eea8075babf5a6886791eec98e4ba938194a944f71afa480e3f7301",
         "common_voice_en_42020013.mp3",
         "f671e64dd2be9741bf0182aa4e71536e50a187035bc2e8cdbdf6e6ed8898236c",
         "Philpott was for many years Treasurer and then President of the British Psychological Society.",
         null,
         "2",
         "0",
         "twenties",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "11",
         "80fe1dd00d745059a22925385d295a49722b9639f1b2b3844a8e94f491cbbbde88ef7b9e9473bf01e85520e00cdb66575ee1fadf92546e1914596d98cf93e290",
         "common_voice_en_42046273.mp3",
         "f67de257b2ca07e7f5405922619b44311e0cd3e596a1b4be6729bfdbef3913a5",
         "Both the engines and the gearbox proved to be unreliable.",
         null,
         "2",
         "0",
         "twenties",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "12",
         "8532ee1920f74cab963f7b2895f0b5d5519a8762bb1f3b742293915270928c8826003a955e9a9a092982572ddaceed20ff3eaedd2517aa8c4a3759585bfb5493",
         "common_voice_en_42458975.mp3",
         "f745210fe8bb5ef48521b3a1af2fbfac3cee6b58464d294d8a129627298c437d",
         "The award went to David Foster and Jeremy Lubbock.",
         null,
         "2",
         "0",
         "fourties",
         null,
         "India and South Asia (India, Pakistan, Sri Lanka)",
         null,
         "en",
         null
        ],
        [
         "13",
         "89e2882d489f880be5d572f3fdc58e8440c0a18e53f10832d9d70869cea2d7b1b734209474ebd37606239ebb6c2276ca6f023394c1271d1cd66cc2a9e4ab84c6",
         "common_voice_en_42494012.mp3",
         "f75de249735821e402b6a49f9e61f22866a9609d3b4d3b5c22bca596835988de",
         "Its products range from suspension forks to derailleurs.",
         null,
         "2",
         "0",
         "thirties",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "14",
         "95ad670587afb013217297ae3da765f6ec63f8b4a76b96415a4c9e136cc964cc4cee46c7d5b4373f4144decf56767d2ce609c95994d2a7ec89380a9bc684698a",
         "common_voice_en_42435518.mp3",
         "f738847e1791781fe6bb3ff7f994fff77cc0e731de15648fcdc3bd124675ae66",
         "It belongs to the large family of Franco-Belgian comics.",
         null,
         "2",
         "0",
         "twenties",
         null,
         "Russian",
         null,
         "en",
         null
        ],
        [
         "15",
         "a862e97505d11e3e4a47fbe1513473b1527c600e5195b3268656fe21e6316c58aa823fb347128e8c7afc5dc97702dc34d454ee54f525c74668ff2024d4174add",
         "common_voice_en_41930868.mp3",
         "f5ca18b7845b9dd591826b1e64cde421da54d2bb87e2ad46285c2ebb16d3a825",
         "He attended Iowa State University, where he played defense on the school's football team.",
         null,
         "2",
         "0",
         null,
         null,
         null,
         null,
         "en",
         null
        ],
        [
         "16",
         "a97730f86fa90560ae105669364412a9ad393b32839d0151236604af188212aab60bf5168a7975fdd0a448dd3131543f5c0032e737a7164b41e9d9d85ffd6660",
         "common_voice_en_42251480.mp3",
         "f6c46ea54238ce173197efd95fc999ec5a72949afa630572480aee123d67dd90",
         "Some outside scholars examining the system in depth disagree with the \"official\" results.",
         null,
         "2",
         "0",
         "sixties",
         "male_masculine",
         "Filipino",
         null,
         "en",
         null
        ],
        [
         "17",
         "aca627868889f3be90974df9e5b1207f96b4b4c8c4edbd02d7992681d5cc988490c8d59c21342b2850ff33555e7c497104b5e3bfba3c6fa9ad2f4c57566dd691",
         "common_voice_en_41923270.mp3",
         "f5b7b82e503a4f77d7a0aeb8c149fd048693a4398c68df47e1ef00ef3c9cdb3e",
         "Marshallville students attend Green Local School District in nearby Smithville.",
         null,
         "2",
         "0",
         "thirties",
         null,
         "Malaysian English",
         null,
         "en",
         null
        ],
        [
         "18",
         "b05193e293be8fd4e274486c5807611338d00b8aeb0dcc5cc9b17ba465463718dd496b6a750742b6ef19bcb2d131b5d8546e342a24be7cfe45cb9b1f89f33a3f",
         "common_voice_en_42594358.mp3",
         "f8296faf7e5f489cd6525bd7b6b95f457ebc82776f2b721180a09aeaa41293a2",
         "Political positions inside and outside the party are open to women.",
         null,
         "2",
         "0",
         "thirties",
         "female_feminine",
         "United States English",
         null,
         "en",
         null
        ],
        [
         "19",
         "b361b8637db1093983f670ae132d32afbff84e2ca81655df84561ca86313a6363a4d6ee447f4656ddf6247e0b589dd5ddedbd8c652ebc3aacc3b5a74d624e85d",
         "common_voice_en_42141720.mp3",
         "145e1edf6ba791fd1d9c8ab1154c3aca5f8ef783d0b8a5357a36d952ee571298",
         "Despite their frequent bothering, he never paid them any attention.",
         null,
         "2",
         "0",
         null,
         null,
         null,
         null,
         "en",
         null
        ],
        [
         "20",
         "cab08a0c4ae924c447b0a23eabc3594f2368a1b98f14b4025d79b4d24c2cf93c3a9d5432fe10197d56091f8e830da7fcdcb4d7ffc6ebf4fdf166dd88335a427b",
         "common_voice_en_42498572.mp3",
         "1cd411fbc31d40c2a78e14387704635b41d21d27eec1a53fac4f9fc7fc2fef12",
         "A human being cannot be safely trusted solely to the mercy of another.",
         null,
         "2",
         "0",
         "twenties",
         null,
         "England English",
         null,
         "en",
         null
        ],
        [
         "21",
         "d00446e08c607630db0b554cb2c45a91bb46f87042798b0b18a106c5d68b3779c2e2cb46c1b73393743f0c93e08cd978fddac08e3b6ea40ceda2b1ff6e858bc4",
         "common_voice_en_42216083.mp3",
         "1954e2b1eccc4f78e253959b5176e5186cb18b98bb2ea38b5d60b8e41d367214",
         "One area where training specific behavior has gained significant attention is in sports.",
         null,
         "2",
         "0",
         "fourties",
         "male_masculine",
         "United States English",
         null,
         "en",
         null
        ],
        [
         "22",
         "dc40c94b0f302d96d0754793e1b46cbd99f2599beca3474603085a2a908d1ec18b3e7d5357011fd9ad7f05394bb76b07e86f03666a4ce8507f2355e7f1e83e43",
         "common_voice_en_42466107.mp3",
         "f768d66aa599bfc58f8e2e636f51b5fd1b9fa3168294d4480153b263f6f55775",
         "However, the series was launched without this technology.",
         null,
         "2",
         "0",
         "teens",
         "female_feminine",
         "United States English",
         null,
         "en",
         null
        ],
        [
         "23",
         "fcec237c8a98418623d37d238b2de928bc0b7e8637d8326eb4616d7e5a32ebc7f7a4df4af5b5e74c55009280f5e9ad62cd8219d0930cf20322bba65695c76464",
         "common_voice_en_41970688.mp3",
         "f62f4bbca135f63471e79e6131b8f09f0b837e80bcf74f6363dcec0894dce60b",
         "Wiley graduated from Sault Ste.",
         null,
         "2",
         "0",
         "fifties",
         null,
         "England English",
         null,
         "en",
         null
        ],
        [
         "24",
         "ff8ee8cde3ab040f91ed8427f94f0466b7bc07d024d8b8f2a47cf390aefd3a5a77839d6f218b9f0d20b5c79b238474021863565d22550c681dd27a7e5e98cdaf",
         "common_voice_en_42437221.mp3",
         "f73569dc5395cc08e654bb9b1e521e00ff8e90e017b4aed068f4e93f5d22d23d",
         "Lenny Hart was also the Grateful Dead's original money manager.",
         null,
         "2",
         "0",
         null,
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "25",
         "302b51de89ba0e815cce38bba05ee858e556375d19c74554175b32c2f93dc57bc44d0a3c15bf844eea5cd752b97c28fee9b2310029276363f9eabc67eb0c9cca",
         "common_voice_en_42511436.mp3",
         "f7b69a4bfcc4e2033a48edd7823083e621cc407dd2e09b5b949b3593a4d9b825",
         "He later commented that he did not support the death of Norwegian military personnel.",
         null,
         "2",
         "0",
         "sixties",
         "female_feminine",
         "United States English",
         null,
         "en",
         null
        ],
        [
         "26",
         "302b51de89ba0e815cce38bba05ee858e556375d19c74554175b32c2f93dc57bc44d0a3c15bf844eea5cd752b97c28fee9b2310029276363f9eabc67eb0c9cca",
         "common_voice_en_42511438.mp3",
         "f7ae182e9060442207d2a0f147cf57fb016486ca89291b5f367d09aff8d81a38",
         "The truth will come out one day as it happens all the time.",
         null,
         "2",
         "0",
         "sixties",
         "female_feminine",
         "United States English",
         null,
         "en",
         null
        ],
        [
         "27",
         "4a3d198ccac94aa3330809ce2eef552dec6625d116950888019d1d5fbd71e82e022f2878fa2d1a4af25743f6a90ef234165fe3eaba26198c29f3afdb054ba1f8",
         "common_voice_en_41977227.mp3",
         "f616f979f9419305e3d66a88b0205c2fe36b8aaa049b3fb88076e367e45e6233",
         "Colonel Beall served throughout the war as the only Commandant of the Marine Corps.",
         null,
         "2",
         "0",
         "thirties",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "28",
         "4a3d198ccac94aa3330809ce2eef552dec6625d116950888019d1d5fbd71e82e022f2878fa2d1a4af25743f6a90ef234165fe3eaba26198c29f3afdb054ba1f8",
         "common_voice_en_41977272.mp3",
         "f5d3d3f0979a45dd106186a1de94d1f1e45971ec9834d941bfc71574d2fc48ca",
         "It was named a Notable Book of the Year by \"The New York Times\".",
         null,
         "2",
         "0",
         "thirties",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "29",
         "652179379c6aa7b508476ee6b0269d93da89555358a72badb8dc1f76e0cf8a0b4d3c95c38702134aeaed9af7a64ac13cd15f89cd3a6278e2338a0f8855134789",
         "common_voice_en_41940683.mp3",
         "f5d53c83ccbd878634f5fb9fa426372633c2c7740410d63c682984d810643597",
         "His servants obeyed his orders, the monks being powerless to interfere.",
         null,
         "2",
         "0",
         "twenties",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "30",
         "652179379c6aa7b508476ee6b0269d93da89555358a72badb8dc1f76e0cf8a0b4d3c95c38702134aeaed9af7a64ac13cd15f89cd3a6278e2338a0f8855134789",
         "common_voice_en_41940779.mp3",
         "f5f60ee5a2a66f5aea2078c2a8888bd730da07a4dd5a0b7835932bc52ff0787c",
         "To deal with this, Unicode provides the mechanism of canonical equivalence.",
         null,
         "2",
         "0",
         "twenties",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "31",
         "8d1408d97064c03b347bd3788bca175718af604f4a6f7787221e5be323955f96021bc259a3b79a2fefdc9ca3c30fc8ad0a8928ab5038105870963925e768ba95",
         "common_voice_en_42593468.mp3",
         "f7ff60501cf6ac62a19a31a821f4ca9471646724e47401e5a12ddc5df416455b",
         "Raff was born in Lachen in Switzerland.",
         null,
         "2",
         "0",
         "twenties",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "32",
         "8d1408d97064c03b347bd3788bca175718af604f4a6f7787221e5be323955f96021bc259a3b79a2fefdc9ca3c30fc8ad0a8928ab5038105870963925e768ba95",
         "common_voice_en_42593505.mp3",
         "f82531a3c1387fab7f04877c0ac186f982b071cc7fdf74de61b3dae842930d97",
         "While a large percentage consists of incoherent or chaotic sound, referred to as noise.",
         null,
         "2",
         "0",
         "twenties",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "33",
         "b1f27cc2cbe7f65947ba978603322fae97a29a2b348b87bbe4946fdf011443599b807565bfcc38b2075afa78768495e1f6175eb7f3eaab236163199ae00b25f8",
         "common_voice_en_41944826.mp3",
         "f5e5c88d3cf13d94c3911dc0cb47feb093b4a350f631cebb23722d957c3b00a5",
         "Sharity-Light runs in user space rather than kernel space.",
         null,
         "2",
         "0",
         "twenties",
         null,
         null,
         null,
         "en",
         null
        ],
        [
         "34",
         "b1f27cc2cbe7f65947ba978603322fae97a29a2b348b87bbe4946fdf011443599b807565bfcc38b2075afa78768495e1f6175eb7f3eaab236163199ae00b25f8",
         "common_voice_en_41944852.mp3",
         "f6027ca018b0b419249ff775fdb068ac081b8ebabefc83d2ed56d2c4d9095a36",
         "It stars Michael Gross, Alexis Arquette, and Hilary Swank.",
         null,
         "2",
         "0",
         "twenties",
         null,
         null,
         null,
         "en",
         null
        ],
        [
         "35",
         "f946b80de4f5781375def733abf0af634dcc71638c5c993643583a438580faa7c6f560f47b7a4e36f21ae5f8e548b24a3f7100df328b98f3d45324074eca832b",
         "common_voice_en_41958320.mp3",
         "f61edc68c314ced7d14bda1721af942f2a8739a092e3737ef97a282a51ccabc9",
         "Her grave is located at the Hietzing Cemetery.",
         null,
         "4",
         "0",
         "twenties",
         null,
         "United States English,Southern United States English,Lightly Southern",
         null,
         "en",
         null
        ],
        [
         "36",
         "f946b80de4f5781375def733abf0af634dcc71638c5c993643583a438580faa7c6f560f47b7a4e36f21ae5f8e548b24a3f7100df328b98f3d45324074eca832b",
         "common_voice_en_41958333.mp3",
         "0829b2f421ecac490253ff04e0791c85d7fce953a0a39c306ce2381d3f02e1cc",
         "Future exploration will have to involve the smaller basins as well as",
         null,
         "4",
         "0",
         "twenties",
         null,
         "United States English,Southern United States English,Lightly Southern",
         null,
         "en",
         null
        ],
        [
         "37",
         "29b8505586cd43382cd695da6b943f401104be710a5b60e814ac5fe7e06b39459cf8fe1701ca83f8154b3ccd749df7c2aef33ff23950bb1a135b1e1c393dbcf6",
         "common_voice_en_41969910.mp3",
         "f6261dfe192766a460c62fbc9a3dd42bae107e9dcedc0b21d3c63db749e0643d",
         "Stevenson was born in Lindsay, Ontario.",
         null,
         "2",
         "0",
         "fourties",
         "female_feminine",
         "Scottish English",
         null,
         "en",
         null
        ],
        [
         "38",
         "29b8505586cd43382cd695da6b943f401104be710a5b60e814ac5fe7e06b39459cf8fe1701ca83f8154b3ccd749df7c2aef33ff23950bb1a135b1e1c393dbcf6",
         "common_voice_en_42182410.mp3",
         "162865aa8fb62c3aafea4fc4d97d16817e033321294206360802a6d3e68bf1ec",
         "Follow the link to find the list of World Alliances.",
         null,
         "2",
         "0",
         "fourties",
         "female_feminine",
         "Scottish English",
         null,
         "en",
         null
        ],
        [
         "39",
         "29b8505586cd43382cd695da6b943f401104be710a5b60e814ac5fe7e06b39459cf8fe1701ca83f8154b3ccd749df7c2aef33ff23950bb1a135b1e1c393dbcf6",
         "common_voice_en_42264420.mp3",
         "19166a955f3aefacf15bab9d6a2a1a54659fee229ba2851dbc7289332205a20c",
         "A teaspoon is typically characterized by its small size and long handle.",
         null,
         "2",
         "0",
         "fourties",
         "female_feminine",
         "Scottish English",
         null,
         "en",
         null
        ],
        [
         "40",
         "78899c686b3bdef6619b63f98b03d3f5fdf2ca7427d0ce6dace8a10befc17941b33c5b7b7c2a808f0c670e9560ad5da356b6f4b06407ce3e1a299787168c3b6a",
         "common_voice_en_41918312.mp3",
         "f5ac5625dcdcef23e2d6d456dbf6f00e964ff14fd17a739b4ed360bfac85f93b",
         "These demos helped build hype around the band.",
         null,
         "2",
         "0",
         "twenties",
         "male_masculine",
         "India and South Asia (India, Pakistan, Sri Lanka)",
         null,
         "en",
         null
        ],
        [
         "41",
         "78899c686b3bdef6619b63f98b03d3f5fdf2ca7427d0ce6dace8a10befc17941b33c5b7b7c2a808f0c670e9560ad5da356b6f4b06407ce3e1a299787168c3b6a",
         "common_voice_en_41918321.mp3",
         "f5b2c816cad7667f24734349dce5bd5c006d57d1bb7a839057571b019c3a4c15",
         "Once a cat burglar, a master among jewel thieves.",
         null,
         "2",
         "0",
         "twenties",
         "male_masculine",
         "India and South Asia (India, Pakistan, Sri Lanka)",
         null,
         "en",
         null
        ],
        [
         "42",
         "78899c686b3bdef6619b63f98b03d3f5fdf2ca7427d0ce6dace8a10befc17941b33c5b7b7c2a808f0c670e9560ad5da356b6f4b06407ce3e1a299787168c3b6a",
         "common_voice_en_41918336.mp3",
         "f59596aee63728b7586c68492dea27581bbf67d8d984a321e1a665891be9a1ef",
         "Teams are seeded according to their regular-season record.",
         null,
         "2",
         "0",
         "twenties",
         "male_masculine",
         "India and South Asia (India, Pakistan, Sri Lanka)",
         null,
         "en",
         null
        ],
        [
         "43",
         "79d543138d8b1a49e9c2e74ff9a129bd3f34bd597cff59f8485b011eac15cb6113062d17df49c5b649e4c4793ebce6bb76488cc81bf22a7de0f1a37eaeff9b18",
         "common_voice_en_41910645.mp3",
         "f569b561b214ba0de8dad201dde830ea7c68feb9157d79ef16411b1668e2873f",
         "It is located about west of Richfield.",
         null,
         "2",
         "0",
         "twenties",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "44",
         "79d543138d8b1a49e9c2e74ff9a129bd3f34bd597cff59f8485b011eac15cb6113062d17df49c5b649e4c4793ebce6bb76488cc81bf22a7de0f1a37eaeff9b18",
         "common_voice_en_41910679.mp3",
         "f591f7de61805639d04b507b61a5d934fc550186dd89588a5a364a52d2054d29",
         "This mature wood is mainly oak and beech on clay soils.",
         null,
         "2",
         "0",
         "twenties",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "45",
         "79d543138d8b1a49e9c2e74ff9a129bd3f34bd597cff59f8485b011eac15cb6113062d17df49c5b649e4c4793ebce6bb76488cc81bf22a7de0f1a37eaeff9b18",
         "common_voice_en_42604282.mp3",
         "f82ce5211d55cf6328331a667ccfafdcf69e556d429d4aa99fbdb91d84ebc5e0",
         "If anything, I was more competent in handling physical affairs.",
         null,
         "2",
         "0",
         "twenties",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "46",
         "b1b825daa6904d4323e932d29400e0e7e0b0cd3a51a7e812a21d499a06a9b16a4b671f32675b2bcc569ec67c5d84b182e36d6cc2efa8eb3b3f80034a3df472a2",
         "common_voice_en_42510046.mp3",
         "f7a837c21ee18124ba65011003ffed44f3ed815cf648eec3e4bb91cd98fd3a8e",
         "The village is located by the Deer River.",
         null,
         "2",
         "0",
         "twenties",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "47",
         "b1b825daa6904d4323e932d29400e0e7e0b0cd3a51a7e812a21d499a06a9b16a4b671f32675b2bcc569ec67c5d84b182e36d6cc2efa8eb3b3f80034a3df472a2",
         "common_voice_en_42510071.mp3",
         "f7b54c63fbdfa7d4e8e2d67b086435fac649233155267a9245696374bdabf234",
         "A few tens of planets have been found around red giants.",
         null,
         "2",
         "0",
         "twenties",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "48",
         "b1b825daa6904d4323e932d29400e0e7e0b0cd3a51a7e812a21d499a06a9b16a4b671f32675b2bcc569ec67c5d84b182e36d6cc2efa8eb3b3f80034a3df472a2",
         "common_voice_en_42510072.mp3",
         "f7a4deaa4e75f55de247ef125e0500a48c7bc9ee3bb2ae35a06d6b0c6767d762",
         "Many possibilities of what could be changed after recess were discussed.",
         null,
         "2",
         "0",
         "twenties",
         null,
         "United States English",
         null,
         "en",
         null
        ],
        [
         "49",
         "c3c8cb4802b0f8b865586cb8cf05b16b45539f2b568f5e0293872c0c418cb2110ab8a41f7dcdc33c1bfc6af805f344dfc67f5801f60df91a998cb2eec37f177e",
         "common_voice_en_41944462.mp3",
         "f5e753db4b6b001edfa177d8d62db306394128cf771027aa8bd30b679988a736",
         "His long career in radio included starring in the series \"Dangerously Yours\".",
         null,
         "2",
         "0",
         "thirties",
         "female_feminine",
         "L2",
         null,
         "en",
         null
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 249
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>path</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_domain</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accents</th>\n",
       "      <th>variant</th>\n",
       "      <th>locale</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>116398939d6be70fc5fb532924a130c0adf286ac283499...</td>\n",
       "      <td>common_voice_en_41923025.mp3</td>\n",
       "      <td>f5a2a431746c5229ab696ba0e1a518fe7b26e208ff3b84...</td>\n",
       "      <td>He was born at Wichenford, in Worcestershire, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>thirties</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24a4da2e8f053a45a0715849c222a40a4b0da9872efb2e...</td>\n",
       "      <td>common_voice_en_42356358.mp3</td>\n",
       "      <td>f6f009587d8812c147af1cc05079e1fcd8120c8a98cdf8...</td>\n",
       "      <td>The Portuguese division was overrun and withdr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>teens</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30849595699bc853c3810a78448acede46888b4e2d0809...</td>\n",
       "      <td>common_voice_en_42165090.mp3</td>\n",
       "      <td>f69afa5e77812e8be0085c874d2a9767323c78ffb43ba6...</td>\n",
       "      <td>Her health by this stage was also poor.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42d53f34c1bc50f7a7c4ed1765a8d1ffeaf5cd441513cc...</td>\n",
       "      <td>common_voice_en_41921729.mp3</td>\n",
       "      <td>f5739acbefdbd3aac990792966fac4d40dcb39eb8dfa21...</td>\n",
       "      <td>His sporting interests outside of cricket incl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>nineties</td>\n",
       "      <td>NaN</td>\n",
       "      <td>England English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>436b9e1f9da710d74eb01209f8f269bee70e93cadf2053...</td>\n",
       "      <td>common_voice_en_42528393.mp3</td>\n",
       "      <td>f7d35c60d76f025c45a9495757d1ee0e2b7c206317a288...</td>\n",
       "      <td>The following year he was elected to be part o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>teens</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>403c1ec9623855b620b043ca2ca76c02989e679e32ba27...</td>\n",
       "      <td>common_voice_en_42220527.mp3</td>\n",
       "      <td>f6a82535d414098f3f8b3ee89c493cd32500bea023cfe6...</td>\n",
       "      <td>Instead a dirt highway going north-east reache...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>nigerian accent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>403c1ec9623855b620b043ca2ca76c02989e679e32ba27...</td>\n",
       "      <td>common_voice_en_42220889.mp3</td>\n",
       "      <td>f6bcef67cc94a7c400363a92c50f2b3ae22c359bf57b6e...</td>\n",
       "      <td>Their soles need to possess both \"slip\" and \"g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>nigerian accent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>403c1ec9623855b620b043ca2ca76c02989e679e32ba27...</td>\n",
       "      <td>common_voice_en_42221034.mp3</td>\n",
       "      <td>f6b6c17bccc00f006a1264b40f65782f57e132d41d0d3b...</td>\n",
       "      <td>Sloman was married with three daughters.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>nigerian accent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>403c1ec9623855b620b043ca2ca76c02989e679e32ba27...</td>\n",
       "      <td>common_voice_en_42221077.mp3</td>\n",
       "      <td>f6a6f1b3fda1e933bf19f11b07e7b25e7dd73260748efc...</td>\n",
       "      <td>He was named as Charles by his parents when he...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>nigerian accent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>403c1ec9623855b620b043ca2ca76c02989e679e32ba27...</td>\n",
       "      <td>common_voice_en_42221538.mp3</td>\n",
       "      <td>f6b5acc7a10f9299c4ed434743cd394c3cb68e79f159ab...</td>\n",
       "      <td>Danny Kingsbury was the architect of this new ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>nigerian accent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             client_id  \\\n",
       "0    116398939d6be70fc5fb532924a130c0adf286ac283499...   \n",
       "1    24a4da2e8f053a45a0715849c222a40a4b0da9872efb2e...   \n",
       "2    30849595699bc853c3810a78448acede46888b4e2d0809...   \n",
       "3    42d53f34c1bc50f7a7c4ed1765a8d1ffeaf5cd441513cc...   \n",
       "4    436b9e1f9da710d74eb01209f8f269bee70e93cadf2053...   \n",
       "..                                                 ...   \n",
       "244  403c1ec9623855b620b043ca2ca76c02989e679e32ba27...   \n",
       "245  403c1ec9623855b620b043ca2ca76c02989e679e32ba27...   \n",
       "246  403c1ec9623855b620b043ca2ca76c02989e679e32ba27...   \n",
       "247  403c1ec9623855b620b043ca2ca76c02989e679e32ba27...   \n",
       "248  403c1ec9623855b620b043ca2ca76c02989e679e32ba27...   \n",
       "\n",
       "                             path  \\\n",
       "0    common_voice_en_41923025.mp3   \n",
       "1    common_voice_en_42356358.mp3   \n",
       "2    common_voice_en_42165090.mp3   \n",
       "3    common_voice_en_41921729.mp3   \n",
       "4    common_voice_en_42528393.mp3   \n",
       "..                            ...   \n",
       "244  common_voice_en_42220527.mp3   \n",
       "245  common_voice_en_42220889.mp3   \n",
       "246  common_voice_en_42221034.mp3   \n",
       "247  common_voice_en_42221077.mp3   \n",
       "248  common_voice_en_42221538.mp3   \n",
       "\n",
       "                                           sentence_id  \\\n",
       "0    f5a2a431746c5229ab696ba0e1a518fe7b26e208ff3b84...   \n",
       "1    f6f009587d8812c147af1cc05079e1fcd8120c8a98cdf8...   \n",
       "2    f69afa5e77812e8be0085c874d2a9767323c78ffb43ba6...   \n",
       "3    f5739acbefdbd3aac990792966fac4d40dcb39eb8dfa21...   \n",
       "4    f7d35c60d76f025c45a9495757d1ee0e2b7c206317a288...   \n",
       "..                                                 ...   \n",
       "244  f6a82535d414098f3f8b3ee89c493cd32500bea023cfe6...   \n",
       "245  f6bcef67cc94a7c400363a92c50f2b3ae22c359bf57b6e...   \n",
       "246  f6b6c17bccc00f006a1264b40f65782f57e132d41d0d3b...   \n",
       "247  f6a6f1b3fda1e933bf19f11b07e7b25e7dd73260748efc...   \n",
       "248  f6b5acc7a10f9299c4ed434743cd394c3cb68e79f159ab...   \n",
       "\n",
       "                                              sentence sentence_domain  \\\n",
       "0    He was born at Wichenford, in Worcestershire, ...             NaN   \n",
       "1    The Portuguese division was overrun and withdr...             NaN   \n",
       "2              Her health by this stage was also poor.             NaN   \n",
       "3    His sporting interests outside of cricket incl...             NaN   \n",
       "4    The following year he was elected to be part o...             NaN   \n",
       "..                                                 ...             ...   \n",
       "244  Instead a dirt highway going north-east reache...             NaN   \n",
       "245  Their soles need to possess both \"slip\" and \"g...             NaN   \n",
       "246           Sloman was married with three daughters.             NaN   \n",
       "247  He was named as Charles by his parents when he...             NaN   \n",
       "248  Danny Kingsbury was the architect of this new ...             NaN   \n",
       "\n",
       "     up_votes  down_votes       age           gender                accents  \\\n",
       "0           2           0  thirties              NaN  United States English   \n",
       "1           2           0     teens              NaN  United States English   \n",
       "2           2           0       NaN              NaN                    NaN   \n",
       "3           2           0  nineties              NaN        England English   \n",
       "4           2           0     teens              NaN  United States English   \n",
       "..        ...         ...       ...              ...                    ...   \n",
       "244         2           0  twenties  female_feminine        nigerian accent   \n",
       "245         2           0  twenties  female_feminine        nigerian accent   \n",
       "246         2           0  twenties  female_feminine        nigerian accent   \n",
       "247         2           0  twenties  female_feminine        nigerian accent   \n",
       "248         2           0  twenties  female_feminine        nigerian accent   \n",
       "\n",
       "     variant locale  segment  \n",
       "0        NaN     en      NaN  \n",
       "1        NaN     en      NaN  \n",
       "2        NaN     en      NaN  \n",
       "3        NaN     en      NaN  \n",
       "4        NaN     en      NaN  \n",
       "..       ...    ...      ...  \n",
       "244      NaN     en      NaN  \n",
       "245      NaN     en      NaN  \n",
       "246      NaN     en      NaN  \n",
       "247      NaN     en      NaN  \n",
       "248      NaN     en      NaN  \n",
       "\n",
       "[249 rows x 13 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def inspect_tsv_file(file_path):\n",
    "    try:\n",
    "        print(f\"Inspecting TSV file at: {file_path}\")\n",
    "        df = pd.read_csv(file_path, sep='\\t')  # Use '\\t' to indicate tab separation\n",
    "        print(\"First few rows of the TSV file:\")\n",
    "        print(df.head())  # Show the first few rows of the dataset\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading TSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Inspect one of the TSV files from the directory (you can change this to another file if needed)\n",
    "sample_tsv_file = os.path.join(data_dir, \"validated.tsv\")  # Change this to the relevant file\n",
    "inspect_tsv_file(sample_tsv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4f8b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--model_name_or_path',\n",
    "        type=str,\n",
    "        default='microsoft/Phi-4-multimodal-instruct',\n",
    "        help='Model name or path to load from',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lang\",\n",
    "        type=str,\n",
    "        default=\"en_sl\",\n",
    "        help=\"Language pair for translation.\",\n",
    "    )\n",
    "    parser.add_argument('--output_dir', type=str, default='./output/', help='Output directory')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')\n",
    "    parser.add_argument('--batch_size_per_gpu', type=int, default=32, help='Batch size per GPU')\n",
    "    parser.add_argument('--num_train_epochs', type=int, default=1, help='Number of training epochs')\n",
    "    parser.add_argument('--learning_rate', type=float, default=4.0e-5, help='Learning rate')\n",
    "    parser.add_argument('--wd', type=float, default=0.01, help='Weight decay')\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    with accelerator.local_main_process_first():\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model = create_model(\n",
    "            args.model_name_or_path,\n",
    "        )\n",
    "\n",
    "    model.set_lora_adapter('speech')\n",
    "\n",
    "    # Load the dataset with Pandas and convert to Hugging Face Dataset\n",
    "    validated_path = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/validated.tsv\"\n",
    "    df = pd.read_csv(validated_path, sep=\"\\t\")\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    # Split the dataset into training and evaluation sets\n",
    "    train_size = int(0.8 * len(hf_dataset))\n",
    "    eval_size = len(hf_dataset) - train_size\n",
    "    train_dataset_raw, eval_dataset_raw = hf_dataset.train_test_split(\n",
    "        test_size=eval_size / len(hf_dataset)\n",
    "    ).values()\n",
    "\n",
    "    # Create CoVoSTDataset instances\n",
    "    train_dataset = CoVoSTDataset(processor, train_dataset_raw, training=True, lang=args.lang)\n",
    "    eval_dataset = CoVoSTDataset(processor, eval_dataset_raw, training=False, lang=args.lang)\n",
    "\n",
    "    num_gpus = accelerator.num_processes\n",
    "    print(f'training on {num_gpus} GPUs')\n",
    "    gradient_accumulation_steps = args.batch_size // (num_gpus * args.batch_size_per_gpu)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        per_device_train_batch_size=args.batch_size_per_gpu,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.wd,\n",
    "        output_dir=args.output_dir,\n",
    "        save_strategy='no',\n",
    "        remove_unused_columns=False,\n",
    "        disable_tqdm=True,\n",
    "    )\n",
    "\n",
    "    # Evaluate before fine-tuning\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        eval_batch_size=args.batch_size_per_gpu,\n",
    "    )\n",
    "    print(f'BLEU Score before finetuning: {score}')\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=covost_collate_fn,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "\n",
    "    # Evaluate after fine-tuning\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        eval_batch_size=args.batch_size_per_gpu,\n",
    "    )\n",
    "    print(f'BLEU Score after finetuning: {score}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d61f6",
   "metadata": {},
   "source": [
    "## Loading smaller version of Common Vocie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8324ccf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files and directories in data_dir:\n",
      "['unvalidated_sentences.tsv', 'reported.tsv', 'validated_sentences.tsv', 'clip_durations.tsv', 'invalidated.tsv', 'clips', 'validated.tsv', 'other.tsv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en\"\n",
    "print(\"Files and directories in data_dir:\")\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf25ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files and directories in data_dir:\n",
      "['unvalidated_sentences.tsv', 'reported.tsv', 'validated_sentences.tsv', 'clip_durations.tsv', 'invalidated.tsv', 'clips', 'validated.tsv', 'other.tsv']\n",
      "                                           client_id  \\\n",
      "0  116398939d6be70fc5fb532924a130c0adf286ac283499...   \n",
      "1  24a4da2e8f053a45a0715849c222a40a4b0da9872efb2e...   \n",
      "2  30849595699bc853c3810a78448acede46888b4e2d0809...   \n",
      "3  42d53f34c1bc50f7a7c4ed1765a8d1ffeaf5cd441513cc...   \n",
      "4  436b9e1f9da710d74eb01209f8f269bee70e93cadf2053...   \n",
      "\n",
      "                           path  \\\n",
      "0  common_voice_en_41923025.mp3   \n",
      "1  common_voice_en_42356358.mp3   \n",
      "2  common_voice_en_42165090.mp3   \n",
      "3  common_voice_en_41921729.mp3   \n",
      "4  common_voice_en_42528393.mp3   \n",
      "\n",
      "                                         sentence_id  \\\n",
      "0  f5a2a431746c5229ab696ba0e1a518fe7b26e208ff3b84...   \n",
      "1  f6f009587d8812c147af1cc05079e1fcd8120c8a98cdf8...   \n",
      "2  f69afa5e77812e8be0085c874d2a9767323c78ffb43ba6...   \n",
      "3  f5739acbefdbd3aac990792966fac4d40dcb39eb8dfa21...   \n",
      "4  f7d35c60d76f025c45a9495757d1ee0e2b7c206317a288...   \n",
      "\n",
      "                                            sentence sentence_domain  \\\n",
      "0  He was born at Wichenford, in Worcestershire, ...             NaN   \n",
      "1  The Portuguese division was overrun and withdr...             NaN   \n",
      "2            Her health by this stage was also poor.             NaN   \n",
      "3  His sporting interests outside of cricket incl...             NaN   \n",
      "4  The following year he was elected to be part o...             NaN   \n",
      "\n",
      "   up_votes  down_votes       age gender                accents  variant  \\\n",
      "0         2           0  thirties    NaN  United States English      NaN   \n",
      "1         2           0     teens    NaN  United States English      NaN   \n",
      "2         2           0       NaN    NaN                    NaN      NaN   \n",
      "3         2           0  nineties    NaN        England English      NaN   \n",
      "4         2           0     teens    NaN  United States English      NaN   \n",
      "\n",
      "  locale  segment  \n",
      "0     en      NaN  \n",
      "1     en      NaN  \n",
      "2     en      NaN  \n",
      "3     en      NaN  \n",
      "4     en      NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_dir = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en\"\n",
    "print(\"Files and directories in data_dir:\")\n",
    "print(os.listdir(data_dir))\n",
    "\n",
    "validated_path = os.path.join(data_dir, \"validated.tsv\")\n",
    "if os.path.exists(validated_path):\n",
    "    df = pd.read_csv(validated_path, sep=\"\\t\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"validated.tsv is missing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89193aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 202 examples [00:00, 62115.06 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully with 202 examples!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "try:\n",
    "    dataset = load_dataset(\n",
    "         \"csv\",\n",
    "        data_files=\"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/validated.tsv\",\n",
    "        column_names=[\"client_id\", \"path\", \"sentence_id\" \"sentence\", \"sentence_domain\", \"up_votes\", \"down_votes\", \"age\", \"gender\", \"accent\", \"variant\", \"locale\", \"segment\"],\n",
    "        split=\"train\",    \n",
    "        )\n",
    "    print(f\"Dataset loaded successfully with {len(dataset)} examples!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d0a755d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:00, ? examples/s]Failed to read file '/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/validated.tsv' with error <class 'pandas.errors.ParserError'>: Error tokenizing data. C error: Expected 4 fields in line 182, saw 5\n",
      "\n",
      "Generating train split: 0 examples [00:00, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading dataset: An error occurred while generating the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "try:\n",
    "    # Load the dataset and let it infer the column names from the header\n",
    "    dataset = load_dataset(\n",
    "        \"csv\",\n",
    "        data_files=\"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/validated.tsv\",\n",
    "        # column_names=[\"client_id\", \"path\", \"sentence_id\" \"sentence\", \"sentence_domain\", \"up_votes\", \"down_votes\", \"age\", \"gender\", \"accent\", \"variant\", \"locale\", \"segment\"],\n",
    "\n",
    "    )\n",
    "    \n",
    "    # Print the inferred column names\n",
    "    print(f\"Dataset loaded successfully with {len(dataset)} examples!\")\n",
    "    print(f\"Columns: {dataset.column_names}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e57eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully!\n",
      "First 5 rows of the file:\n",
      "                                           client_id  \\\n",
      "0  116398939d6be70fc5fb532924a130c0adf286ac283499...   \n",
      "1  24a4da2e8f053a45a0715849c222a40a4b0da9872efb2e...   \n",
      "2  30849595699bc853c3810a78448acede46888b4e2d0809...   \n",
      "3  42d53f34c1bc50f7a7c4ed1765a8d1ffeaf5cd441513cc...   \n",
      "4  436b9e1f9da710d74eb01209f8f269bee70e93cadf2053...   \n",
      "\n",
      "                           path  \\\n",
      "0  common_voice_en_41923025.mp3   \n",
      "1  common_voice_en_42356358.mp3   \n",
      "2  common_voice_en_42165090.mp3   \n",
      "3  common_voice_en_41921729.mp3   \n",
      "4  common_voice_en_42528393.mp3   \n",
      "\n",
      "                                         sentence_id  \\\n",
      "0  f5a2a431746c5229ab696ba0e1a518fe7b26e208ff3b84...   \n",
      "1  f6f009587d8812c147af1cc05079e1fcd8120c8a98cdf8...   \n",
      "2  f69afa5e77812e8be0085c874d2a9767323c78ffb43ba6...   \n",
      "3  f5739acbefdbd3aac990792966fac4d40dcb39eb8dfa21...   \n",
      "4  f7d35c60d76f025c45a9495757d1ee0e2b7c206317a288...   \n",
      "\n",
      "                                            sentence sentence_domain  \\\n",
      "0  He was born at Wichenford, in Worcestershire, ...             NaN   \n",
      "1  The Portuguese division was overrun and withdr...             NaN   \n",
      "2            Her health by this stage was also poor.             NaN   \n",
      "3  His sporting interests outside of cricket incl...             NaN   \n",
      "4  The following year he was elected to be part o...             NaN   \n",
      "\n",
      "   up_votes  down_votes       age gender                accents  variant  \\\n",
      "0         2           0  thirties    NaN  United States English      NaN   \n",
      "1         2           0     teens    NaN  United States English      NaN   \n",
      "2         2           0       NaN    NaN                    NaN      NaN   \n",
      "3         2           0  nineties    NaN        England English      NaN   \n",
      "4         2           0     teens    NaN  United States English      NaN   \n",
      "\n",
      "  locale  segment  \n",
      "0     en      NaN  \n",
      "1     en      NaN  \n",
      "2     en      NaN  \n",
      "3     en      NaN  \n",
      "4     en      NaN  \n",
      "\n",
      "Columns in the file:\n",
      "['client_id', 'path', 'sentence_id', 'sentence', 'sentence_domain', 'up_votes', 'down_votes', 'age', 'gender', 'accents', 'variant', 'locale', 'segment']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the validated.tsv file\n",
    "validated_path = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/validated.tsv\"\n",
    "\n",
    "# Check if the file exists and load it with Pandas\n",
    "try:\n",
    "    # Load the .tsv file\n",
    "    df = pd.read_csv(validated_path, sep=\"\\t\")  # Use '\\t' as the separator for .tsv files\n",
    "    \n",
    "    # Print the first few rows of the file\n",
    "    print(\"File loaded successfully!\")\n",
    "    print(\"First 5 rows of the file:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Print the column names\n",
    "    print(\"\\nColumns in the file:\")\n",
    "    print(df.columns.tolist())\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {validated_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97c66b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['client_id', 'path', 'sentence_id', 'sentence', 'sentence_domain', 'up_votes', 'down_votes', 'age', 'gender', 'accents', 'variant', 'locale', 'segment'],\n",
      "    num_rows: 249\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the .tsv file with Pandas\n",
    "df = pd.read_csv(validated_path, sep=\"\\t\")\n",
    "\n",
    "# Convert the Pandas DataFrame to a Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Print the first few rows of the Hugging Face Dataset\n",
    "print(hf_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3620e554",
   "metadata": {},
   "source": [
    "## Load smaller version into pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fede092",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torchaudio\n",
    "\n",
    "from datasets import Dataset\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "INSTSRUCTION = {\n",
    "    \"en_zh-CN\": \"Translate the audio to Mandarin.\",\n",
    "    \"en_id\": \"Translate the audio to Indonesian.\",\n",
    "    \"en_sl\": \"Translate the audio to Slovenian.\",\n",
    "}\n",
    "TOKENIZER = {\n",
    "    \"en_zh-CN\": \"zh\",\n",
    "    \"en_ja\": \"ja-mecab\",\n",
    "}\n",
    "ANSWER_SUFFIX = \"<|end|><|endoftext|>\"\n",
    "_IGNORE_INDEX = -100\n",
    "_TRAIN_SIZE = 50000\n",
    "_EVAL_SIZE = 200\n",
    "\n",
    "class MultipleTokenBatchStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"Stopping criteria capable of receiving multiple stop-tokens and handling batched inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, stop_tokens: torch.LongTensor, batch_size: int = 1) -> None:\n",
    "        \"\"\"Initialize the multiple token batch stopping criteria.\n",
    "\n",
    "        Args:\n",
    "            stop_tokens: Stop-tokens.\n",
    "            batch_size: Batch size.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.stop_tokens = stop_tokens\n",
    "        self.max_stop_tokens = stop_tokens.shape[-1]\n",
    "        self.stop_tokens_idx = torch.zeros(batch_size, dtype=torch.long, device=stop_tokens.device)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # Only gather the maximum number of inputs compatible with stop tokens\n",
    "        # and checks whether generated inputs are equal to `stop_tokens`\n",
    "        generated_inputs = torch.eq(input_ids[:, -self.max_stop_tokens :].unsqueeze(1), self.stop_tokens)\n",
    "        equal_generated_inputs = torch.all(generated_inputs, dim=2)\n",
    "\n",
    "        # Mark the position where a stop token has been produced for each input in the batch,\n",
    "        # but only if the corresponding entry is not already set\n",
    "        sequence_idx = torch.any(equal_generated_inputs, dim=1)\n",
    "        sequence_set_mask = self.stop_tokens_idx == 0\n",
    "        self.stop_tokens_idx[sequence_idx & sequence_set_mask] = input_ids.shape[-1]\n",
    "\n",
    "        return torch.all(self.stop_tokens_idx)\n",
    "\n",
    "def preprocess_dataset(validated_path, base_audio_dir):\n",
    "    # Load the dataset using Pandas\n",
    "    df = pd.read_csv(validated_path, sep=\"\\t\")\n",
    "\n",
    "    # Print column names to verify structure\n",
    "    print(\"Columns in the dataset:\", df.columns)\n",
    "\n",
    "    # Add audio data to the dataset\n",
    "    def load_audio(row):\n",
    "        try:\n",
    "            # Construct the full path to the audio file\n",
    "            audio_file_path = os.path.join(base_audio_dir, row[\"path\"])\n",
    "            waveform, sampling_rate = torchaudio.load(audio_file_path)\n",
    "            return {\"array\": waveform.squeeze().numpy(), \"sampling_rate\": sampling_rate}\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio for file {row['path']}: {e}\")\n",
    "            return {\"array\": None, \"sampling_rate\": None}\n",
    "\n",
    "    # Ensure the \"path\" column exists\n",
    "    if \"path\" not in df.columns:\n",
    "        raise KeyError(\"The 'path' column is missing in the dataset. Please check the dataset structure.\")\n",
    "\n",
    "    df[\"audio\"] = df.apply(load_audio, axis=1)\n",
    "\n",
    "    # Filter out rows where audio could not be loaded\n",
    "    df = df[df[\"audio\"].apply(lambda x: x[\"array\"] is not None)]\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "    return hf_dataset\n",
    "\n",
    "\n",
    "class CoVoSTDataset(Dataset):\n",
    "    def __init__(self, processor, dataset, training=True, lang=\"en_zh-CN\"):\n",
    "        self.dataset = dataset\n",
    "        self.training = training\n",
    "        self.processor = processor\n",
    "        self.instruction = INSTSRUCTION[lang]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "\n",
    "        # Debugging: Check data structure\n",
    "        print(f\"Data at index {idx}: {data}\")\n",
    "\n",
    "        # Ensure translation field exists\n",
    "        assert \"translation\" in data, f\"Missing 'translation' field in data: {data}\"\n",
    "        assert data[\"translation\"], f\"Empty 'translation' field in data: {data}\"\n",
    "\n",
    "        user_message = {\n",
    "            'role': 'user',\n",
    "            'content': '<|audio_1|>\\n' + self.instruction,\n",
    "        }\n",
    "        prompt = self.processor.tokenizer.apply_chat_template(\n",
    "            [user_message], tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Debugging: Check generated prompt\n",
    "        print(f\"Generated prompt: {prompt}\")\n",
    "\n",
    "        try:\n",
    "            inputs = self.processor(\n",
    "                text=prompt,\n",
    "                audios=[(data[\"audio\"][\"array\"], data[\"audio\"][\"sampling_rate\"])],\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error in processor: {e}\")\n",
    "            raise\n",
    "\n",
    "        answer = f\"{data['translation']}{ANSWER_SUFFIX}\"\n",
    "        answer_ids = self.processor.tokenizer(answer, return_tensors='pt').input_ids\n",
    "\n",
    "        if self.training:\n",
    "            input_ids = torch.cat([inputs.input_ids, answer_ids], dim=1)\n",
    "            labels = torch.full_like(input_ids, _IGNORE_INDEX)\n",
    "            labels[:, -answer_ids.shape[1]:] = answer_ids\n",
    "        else:\n",
    "            input_ids = inputs.input_ids\n",
    "            labels = answer_ids\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'input_audio_embeds': inputs.input_audio_embeds,\n",
    "            'audio_embed_sizes': inputs.audio_embed_sizes,\n",
    "        }\n",
    "\n",
    "def pad_sequence(sequences, padding_side='right', padding_value=0):\n",
    "    \"\"\"\n",
    "    Pad a list of sequences to the same length.\n",
    "    sequences: list of tensors in [seq_len, *] shape\n",
    "    \"\"\"\n",
    "    assert padding_side in ['right', 'left']\n",
    "    max_size = sequences[0].size()\n",
    "    trailing_dims = max_size[1:]\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    batch_size = len(sequences)\n",
    "    output = sequences[0].new_full((batch_size, max_len) + trailing_dims, padding_value)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = seq.size(0)\n",
    "        if padding_side == 'right':\n",
    "            output.data[i, :length] = seq\n",
    "        else:\n",
    "            output.data[i, -length:] = seq\n",
    "    return output\n",
    "\n",
    "\n",
    "def cat_with_pad(tensors, dim, padding_value=0):\n",
    "    \"\"\"\n",
    "    cat along dim, while pad to max for all other dims\n",
    "    \"\"\"\n",
    "    ndim = tensors[0].dim()\n",
    "    assert all(\n",
    "        t.dim() == ndim for t in tensors[1:]\n",
    "    ), 'All tensors must have the same number of dimensions'\n",
    "\n",
    "    out_size = [max(t.shape[i] for t in tensors) for i in range(ndim)]\n",
    "    out_size[dim] = sum(t.shape[dim] for t in tensors)\n",
    "    output = tensors[0].new_full(out_size, padding_value)\n",
    "\n",
    "    index = 0\n",
    "    for t in tensors:\n",
    "        # Create a slice list where every dimension except dim is full slice\n",
    "        slices = [slice(0, t.shape[d]) for d in range(ndim)]\n",
    "        # Update only the concat dimension slice\n",
    "        slices[dim] = slice(index, index + t.shape[dim])\n",
    "\n",
    "        output[slices] = t\n",
    "        index += t.shape[dim]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def covost_collate_fn(batch):\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    input_audio_embeds_list = []\n",
    "    audio_embed_sizes_list = []\n",
    "    audio_attention_mask_list = []\n",
    "    for inputs in batch:\n",
    "        input_ids_list.append(inputs['input_ids'][0])\n",
    "        labels_list.append(inputs['labels'][0])\n",
    "        input_audio_embeds_list.append(inputs['input_audio_embeds'])\n",
    "        audio_embed_sizes_list.append(inputs['audio_embed_sizes'])\n",
    "        audio_attention_mask_list.append(\n",
    "            inputs['input_audio_embeds'].new_full((inputs['input_audio_embeds'].size(1),), True, dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        input_ids = pad_sequence(input_ids_list, padding_side='left', padding_value=0)\n",
    "        labels = pad_sequence(labels_list, padding_side='left', padding_value=0)\n",
    "        audio_attention_mask = (\n",
    "            pad_sequence(audio_attention_mask_list, padding_side='right', padding_value=False)\n",
    "            if len(audio_attention_mask_list) > 1\n",
    "            else None\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(input_ids_list)\n",
    "        print(labels_list)\n",
    "        raise\n",
    "    attention_mask = (input_ids != 0).long()\n",
    "    input_audio_embeds = cat_with_pad(input_audio_embeds_list, dim=0)\n",
    "    audio_embed_sizes = torch.cat(audio_embed_sizes_list)\n",
    "\n",
    "    return BatchFeature(\n",
    "        {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "            'input_audio_embeds': input_audio_embeds,\n",
    "            'audio_embed_sizes': audio_embed_sizes,\n",
    "            'audio_attention_mask': audio_attention_mask,\n",
    "            'input_mode': 2,  # speech mode\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d9665",
   "metadata": {},
   "source": [
    "## Model Training Pipeline using smaller version of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61617241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/transformers/models/auto/image_processing_auto.py:604: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "/home/kelechi/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.45it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.29 GiB. GPU 0 has a total capacity of 23.65 GiB of which 1.70 GiB is free. Process 190848 has 21.56 GiB memory in use. Including non-PyTorch memory, this process has 384.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 183\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLEU Score after finetuning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 183\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 114\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39mlocal_main_process_first():\n\u001b[1;32m    110\u001b[0m     processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    111\u001b[0m         args\u001b[38;5;241m.\u001b[39mmodel_name_or_path,\n\u001b[1;32m    112\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    113\u001b[0m     )\n\u001b[0;32m--> 114\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m model\u001b[38;5;241m.\u001b[39mset_lora_adapter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeech\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Preprocess the dataset\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name_or_path, use_flash_attention)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_model\u001b[39m(model_name_or_path, use_flash_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_flash_attention\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_attn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflash_attention_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_flash_attention\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msdpa\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/transformers/modeling_utils.py:3698\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3694\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3695\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3696\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3697\u001b[0m         )\n\u001b[0;32m-> 3698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.29 GiB. GPU 0 has a total capacity of 23.65 GiB of which 1.70 GiB is free. Process 190848 has 21.56 GiB memory in use. Including non-PyTorch memory, this process has 384.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def create_model(model_name_or_path, use_flash_attention=False):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        torch_dtype=torch.bfloat16 if use_flash_attention else torch.float32,\n",
    "        _attn_implementation='flash_attention_2' if use_flash_attention else 'sdpa',\n",
    "        trust_remote_code=True,\n",
    "    ).to('cuda')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model, processor, eval_dataset, save_path=None, disable_tqdm=False, eval_batch_size=1\n",
    "):\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "\n",
    "    model.eval()\n",
    "    all_generated_texts = []\n",
    "    all_labels = []\n",
    "\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=eval_batch_size,\n",
    "        collate_fn=covost_collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=8,\n",
    "        prefetch_factor=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    stop_tokens = [\"<|end|>\", processor.tokenizer.eos_token]\n",
    "    stop_tokens_ids = processor.tokenizer(stop_tokens, add_special_tokens=False, padding=\"longest\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "    stop_tokens_ids = stop_tokens_ids.to(f'cuda:{local_rank}')\n",
    "\n",
    "    for inputs in tqdm(\n",
    "        eval_dataloader, disable=(rank != 0) or disable_tqdm, desc='running eval'\n",
    "    ):\n",
    "        stopping_criteria=StoppingCriteriaList([MultipleTokenBatchStoppingCriteria(stop_tokens_ids, batch_size=inputs.input_ids.size(0))])\n",
    "        inputs = inputs.to(f'cuda:{local_rank}')\n",
    "        generated_ids = model.generate(\n",
    "            **inputs, eos_token_id=processor.tokenizer.eos_token_id, max_new_tokens=64,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "        )\n",
    "\n",
    "        stop_tokens_idx = stopping_criteria[0].stop_tokens_idx.reshape(inputs.input_ids.size(0), -1)[:, 0]\n",
    "\n",
    "        stop_tokens_idx = torch.where(\n",
    "            stop_tokens_idx > 0,\n",
    "            stop_tokens_idx - stop_tokens_ids.shape[-1],\n",
    "            generated_ids.shape[-1],\n",
    "        )\n",
    "        generated_text = [\n",
    "            processor.decode(_pred_ids[inputs[\"input_ids\"].shape[1] : _stop_tokens_idx], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "            for _pred_ids, _stop_tokens_idx in zip(generated_ids, stop_tokens_idx)\n",
    "        ]\n",
    "        all_generated_texts.extend(generated_text)\n",
    "        labels = [processor.decode(_label_ids[_label_ids != 0]).removesuffix(ANSWER_SUFFIX) for _label_ids in inputs[\"labels\"]]\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    all_generated_texts = gather_object(all_generated_texts)\n",
    "    all_labels = gather_object(all_labels)\n",
    "    \n",
    "    if rank == 0:\n",
    "        assert len(all_generated_texts) == len(all_labels)\n",
    "        bleu = sacrebleu.corpus_bleu(all_generated_texts, [all_labels])\n",
    "        print(bleu)\n",
    "        if save_path:\n",
    "            with open(save_path, 'w') as f:\n",
    "                save_dict = {\n",
    "                    'all_generated_texts': all_generated_texts,\n",
    "                    'all_labels': all_labels,\n",
    "                    'score': bleu.score,\n",
    "                }\n",
    "                json.dump(save_dict, f)\n",
    "\n",
    "        return bleu.score\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    import os\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Suppress tokenizer parallelism warning\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--model_name_or_path',\n",
    "        type=str,\n",
    "        default='microsoft/Phi-4-multimodal-instruct',\n",
    "        help='Model name or path to load from',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lang\",\n",
    "        type=str,\n",
    "        default=\"en_sl\",\n",
    "        help=\"Language pair for translation.\",\n",
    "    )\n",
    "    parser.add_argument('--output_dir', type=str, default='./output/', help='Output directory')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')\n",
    "    parser.add_argument('--batch_size_per_gpu', type=int, default=32, help='Batch size per GPU')\n",
    "    parser.add_argument('--num_train_epochs', type=int, default=1, help='Number of training epochs')\n",
    "    parser.add_argument('--learning_rate', type=float, default=4.0e-5, help='Learning rate')\n",
    "    parser.add_argument('--wd', type=float, default=0.01, help='Weight decay')\n",
    "    parser.add_argument('--no-tqdm', dest='tqdm', action='store_false', help='Disable tqdm')\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    with accelerator.local_main_process_first():\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model = create_model(\n",
    "            args.model_name_or_path,\n",
    "        )\n",
    "\n",
    "    model.set_lora_adapter('speech')\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    validated_path = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/validated.tsv\"\n",
    "    base_audio_dir = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/clips\"\n",
    "    hf_dataset = preprocess_dataset(validated_path, base_audio_dir)\n",
    "\n",
    "    # Split the dataset into training and evaluation sets\n",
    "    train_size = int(0.8 * len(hf_dataset))\n",
    "    eval_size = len(hf_dataset) - train_size\n",
    "    train_dataset_raw, eval_dataset_raw = hf_dataset.train_test_split(\n",
    "        test_size=eval_size / len(hf_dataset)\n",
    "    ).values()\n",
    "\n",
    "    # Create CoVoSTDataset instances\n",
    "    train_dataset = CoVoSTDataset(processor, train_dataset_raw, training=True, lang=args.lang)\n",
    "    eval_dataset = CoVoSTDataset(processor, eval_dataset_raw, training=False, lang=args.lang)\n",
    "\n",
    "    num_gpus = accelerator.num_processes\n",
    "    print(f'training on {num_gpus} GPUs')\n",
    "    gradient_accumulation_steps = args.batch_size // (num_gpus * args.batch_size_per_gpu)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        per_device_train_batch_size=args.batch_size_per_gpu,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.wd,\n",
    "        output_dir=args.output_dir,\n",
    "        save_strategy='no',\n",
    "        remove_unused_columns=False,\n",
    "        disable_tqdm=True,\n",
    "    )\n",
    "\n",
    "    # Evaluate before fine-tuning\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        eval_batch_size=args.batch_size_per_gpu,\n",
    "    )\n",
    "    print(f'BLEU Score before finetuning: {score}')\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=covost_collate_fn,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "\n",
    "    # Evaluate after fine-tuning\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        eval_batch_size=args.batch_size_per_gpu,\n",
    "    )\n",
    "    print(f'BLEU Score after finetuning: {score}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa16563",
   "metadata": {},
   "source": [
    "## Verify issues with datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bc4bd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All audio files are present and valid.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def verify_audio_paths(validated_path, base_audio_dir):\n",
    "    \"\"\"\n",
    "    Verify the existence of audio files specified in the dataset.\n",
    "\n",
    "    Args:\n",
    "        validated_path (str): Path to the validated.tsv file.\n",
    "        base_audio_dir (str): Base directory containing the audio files.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load the dataset using Pandas\n",
    "    if not os.path.exists(validated_path):\n",
    "        print(f\"Error: The file {validated_path} does not exist.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(validated_path, sep=\"\\t\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # Check if the 'path' column exists\n",
    "    if \"path\" not in df.columns:\n",
    "        print(\"Error: The 'path' column is missing in the dataset.\")\n",
    "        return\n",
    "\n",
    "    # Verify each audio file\n",
    "    missing_files = []\n",
    "    for idx, row in df.iterrows():\n",
    "        audio_file_path = os.path.join(base_audio_dir, row[\"path\"])\n",
    "        if not os.path.exists(audio_file_path):\n",
    "            missing_files.append(audio_file_path)\n",
    "\n",
    "    # Report results\n",
    "    if missing_files:\n",
    "        print(f\"Found {len(missing_files)} missing audio files:\")\n",
    "        for file in missing_files[:10]:  # Show only the first 10 missing files\n",
    "            print(f\"  - {file}\")\n",
    "        print(\"...\")\n",
    "    else:\n",
    "        print(\"All audio files are present and valid.\")\n",
    "\n",
    "# Example usage\n",
    "validated_path = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/validated.tsv\"\n",
    "base_audio_dir = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/clips\"\n",
    "\n",
    "verify_audio_paths(validated_path, base_audio_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a9cc1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully with 249 rows.\n",
      "Loading audio files...\n",
      "All audio files loaded successfully.\n",
      "Converting to Hugging Face Dataset...\n",
      "Audio loading verification complete.\n",
      "First few rows of the dataset:\n",
      "                                           client_id  \\\n",
      "0  116398939d6be70fc5fb532924a130c0adf286ac283499...   \n",
      "1  24a4da2e8f053a45a0715849c222a40a4b0da9872efb2e...   \n",
      "2  30849595699bc853c3810a78448acede46888b4e2d0809...   \n",
      "3  42d53f34c1bc50f7a7c4ed1765a8d1ffeaf5cd441513cc...   \n",
      "4  436b9e1f9da710d74eb01209f8f269bee70e93cadf2053...   \n",
      "\n",
      "                           path  \\\n",
      "0  common_voice_en_41923025.mp3   \n",
      "1  common_voice_en_42356358.mp3   \n",
      "2  common_voice_en_42165090.mp3   \n",
      "3  common_voice_en_41921729.mp3   \n",
      "4  common_voice_en_42528393.mp3   \n",
      "\n",
      "                                         sentence_id  \\\n",
      "0  f5a2a431746c5229ab696ba0e1a518fe7b26e208ff3b84...   \n",
      "1  f6f009587d8812c147af1cc05079e1fcd8120c8a98cdf8...   \n",
      "2  f69afa5e77812e8be0085c874d2a9767323c78ffb43ba6...   \n",
      "3  f5739acbefdbd3aac990792966fac4d40dcb39eb8dfa21...   \n",
      "4  f7d35c60d76f025c45a9495757d1ee0e2b7c206317a288...   \n",
      "\n",
      "                                            sentence sentence_domain  \\\n",
      "0  He was born at Wichenford, in Worcestershire, ...             NaN   \n",
      "1  The Portuguese division was overrun and withdr...             NaN   \n",
      "2            Her health by this stage was also poor.             NaN   \n",
      "3  His sporting interests outside of cricket incl...             NaN   \n",
      "4  The following year he was elected to be part o...             NaN   \n",
      "\n",
      "   up_votes  down_votes       age gender                accents  variant  \\\n",
      "0         2           0  thirties    NaN  United States English      NaN   \n",
      "1         2           0     teens    NaN  United States English      NaN   \n",
      "2         2           0       NaN    NaN                    NaN      NaN   \n",
      "3         2           0  nineties    NaN        England English      NaN   \n",
      "4         2           0     teens    NaN  United States English      NaN   \n",
      "\n",
      "  locale  segment                                              audio  \n",
      "0     en      NaN  {'array': [0.0, 2.9239527e-12, -4.657857e-13, ...  \n",
      "1     en      NaN  {'array': [0.0, 1.8414684e-12, 2.838348e-12, 1...  \n",
      "2     en      NaN  {'array': [0.0, -1.6061019e-14, 5.6447285e-14,...  \n",
      "3     en      NaN  {'array': [0.0, -2.6382298e-13, -4.938348e-13,...  \n",
      "4     en      NaN  {'array': [0.0, 8.247058e-12, 5.176088e-12, 1....  \n",
      "\n",
      "Columns in the dataset:\n",
      "['client_id', 'path', 'sentence_id', 'sentence', 'sentence_domain', 'up_votes', 'down_votes', 'age', 'gender', 'accents', 'variant', 'locale', 'segment', 'audio']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from datasets import Dataset\n",
    "\n",
    "def verify_audio_loading(validated_path, base_audio_dir):\n",
    "    \"\"\"\n",
    "    Verify the audio loading logic and structure of the audio field.\n",
    "\n",
    "    Args:\n",
    "        validated_path (str): Path to the validated.tsv file.\n",
    "        base_audio_dir (str): Base directory containing the audio files.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Step 1: Load the dataset using Pandas\n",
    "    if not os.path.exists(validated_path):\n",
    "        print(f\"Error: The file {validated_path} does not exist.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(validated_path, sep=\"\\t\")\n",
    "        print(f\"Dataset loaded successfully with {len(df)} rows.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Check if the 'path' column exists\n",
    "    if \"path\" not in df.columns:\n",
    "        print(\"Error: The 'path' column is missing in the dataset.\")\n",
    "        return\n",
    "\n",
    "    # Step 3: Verify audio loading for each row\n",
    "    def load_audio(row):\n",
    "        try:\n",
    "            # Construct the full path to the audio file\n",
    "            audio_file_path = os.path.join(base_audio_dir, row[\"path\"])\n",
    "            waveform, sampling_rate = torchaudio.load(audio_file_path)\n",
    "            return {\"array\": waveform.squeeze().numpy(), \"sampling_rate\": sampling_rate}\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio for file {row['path']}: {e}\")\n",
    "            return {\"array\": None, \"sampling_rate\": None}\n",
    "\n",
    "    # Step 4: Apply the audio loading logic\n",
    "    print(\"Loading audio files...\")\n",
    "    df[\"audio\"] = df.apply(load_audio, axis=1)\n",
    "\n",
    "    # Step 5: Check for invalid audio fields\n",
    "    invalid_audio_rows = df[df[\"audio\"].apply(lambda x: x[\"array\"] is None or x[\"sampling_rate\"] is None)]\n",
    "    if not invalid_audio_rows.empty:\n",
    "        print(f\"Found {len(invalid_audio_rows)} rows with invalid audio fields:\")\n",
    "        print(invalid_audio_rows[[\"path\", \"audio\"]].head(10))  # Show the first 10 invalid rows\n",
    "    else:\n",
    "        print(\"All audio files loaded successfully.\")\n",
    "\n",
    "    # Step 6: Convert to Hugging Face Dataset and verify structure\n",
    "    print(\"Converting to Hugging Face Dataset...\")\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "\n",
    "    for idx, row in enumerate(hf_dataset):\n",
    "        if not isinstance(row[\"audio\"], dict) or \"array\" not in row[\"audio\"] or \"sampling_rate\" not in row[\"audio\"]:\n",
    "            print(f\"Invalid audio field at index {idx}: {row['audio']}\")\n",
    "\n",
    "    print(\"Audio loading verification complete.\")\n",
    "\n",
    "# print the first few rows of the dataset\n",
    "    print(\"First few rows of the dataset:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Print the column names\n",
    "    print(\"\\nColumns in the dataset:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "validated_path = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/validated.tsv\"\n",
    "base_audio_dir = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/clips\"\n",
    "\n",
    "verify_audio_loading(validated_path, base_audio_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%pip install torchaudio\n",
    "\n",
    "from datasets import Dataset\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "INSTSRUCTION = {\n",
    "    \"en_zh-CN\": \"Translate the audio to Mandarin.\",\n",
    "    \"en_id\": \"Translate the audio to Indonesian.\",\n",
    "    \"en_sl\": \"Translate the audio to Slovenian.\",\n",
    "}\n",
    "TOKENIZER = {\n",
    "    \"en_zh-CN\": \"zh\",\n",
    "    \"en_ja\": \"ja-mecab\",\n",
    "}\n",
    "ANSWER_SUFFIX = \"<|end|><|endoftext|>\"\n",
    "_IGNORE_INDEX = -100\n",
    "_TRAIN_SIZE = 50000\n",
    "_EVAL_SIZE = 200\n",
    "\n",
    "class MultipleTokenBatchStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"Stopping criteria capable of receiving multiple stop-tokens and handling batched inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, stop_tokens: torch.LongTensor, batch_size: int = 1) -> None:\n",
    "        \"\"\"Initialize the multiple token batch stopping criteria.\n",
    "\n",
    "        Args:\n",
    "            stop_tokens: Stop-tokens.\n",
    "            batch_size: Batch size.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.stop_tokens = stop_tokens\n",
    "        self.max_stop_tokens = stop_tokens.shape[-1]\n",
    "        self.stop_tokens_idx = torch.zeros(batch_size, dtype=torch.long, device=stop_tokens.device)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # Only gather the maximum number of inputs compatible with stop tokens\n",
    "        # and checks whether generated inputs are equal to `stop_tokens`\n",
    "        generated_inputs = torch.eq(input_ids[:, -self.max_stop_tokens :].unsqueeze(1), self.stop_tokens)\n",
    "        equal_generated_inputs = torch.all(generated_inputs, dim=2)\n",
    "\n",
    "        # Mark the position where a stop token has been produced for each input in the batch,\n",
    "        # but only if the corresponding entry is not already set\n",
    "        sequence_idx = torch.any(equal_generated_inputs, dim=1)\n",
    "        sequence_set_mask = self.stop_tokens_idx == 0\n",
    "        self.stop_tokens_idx[sequence_idx & sequence_set_mask] = input_ids.shape[-1]\n",
    "\n",
    "        return torch.all(self.stop_tokens_idx)\n",
    "\n",
    "def preprocess_dataset(validated_path, base_audio_dir):\n",
    "    # Load the dataset using Pandas\n",
    "    df = pd.read_csv(validated_path, sep=\"\\t\")\n",
    "\n",
    "    # Print column names to verify structure\n",
    "    print(\"Columns in the dataset:\", df.columns)\n",
    "\n",
    "    # Add audio data to the dataset\n",
    "    def load_audio(row):\n",
    "        try:\n",
    "            # Construct the full path to the audio file\n",
    "            audio_file_path = os.path.join(base_audio_dir, row[\"path\"])\n",
    "            waveform, sampling_rate = torchaudio.load(audio_file_path)\n",
    "            return {\"array\": waveform.squeeze().numpy(), \"sampling_rate\": sampling_rate}\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio for file {row['path']}: {e}\")\n",
    "            return {\"array\": None, \"sampling_rate\": None}\n",
    "\n",
    "    # Ensure the \"path\" column exists\n",
    "    if \"path\" not in df.columns:\n",
    "        raise KeyError(\"The 'path' column is missing in the dataset. Please check the dataset structure.\")\n",
    "\n",
    "    df[\"audio\"] = df.apply(load_audio, axis=1)\n",
    "\n",
    "    # Filter out rows where audio could not be loaded\n",
    "    df = df[df[\"audio\"].apply(lambda x: x[\"array\"] is not None)]\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "    return hf_dataset\n",
    "\n",
    "\n",
    "class CoVoSTDataset(Dataset):\n",
    "    def __init__(self, processor, dataset, training=True, lang=\"en_zh-CN\"):\n",
    "        self.dataset = dataset  # Renamed from self.data to self.dataset\n",
    "        self.training = training\n",
    "        self.processor = processor\n",
    "        self.instruction = INSTSRUCTION[lang]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        user_message = {\n",
    "            'role': 'user',\n",
    "            'content': '<|audio_1|>\\n' + self.instruction,\n",
    "        }\n",
    "        prompt = self.processor.tokenizer.apply_chat_template(\n",
    "            [user_message], tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = self.processor(\n",
    "            text=prompt,\n",
    "            audios=[(data[\"path\"][\"array\"], data[\"path\"][\"sampling_rate\"])],\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        answer = f\"{data['translation']}{ANSWER_SUFFIX}\"\n",
    "        answer_ids = self.processor.tokenizer(answer, return_tensors='pt').input_ids\n",
    "        if self.training:\n",
    "            input_ids = torch.cat([inputs.input_ids, answer_ids], dim=1)\n",
    "            labels = torch.full_like(input_ids, _IGNORE_INDEX)\n",
    "            labels[:, -answer_ids.shape[1]:] = answer_ids\n",
    "        else:\n",
    "            input_ids = inputs.input_ids\n",
    "            labels = answer_ids\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'input_audio_embeds': inputs.input_audio_embeds,\n",
    "            'audio_embed_sizes': inputs.audio_embed_sizes,\n",
    "        }\n",
    "\n",
    "def pad_sequence(sequences, padding_side='right', padding_value=0):\n",
    "    \"\"\"\n",
    "    Pad a list of sequences to the same length.\n",
    "    sequences: list of tensors in [seq_len, *] shape\n",
    "    \"\"\"\n",
    "    assert padding_side in ['right', 'left']\n",
    "    max_size = sequences[0].size()\n",
    "    trailing_dims = max_size[1:]\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    batch_size = len(sequences)\n",
    "    output = sequences[0].new_full((batch_size, max_len) + trailing_dims, padding_value)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = seq.size(0)\n",
    "        if padding_side == 'right':\n",
    "            output.data[i, :length] = seq\n",
    "        else:\n",
    "            output.data[i, -length:] = seq\n",
    "    return output\n",
    "\n",
    "\n",
    "def cat_with_pad(tensors, dim, padding_value=0):\n",
    "    \"\"\"\n",
    "    cat along dim, while pad to max for all other dims\n",
    "    \"\"\"\n",
    "    ndim = tensors[0].dim()\n",
    "    assert all(\n",
    "        t.dim() == ndim for t in tensors[1:]\n",
    "    ), 'All tensors must have the same number of dimensions'\n",
    "\n",
    "    out_size = [max(t.shape[i] for t in tensors) for i in range(ndim)]\n",
    "    out_size[dim] = sum(t.shape[dim] for t in tensors)\n",
    "    output = tensors[0].new_full(out_size, padding_value)\n",
    "\n",
    "    index = 0\n",
    "    for t in tensors:\n",
    "        # Create a slice list where every dimension except dim is full slice\n",
    "        slices = [slice(0, t.shape[d]) for d in range(ndim)]\n",
    "        # Update only the concat dimension slice\n",
    "        slices[dim] = slice(index, index + t.shape[dim])\n",
    "\n",
    "        output[slices] = t\n",
    "        index += t.shape[dim]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def covost_collate_fn(batch):\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    input_audio_embeds_list = []\n",
    "    audio_embed_sizes_list = []\n",
    "    audio_attention_mask_list = []\n",
    "    for inputs in batch:\n",
    "        input_ids_list.append(inputs['input_ids'][0])\n",
    "        labels_list.append(inputs['labels'][0])\n",
    "        input_audio_embeds_list.append(inputs['input_audio_embeds'])\n",
    "        audio_embed_sizes_list.append(inputs['audio_embed_sizes'])\n",
    "        audio_attention_mask_list.append(\n",
    "            inputs['input_audio_embeds'].new_full((inputs['input_audio_embeds'].size(1),), True, dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        input_ids = pad_sequence(input_ids_list, padding_side='left', padding_value=0)\n",
    "        labels = pad_sequence(labels_list, padding_side='left', padding_value=0)\n",
    "        audio_attention_mask = (\n",
    "            pad_sequence(audio_attention_mask_list, padding_side='right', padding_value=False)\n",
    "            if len(audio_attention_mask_list) > 1\n",
    "            else None\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(input_ids_list)\n",
    "        print(labels_list)\n",
    "        raise\n",
    "    attention_mask = (input_ids != 0).long()\n",
    "    input_audio_embeds = cat_with_pad(input_audio_embeds_list, dim=0)\n",
    "    audio_embed_sizes = torch.cat(audio_embed_sizes_list)\n",
    "\n",
    "    return BatchFeature(\n",
    "        {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "            'input_audio_embeds': input_audio_embeds,\n",
    "            'audio_embed_sizes': audio_embed_sizes,\n",
    "            'audio_attention_mask': audio_attention_mask,\n",
    "            'input_mode': 2,  # speech mode\n",
    "        }\n",
    "    )\n",
    "\n",
    "How d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "264634cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio loaded successfully! Sampling rate: 32000\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import os\n",
    "\n",
    "audio_file_path = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/clips/common_voice_en_41923025.mp3\"\n",
    "\n",
    "try:\n",
    "    waveform, sampling_rate = torchaudio.load(audio_file_path)\n",
    "    print(f\"Audio loaded successfully! Sampling rate: {sampling_rate}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading audio: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ee091ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: Index(['client_id', 'path', 'sentence_id', 'sentence', 'sentence_domain',\n",
      "       'up_votes', 'down_votes', 'age', 'gender', 'accents', 'variant',\n",
      "       'locale', 'segment'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "validated_path = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/validated.tsv\"\n",
    "base_audio_dir = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/clips\"\n",
    "\n",
    "\n",
    "hf_dataset = preprocess_dataset(validated_path, base_audio_dir)\n",
    "for idx, row in enumerate(hf_dataset):\n",
    "    if not isinstance(row[\"audio\"], dict) or \"array\" not in row[\"audio\"] or \"sampling_rate\" not in row[\"audio\"]:\n",
    "        print(f\"Invalid audio field at index {idx}: {row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c001b9f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mprocessor(\n\u001b[1;32m      2\u001b[0m     text\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m      3\u001b[0m     audios\u001b[38;5;241m=\u001b[39m[(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampling_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m])],\n\u001b[1;32m      4\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = self.processor(\n",
    "    text=prompt,\n",
    "    audios=[(data[\"audio\"][\"array\"], data[\"audio\"][\"sampling_rate\"])],\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_ramp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
