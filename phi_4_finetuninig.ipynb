{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ee43e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting backoff\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: backoff\n",
      "Successfully installed backoff-2.2.1\n"
     ]
    }
   ],
   "source": [
    "# Requirments:\n",
    "# pip install requests\n",
    "#!pip install torch\n",
    "#!pip install sacrebleu\n",
    "#!pip install accelerate\n",
    "#!pip install datasets\n",
    "#!pip install transformers\n",
    "#!pip install scipy torchvision\n",
    "#!pip install peft\n",
    "!pip install backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cca5eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "finetune Phi-4-multimodal-instruct on an speech task\n",
    "\n",
    "scipy==1.15.1\n",
    "peft==0.13.2\n",
    "backoff==2.2.1\n",
    "transformers==4.46.1\n",
    "accelerate==1.3.0\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import sacrebleu\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    BatchFeature,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5650c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-multimodal-instruct:\n",
      "- speech_conformer_encoder.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Fetching 3 files: 100%|██████████| 3/3 [01:35<00:00, 31.91s/it]\n",
      "/home/kelechi/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.28it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "You are trying to load the en_sl speech translation dataset. It is required that you manually download the input speech data en. Manual download instructions: Please download the Common Voice Corpus 4 in en from https://commonvoice.mozilla.org/en/datasets and unpack it with `tar xvzf en.tar`. Make sure to pass the path to the directory in which you unpacked the downloaded file as `data_dir`: `datasets.load_dataset('covost2', data_dir=\"path/to/dir\")`\n        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 445\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLEU Score after finetuning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 445\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 332\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    329\u001b[0m rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRANK\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    330\u001b[0m world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWORLD_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 332\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCoVoSTDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommon_voice_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m                             \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest[:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43m_EVAL_SIZE\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m CoVoSTDataset(processor,\n\u001b[1;32m    340\u001b[0m                               data_dir\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mcommon_voice_dir,\n\u001b[1;32m    341\u001b[0m                               split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain[:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_TRAIN_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    342\u001b[0m                               lang\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlang)\n\u001b[1;32m    344\u001b[0m num_gpus \u001b[38;5;241m=\u001b[39m accelerator\u001b[38;5;241m.\u001b[39mnum_processes\n",
      "Cell \u001b[0;32mIn[26], line 49\u001b[0m, in \u001b[0;36mCoVoSTDataset.__init__\u001b[0;34m(self, processor, data_dir, split, lang, rank, world_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, processor, data_dir, split, \n\u001b[1;32m     47\u001b[0m              lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_zh-CN\u001b[39m\u001b[38;5;124m\"\u001b[39m, rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, world_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/covost2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                       \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m                       \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m split\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor \u001b[38;5;241m=\u001b[39m processor\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/datasets/load.py:2084\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2084\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2094\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2095\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/datasets/builder.py:925\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    924\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/datasets/builder.py:1649\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1649\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_duplicate_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBASIC_CHECKS\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_CHECKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_splits_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/datasets/builder.py:979\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    977\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m SplitDict(dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name)\n\u001b[1;32m    978\u001b[0m split_generators_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[0;32m--> 979\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msplit_generators_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# Checksums verification\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS \u001b[38;5;129;01mand\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39mrecord_checksums:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/facebook--covost2/bba950aae1ffa5a14b876b7e09c17b44de2c3cf60e7bd5d459640beffc78e35b/covost2.py:115\u001b[0m, in \u001b[0;36mCovost2._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    112\u001b[0m source_lang, target_lang \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(data_root):\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to load the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m speech translation dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is required that you manually download the input speech data \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_lang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mManual download instructions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m covost_url \u001b[38;5;241m=\u001b[39m COVOST_URL_TEMPLATE\u001b[38;5;241m.\u001b[39mformat(src_lang\u001b[38;5;241m=\u001b[39msource_lang, tgt_lang\u001b[38;5;241m=\u001b[39mtarget_lang)\n\u001b[1;32m    122\u001b[0m extracted_path \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownload_and_extract(covost_url)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: You are trying to load the en_sl speech translation dataset. It is required that you manually download the input speech data en. Manual download instructions: Please download the Common Voice Corpus 4 in en from https://commonvoice.mozilla.org/en/datasets and unpack it with `tar xvzf en.tar`. Make sure to pass the path to the directory in which you unpacked the downloaded file as `data_dir`: `datasets.load_dataset('covost2', data_dir=\"path/to/dir\")`\n        "
     ]
    }
   ],
   "source": [
    "INSTSRUCTION = {\n",
    "    \"en_zh-CN\": \"Translate the audio to Mandarin.\",\n",
    "    \"en_id\": \"Translate the audio to Indonesian.\",\n",
    "    \"en_sl\": \"Translate the audio to Slovenian.\",\n",
    "}\n",
    "TOKENIZER = {\n",
    "    \"en_zh-CN\": \"zh\",\n",
    "    \"en_ja\": \"ja-mecab\",\n",
    "}\n",
    "ANSWER_SUFFIX = \"<|end|><|endoftext|>\"\n",
    "_IGNORE_INDEX = -100\n",
    "_TRAIN_SIZE = 50000\n",
    "_EVAL_SIZE = 200\n",
    "\n",
    "class MultipleTokenBatchStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"Stopping criteria capable of receiving multiple stop-tokens and handling batched inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, stop_tokens: torch.LongTensor, batch_size: int = 1) -> None:\n",
    "        \"\"\"Initialize the multiple token batch stopping criteria.\n",
    "\n",
    "        Args:\n",
    "            stop_tokens: Stop-tokens.\n",
    "            batch_size: Batch size.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.stop_tokens = stop_tokens\n",
    "        self.max_stop_tokens = stop_tokens.shape[-1]\n",
    "        self.stop_tokens_idx = torch.zeros(batch_size, dtype=torch.long, device=stop_tokens.device)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # Only gather the maximum number of inputs compatible with stop tokens\n",
    "        # and checks whether generated inputs are equal to `stop_tokens`\n",
    "        generated_inputs = torch.eq(input_ids[:, -self.max_stop_tokens :].unsqueeze(1), self.stop_tokens)\n",
    "        equal_generated_inputs = torch.all(generated_inputs, dim=2)\n",
    "\n",
    "        # Mark the position where a stop token has been produced for each input in the batch,\n",
    "        # but only if the corresponding entry is not already set\n",
    "        sequence_idx = torch.any(equal_generated_inputs, dim=1)\n",
    "        sequence_set_mask = self.stop_tokens_idx == 0\n",
    "        self.stop_tokens_idx[sequence_idx & sequence_set_mask] = input_ids.shape[-1]\n",
    "\n",
    "        return torch.all(self.stop_tokens_idx)\n",
    "\n",
    "class CoVoSTDataset(Dataset):\n",
    "    def __init__(self, processor, data_dir, split, \n",
    "                 lang=\"en_zh-CN\", rank=0, world_size=1):\n",
    "\n",
    "        self.data = load_dataset(\"facebook/covost2\", \n",
    "                           lang, \n",
    "                           data_dir=data_dir, \n",
    "                           split=split,\n",
    "                           trust_remote_code=True\n",
    "                           )\n",
    "        self.training = \"train\" in split\n",
    "        self.processor = processor\n",
    "        self.instruction = INSTSRUCTION[lang]\n",
    "        \n",
    "        if world_size > 1:\n",
    "            self.data = self.data.shard(world_size, rank) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        {'client_id': '0013037a1d45cc33460806cc3f8ecee9d536c45639ba4cbbf1564f1c051f53ff3c9f89ef2f1bf04badf55b3a2e7654c086f903681a7b6299616cff6f67598eff',\n",
    "        'file': '{data_dir}/clips/common_voice_en_699711.mp3',\n",
    "        'audio': {'path': '{data_dir}/clips/common_voice_en_699711.mp3',\n",
    "        'array': array([-1.28056854e-09, -1.74622983e-09, -1.16415322e-10, ...,\n",
    "                3.92560651e-10,  6.62794264e-10, -3.89536581e-09]),\n",
    "        'sampling_rate': 16000},\n",
    "        'sentence': '\"She\\'ll be all right.\"',\n",
    "        'translation': '她会没事的。',\n",
    "        'id': 'common_voice_en_699711'}\n",
    "        \"\"\"\n",
    "        data = self.data[idx]\n",
    "        user_message = {\n",
    "            'role': 'user',\n",
    "            'content': '<|audio_1|>\\n' + self.instruction,\n",
    "        }\n",
    "        prompt = self.processor.tokenizer.apply_chat_template(\n",
    "            [user_message], tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = self.processor(text=prompt, audios=[(data[\"audio\"][\"array\"], data[\"audio\"][\"sampling_rate\"])], return_tensors='pt')\n",
    "        \n",
    "        answer = f\"{data['translation']}{ANSWER_SUFFIX}\"\n",
    "        answer_ids = self.processor.tokenizer(answer, return_tensors='pt').input_ids\n",
    "        if  self.training:\n",
    "            input_ids = torch.cat([inputs.input_ids, answer_ids], dim=1)\n",
    "            labels = torch.full_like(input_ids, _IGNORE_INDEX)\n",
    "            labels[:, -answer_ids.shape[1] :] = answer_ids\n",
    "        else:\n",
    "            input_ids = inputs.input_ids\n",
    "            labels = answer_ids\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'input_audio_embeds': inputs.input_audio_embeds,\n",
    "            'audio_embed_sizes': inputs.audio_embed_sizes,\n",
    "        }\n",
    "\n",
    "def pad_sequence(sequences, padding_side='right', padding_value=0):\n",
    "    \"\"\"\n",
    "    Pad a list of sequences to the same length.\n",
    "    sequences: list of tensors in [seq_len, *] shape\n",
    "    \"\"\"\n",
    "    assert padding_side in ['right', 'left']\n",
    "    max_size = sequences[0].size()\n",
    "    trailing_dims = max_size[1:]\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    batch_size = len(sequences)\n",
    "    output = sequences[0].new_full((batch_size, max_len) + trailing_dims, padding_value)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = seq.size(0)\n",
    "        if padding_side == 'right':\n",
    "            output.data[i, :length] = seq\n",
    "        else:\n",
    "            output.data[i, -length:] = seq\n",
    "    return output\n",
    "\n",
    "\n",
    "def cat_with_pad(tensors, dim, padding_value=0):\n",
    "    \"\"\"\n",
    "    cat along dim, while pad to max for all other dims\n",
    "    \"\"\"\n",
    "    ndim = tensors[0].dim()\n",
    "    assert all(\n",
    "        t.dim() == ndim for t in tensors[1:]\n",
    "    ), 'All tensors must have the same number of dimensions'\n",
    "\n",
    "    out_size = [max(t.shape[i] for t in tensors) for i in range(ndim)]\n",
    "    out_size[dim] = sum(t.shape[dim] for t in tensors)\n",
    "    output = tensors[0].new_full(out_size, padding_value)\n",
    "\n",
    "    index = 0\n",
    "    for t in tensors:\n",
    "        # Create a slice list where every dimension except dim is full slice\n",
    "        slices = [slice(0, t.shape[d]) for d in range(ndim)]\n",
    "        # Update only the concat dimension slice\n",
    "        slices[dim] = slice(index, index + t.shape[dim])\n",
    "\n",
    "        output[slices] = t\n",
    "        index += t.shape[dim]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def covost_collate_fn(batch):\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    input_audio_embeds_list = []\n",
    "    audio_embed_sizes_list = []\n",
    "    audio_attention_mask_list = []\n",
    "    for inputs in batch:\n",
    "        input_ids_list.append(inputs['input_ids'][0])\n",
    "        labels_list.append(inputs['labels'][0])\n",
    "        input_audio_embeds_list.append(inputs['input_audio_embeds'])\n",
    "        audio_embed_sizes_list.append(inputs['audio_embed_sizes'])\n",
    "        audio_attention_mask_list.append(\n",
    "            inputs['input_audio_embeds'].new_full((inputs['input_audio_embeds'].size(1),), True, dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        input_ids = pad_sequence(input_ids_list, padding_side='left', padding_value=0)\n",
    "        labels = pad_sequence(labels_list, padding_side='left', padding_value=0)\n",
    "        audio_attention_mask = (\n",
    "            pad_sequence(audio_attention_mask_list, padding_side='right', padding_value=False)\n",
    "            if len(audio_attention_mask_list) > 1\n",
    "            else None\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(input_ids_list)\n",
    "        print(labels_list)\n",
    "        raise\n",
    "    attention_mask = (input_ids != 0).long()\n",
    "    input_audio_embeds = cat_with_pad(input_audio_embeds_list, dim=0)\n",
    "    audio_embed_sizes = torch.cat(audio_embed_sizes_list)\n",
    "\n",
    "    return BatchFeature(\n",
    "        {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "            'input_audio_embeds': input_audio_embeds,\n",
    "            'audio_embed_sizes': audio_embed_sizes,\n",
    "            'audio_attention_mask': audio_attention_mask,\n",
    "            'input_mode': 2,  # speech mode\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def create_model(model_name_or_path, use_flash_attention=False):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        torch_dtype=torch.bfloat16 if use_flash_attention else torch.float32,\n",
    "        _attn_implementation='flash_attention_2' if use_flash_attention else 'sdpa',\n",
    "        trust_remote_code=True,\n",
    "    ).to('cuda')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model, processor, eval_dataset, save_path=None, disable_tqdm=False, eval_batch_size=1\n",
    "):\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "\n",
    "    model.eval()\n",
    "    all_generated_texts = []\n",
    "    all_labels = []\n",
    "\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=eval_batch_size,\n",
    "        collate_fn=covost_collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=8,\n",
    "        prefetch_factor=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    stop_tokens = [\"<|end|>\", processor.tokenizer.eos_token]\n",
    "    stop_tokens_ids = processor.tokenizer(stop_tokens, add_special_tokens=False, padding=\"longest\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "    stop_tokens_ids = stop_tokens_ids.to(f'cuda:{local_rank}')\n",
    "\n",
    "    for inputs in tqdm(\n",
    "        eval_dataloader, disable=(rank != 0) or disable_tqdm, desc='running eval'\n",
    "    ):\n",
    "        stopping_criteria=StoppingCriteriaList([MultipleTokenBatchStoppingCriteria(stop_tokens_ids, batch_size=inputs.input_ids.size(0))])\n",
    "        inputs = inputs.to(f'cuda:{local_rank}')\n",
    "        generated_ids = model.generate(\n",
    "            **inputs, eos_token_id=processor.tokenizer.eos_token_id, max_new_tokens=64,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "        )\n",
    "\n",
    "        stop_tokens_idx = stopping_criteria[0].stop_tokens_idx.reshape(inputs.input_ids.size(0), -1)[:, 0]\n",
    "\n",
    "        stop_tokens_idx = torch.where(\n",
    "            stop_tokens_idx > 0,\n",
    "            stop_tokens_idx - stop_tokens_ids.shape[-1],\n",
    "            generated_ids.shape[-1],\n",
    "        )\n",
    "        generated_text = [\n",
    "            processor.decode(_pred_ids[inputs[\"input_ids\"].shape[1] : _stop_tokens_idx], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "            for _pred_ids, _stop_tokens_idx in zip(generated_ids, stop_tokens_idx)\n",
    "        ]\n",
    "        all_generated_texts.extend(generated_text)\n",
    "        labels = [processor.decode(_label_ids[_label_ids != 0]).removesuffix(ANSWER_SUFFIX) for _label_ids in inputs[\"labels\"]]\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    all_generated_texts = gather_object(all_generated_texts)\n",
    "    all_labels = gather_object(all_labels)\n",
    "    \n",
    "    if rank == 0:\n",
    "        assert len(all_generated_texts) == len(all_labels)\n",
    "        bleu = sacrebleu.corpus_bleu(all_generated_texts, [all_labels])\n",
    "        print(bleu)\n",
    "        if save_path:\n",
    "            with open(save_path, 'w') as f:\n",
    "                save_dict = {\n",
    "                    'all_generated_texts': all_generated_texts,\n",
    "                    'all_labels': all_labels,\n",
    "                    'score': bleu.score,\n",
    "                }\n",
    "                json.dump(save_dict, f)\n",
    "\n",
    "        return bleu.score\n",
    "    return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--model_name_or_path',\n",
    "        type=str,\n",
    "        default='microsoft/Phi-4-multimodal-instruct',\n",
    "        help='Model name or path to load from',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--common_voice_dir\",\n",
    "        type=str,\n",
    "        default=\"CommonVoice/EN\",\n",
    "        help=\"Unzipped Common Voice Audio dataset directory, refer to https://commonvoice.mozilla.org/en/datasets, version 4.0\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lang\",\n",
    "        type=str,\n",
    "        default=\"en_sl\",\n",
    "        help=\"Language pair for translation.\",\n",
    "    )\n",
    "    parser.add_argument('--use_flash_attention', action='store_true', help='Use Flash Attention')\n",
    "    parser.add_argument('--output_dir', type=str, default='./output/', help='Output directory')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')\n",
    "    parser.add_argument(\n",
    "        '--batch_size_per_gpu',\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help='Batch size per GPU (adjust this to fit in GPU memory)',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_train_epochs', type=int, default=1, help='Number of training epochs'\n",
    "    )\n",
    "    parser.add_argument('--learning_rate', type=float, default=4.0e-5, help='Learning rate')\n",
    "    parser.add_argument('--wd', type=float, default=0.01, help='Weight decay')\n",
    "    parser.add_argument('--no-tqdm', dest='tqdm', action='store_false', help='Disable tqdm')\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    with accelerator.local_main_process_first():\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model = create_model(\n",
    "            args.model_name_or_path,\n",
    "            use_flash_attention=args.use_flash_attention,\n",
    "        )\n",
    "\n",
    "    model.set_lora_adapter('speech')\n",
    "\n",
    "\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
    "\n",
    "    eval_dataset = CoVoSTDataset(processor,\n",
    "                                 data_dir=args.common_voice_dir,\n",
    "                                 split=f'test[:{_EVAL_SIZE}]',\n",
    "                                 lang=args.lang,\n",
    "                                 rank=rank,\n",
    "                                 world_size=world_size)\n",
    "    \n",
    "    train_dataset = CoVoSTDataset(processor,\n",
    "                                  data_dir=args.common_voice_dir,\n",
    "                                  split=f'train[:{_TRAIN_SIZE}]',\n",
    "                                  lang=args.lang)\n",
    "\n",
    "    num_gpus = accelerator.num_processes\n",
    "    print(f'training on {num_gpus} GPUs')\n",
    "    assert (\n",
    "        args.batch_size % (num_gpus * args.batch_size_per_gpu) == 0\n",
    "    ), 'Batch size must be divisible by the number of GPUs'\n",
    "    gradient_accumulation_steps = args.batch_size // (num_gpus * args.batch_size_per_gpu)\n",
    "\n",
    "    if args.use_flash_attention:\n",
    "        fp16 = False\n",
    "        bf16 = True\n",
    "    else:\n",
    "        fp16 = True\n",
    "        bf16 = False\n",
    "\n",
    "    # hard coded training args\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        per_device_train_batch_size=args.batch_size_per_gpu,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        optim='adamw_torch',\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.95,\n",
    "        adam_epsilon=1e-7,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.wd,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type='linear',\n",
    "        warmup_steps=50,\n",
    "        logging_steps=10,\n",
    "        output_dir=args.output_dir,\n",
    "        save_strategy='no',\n",
    "        save_total_limit=10,\n",
    "        save_only_model=True,\n",
    "        bf16=bf16,\n",
    "        fp16=fp16,\n",
    "        remove_unused_columns=False,\n",
    "        report_to='none',\n",
    "        deepspeed=None,\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        dataloader_num_workers=4,\n",
    "        ddp_find_unused_parameters=True,  # for unused SigLIP layers\n",
    "    )\n",
    "\n",
    "    # eval before fine-tuning\n",
    "    out_path = Path(training_args.output_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        save_path=out_path / 'eval_before.json',\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        eval_batch_size=args.batch_size_per_gpu,\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        print(f'BLEU Score before finetuning: {score}')\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=covost_collate_fn,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    if accelerator.is_main_process:\n",
    "        processor.save_pretrained(training_args.output_dir)\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    # eval after fine-tuning (load saved checkpoint)\n",
    "    # first try to clear GPU memory\n",
    "    del model\n",
    "    del trainer\n",
    "    __import__('gc').collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # reload the model for inference\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        training_args.output_dir,\n",
    "        torch_dtype=torch.bfloat16 if args.use_flash_attention else torch.float32,\n",
    "        trust_remote_code=True,\n",
    "        _attn_implementation='flash_attention_2' if args.use_flash_attention else 'sdpa',\n",
    "    ).to('cuda')\n",
    "\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        save_path=out_path / 'eval_after.json',\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        eval_batch_size=args.batch_size_per_gpu,\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        print(f'BLEU Score after finetuning: {score}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_ramp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
