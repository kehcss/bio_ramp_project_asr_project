{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bdf3686",
   "metadata": {},
   "source": [
    "## Pip Install Requirment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee43e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting backoff\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: backoff\n",
      "Successfully installed backoff-2.2.1\n"
     ]
    }
   ],
   "source": [
    "# Requirments:\n",
    "# pip install requests\n",
    "!pip install torch\n",
    "!pip install sacrebleu\n",
    "!pip install accelerate\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install scipy torchvision\n",
    "!pip install peft\n",
    "!pip install backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487ecb3e",
   "metadata": {},
   "source": [
    "## Load Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca5eeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "finetune Phi-4-multimodal-instruct on an speech task\n",
    "\n",
    "scipy==1.15.1\n",
    "peft==0.13.2\n",
    "backoff==2.2.1\n",
    "transformers==4.46.1\n",
    "accelerate==1.3.0\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import sacrebleu\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    BatchFeature,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5e2b2a",
   "metadata": {},
   "source": [
    "## Original Pipeline Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5650c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTSRUCTION = {\n",
    "    \"en_zh-CN\": \"Translate the audio to Mandarin.\",\n",
    "    \"en_id\": \"Translate the audio to Indonesian.\",\n",
    "    \"en_sl\": \"Translate the audio to Slovenian.\",\n",
    "}\n",
    "TOKENIZER = {\n",
    "    \"en_zh-CN\": \"zh\",\n",
    "    \"en_ja\": \"ja-mecab\",\n",
    "}\n",
    "ANSWER_SUFFIX = \"<|end|><|endoftext|>\"\n",
    "_IGNORE_INDEX = -100\n",
    "_TRAIN_SIZE = 50000\n",
    "_EVAL_SIZE = 200\n",
    "\n",
    "class MultipleTokenBatchStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"Stopping criteria capable of receiving multiple stop-tokens and handling batched inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, stop_tokens: torch.LongTensor, batch_size: int = 1) -> None:\n",
    "        \"\"\"Initialize the multiple token batch stopping criteria.\n",
    "\n",
    "        Args:\n",
    "            stop_tokens: Stop-tokens.\n",
    "            batch_size: Batch size.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.stop_tokens = stop_tokens\n",
    "        self.max_stop_tokens = stop_tokens.shape[-1]\n",
    "        self.stop_tokens_idx = torch.zeros(batch_size, dtype=torch.long, device=stop_tokens.device)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # Only gather the maximum number of inputs compatible with stop tokens\n",
    "        # and checks whether generated inputs are equal to `stop_tokens`\n",
    "        generated_inputs = torch.eq(input_ids[:, -self.max_stop_tokens :].unsqueeze(1), self.stop_tokens)\n",
    "        equal_generated_inputs = torch.all(generated_inputs, dim=2)\n",
    "\n",
    "        # Mark the position where a stop token has been produced for each input in the batch,\n",
    "        # but only if the corresponding entry is not already set\n",
    "        sequence_idx = torch.any(equal_generated_inputs, dim=1)\n",
    "        sequence_set_mask = self.stop_tokens_idx == 0\n",
    "        self.stop_tokens_idx[sequence_idx & sequence_set_mask] = input_ids.shape[-1]\n",
    "\n",
    "        return torch.all(self.stop_tokens_idx)\n",
    "\n",
    "class CoVoSTDataset(Dataset):\n",
    "    def __init__(self, processor, data_dir, split=\"validated\", lang=\"en_zh-CN\", rank=0, world_size=1):\n",
    "        try:\n",
    "            print(f\"Loading dataset with data_dir={data_dir}, split={split}\")\n",
    "            self.data = load_dataset(\n",
    "                \"csv\",\n",
    "                lang,\n",
    "                data_dir=data_dir,\n",
    "                split=split,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(f\"Loaded {split} split with {len(self.data)} examples.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {split} split: {e}\")\n",
    "            self.data = []\n",
    "\n",
    "        self.training = \"validated\" in split\n",
    "        self.processor = processor\n",
    "        self.instruction = INSTSRUCTION[lang]\n",
    "\n",
    "        if world_size > 1:\n",
    "            self.data = self.data.shard(world_size, rank)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        {'client_id': '0013037a1d45cc33460806cc3f8ecee9d536c45639ba4cbbf1564f1c051f53ff3c9f89ef2f1bf04badf55b3a2e7654c086f903681a7b6299616cff6f67598eff',\n",
    "        'file': '{data_dir}/clips/common_voice_en_699711.mp3',\n",
    "        'audio': {'path': '{data_dir}/clips/common_voice_en_699711.mp3',\n",
    "        'array': array([-1.28056854e-09, -1.74622983e-09, -1.16415322e-10, ...,\n",
    "                3.92560651e-10,  6.62794264e-10, -3.89536581e-09]),\n",
    "        'sampling_rate': 16000},\n",
    "        'sentence': '\"She\\'ll be all right.\"',\n",
    "        'translation': '她会没事的。',\n",
    "        'id': 'common_voice_en_699711'}\n",
    "        \"\"\"\n",
    "        data = self.data[idx]\n",
    "        user_message = {\n",
    "            'role': 'user',\n",
    "            'content': '<|audio_1|>\\n' + self.instruction,\n",
    "        }\n",
    "        prompt = self.processor.tokenizer.apply_chat_template(\n",
    "            [user_message], tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = self.processor(text=prompt, audios=[(data[\"audio\"][\"array\"], data[\"audio\"][\"sampling_rate\"])], return_tensors='pt')\n",
    "        \n",
    "        answer = f\"{data['translation']}{ANSWER_SUFFIX}\"\n",
    "        answer_ids = self.processor.tokenizer(answer, return_tensors='pt').input_ids\n",
    "        if  self.training:\n",
    "            input_ids = torch.cat([inputs.input_ids, answer_ids], dim=1)\n",
    "            labels = torch.full_like(input_ids, _IGNORE_INDEX)\n",
    "            labels[:, -answer_ids.shape[1] :] = answer_ids\n",
    "        else:\n",
    "            input_ids = inputs.input_ids\n",
    "            labels = answer_ids\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'input_audio_embeds': inputs.input_audio_embeds,\n",
    "            'audio_embed_sizes': inputs.audio_embed_sizes,\n",
    "        }\n",
    "\n",
    "def pad_sequence(sequences, padding_side='right', padding_value=0):\n",
    "    \"\"\"\n",
    "    Pad a list of sequences to the same length.\n",
    "    sequences: list of tensors in [seq_len, *] shape\n",
    "    \"\"\"\n",
    "    assert padding_side in ['right', 'left']\n",
    "    max_size = sequences[0].size()\n",
    "    trailing_dims = max_size[1:]\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    batch_size = len(sequences)\n",
    "    output = sequences[0].new_full((batch_size, max_len) + trailing_dims, padding_value)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = seq.size(0)\n",
    "        if padding_side == 'right':\n",
    "            output.data[i, :length] = seq\n",
    "        else:\n",
    "            output.data[i, -length:] = seq\n",
    "    return output\n",
    "\n",
    "\n",
    "def cat_with_pad(tensors, dim, padding_value=0):\n",
    "    \"\"\"\n",
    "    cat along dim, while pad to max for all other dims\n",
    "    \"\"\"\n",
    "    ndim = tensors[0].dim()\n",
    "    assert all(\n",
    "        t.dim() == ndim for t in tensors[1:]\n",
    "    ), 'All tensors must have the same number of dimensions'\n",
    "\n",
    "    out_size = [max(t.shape[i] for t in tensors) for i in range(ndim)]\n",
    "    out_size[dim] = sum(t.shape[dim] for t in tensors)\n",
    "    output = tensors[0].new_full(out_size, padding_value)\n",
    "\n",
    "    index = 0\n",
    "    for t in tensors:\n",
    "        # Create a slice list where every dimension except dim is full slice\n",
    "        slices = [slice(0, t.shape[d]) for d in range(ndim)]\n",
    "        # Update only the concat dimension slice\n",
    "        slices[dim] = slice(index, index + t.shape[dim])\n",
    "\n",
    "        output[slices] = t\n",
    "        index += t.shape[dim]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def covost_collate_fn(batch):\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    input_audio_embeds_list = []\n",
    "    audio_embed_sizes_list = []\n",
    "    audio_attention_mask_list = []\n",
    "    for inputs in batch:\n",
    "        input_ids_list.append(inputs['input_ids'][0])\n",
    "        labels_list.append(inputs['labels'][0])\n",
    "        input_audio_embeds_list.append(inputs['input_audio_embeds'])\n",
    "        audio_embed_sizes_list.append(inputs['audio_embed_sizes'])\n",
    "        audio_attention_mask_list.append(\n",
    "            inputs['input_audio_embeds'].new_full((inputs['input_audio_embeds'].size(1),), True, dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        input_ids = pad_sequence(input_ids_list, padding_side='left', padding_value=0)\n",
    "        labels = pad_sequence(labels_list, padding_side='left', padding_value=0)\n",
    "        audio_attention_mask = (\n",
    "            pad_sequence(audio_attention_mask_list, padding_side='right', padding_value=False)\n",
    "            if len(audio_attention_mask_list) > 1\n",
    "            else None\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(input_ids_list)\n",
    "        print(labels_list)\n",
    "        raise\n",
    "    attention_mask = (input_ids != 0).long()\n",
    "    input_audio_embeds = cat_with_pad(input_audio_embeds_list, dim=0)\n",
    "    audio_embed_sizes = torch.cat(audio_embed_sizes_list)\n",
    "\n",
    "    return BatchFeature(\n",
    "        {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "            'input_audio_embeds': input_audio_embeds,\n",
    "            'audio_embed_sizes': audio_embed_sizes,\n",
    "            'audio_attention_mask': audio_attention_mask,\n",
    "            'input_mode': 2,  # speech mode\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def create_model(model_name_or_path, use_flash_attention=False):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        torch_dtype=torch.bfloat16 if use_flash_attention else torch.float32,\n",
    "        _attn_implementation='flash_attention_2' if use_flash_attention else 'sdpa',\n",
    "        trust_remote_code=True,\n",
    "    ).to('cuda')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model, processor, eval_dataset, save_path=None, disable_tqdm=False, eval_batch_size=1\n",
    "):\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "\n",
    "    model.eval()\n",
    "    all_generated_texts = []\n",
    "    all_labels = []\n",
    "\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=eval_batch_size,\n",
    "        collate_fn=covost_collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=8,\n",
    "        prefetch_factor=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    stop_tokens = [\"<|end|>\", processor.tokenizer.eos_token]\n",
    "    stop_tokens_ids = processor.tokenizer(stop_tokens, add_special_tokens=False, padding=\"longest\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "    stop_tokens_ids = stop_tokens_ids.to(f'cuda:{local_rank}')\n",
    "\n",
    "    for inputs in tqdm(\n",
    "        eval_dataloader, disable=(rank != 0) or disable_tqdm, desc='running eval'\n",
    "    ):\n",
    "        stopping_criteria=StoppingCriteriaList([MultipleTokenBatchStoppingCriteria(stop_tokens_ids, batch_size=inputs.input_ids.size(0))])\n",
    "        inputs = inputs.to(f'cuda:{local_rank}')\n",
    "        generated_ids = model.generate(\n",
    "            **inputs, eos_token_id=processor.tokenizer.eos_token_id, max_new_tokens=64,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "        )\n",
    "\n",
    "        stop_tokens_idx = stopping_criteria[0].stop_tokens_idx.reshape(inputs.input_ids.size(0), -1)[:, 0]\n",
    "\n",
    "        stop_tokens_idx = torch.where(\n",
    "            stop_tokens_idx > 0,\n",
    "            stop_tokens_idx - stop_tokens_ids.shape[-1],\n",
    "            generated_ids.shape[-1],\n",
    "        )\n",
    "        generated_text = [\n",
    "            processor.decode(_pred_ids[inputs[\"input_ids\"].shape[1] : _stop_tokens_idx], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "            for _pred_ids, _stop_tokens_idx in zip(generated_ids, stop_tokens_idx)\n",
    "        ]\n",
    "        all_generated_texts.extend(generated_text)\n",
    "        labels = [processor.decode(_label_ids[_label_ids != 0]).removesuffix(ANSWER_SUFFIX) for _label_ids in inputs[\"labels\"]]\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    all_generated_texts = gather_object(all_generated_texts)\n",
    "    all_labels = gather_object(all_labels)\n",
    "    \n",
    "    if rank == 0:\n",
    "        assert len(all_generated_texts) == len(all_labels)\n",
    "        bleu = sacrebleu.corpus_bleu(all_generated_texts, [all_labels])\n",
    "        print(bleu)\n",
    "        if save_path:\n",
    "            with open(save_path, 'w') as f:\n",
    "                save_dict = {\n",
    "                    'all_generated_texts': all_generated_texts,\n",
    "                    'all_labels': all_labels,\n",
    "                    'score': bleu.score,\n",
    "                }\n",
    "                json.dump(save_dict, f)\n",
    "\n",
    "        return bleu.score\n",
    "    return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--model_name_or_path',\n",
    "        type=str,\n",
    "        default='microsoft/Phi-4-multimodal-instruct',\n",
    "        help='Model name or path to load from',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--common_voice_dir\",\n",
    "        type=str,\n",
    "        default=\"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en\",\n",
    "        help=\"Unzipped Common Voice Audio dataset directory, refer to https://commonvoice.mozilla.org/en/datasets, version 4.0\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lang\",\n",
    "        type=str,\n",
    "        default=\"en_sl\",\n",
    "        help=\"Language pair for translation.\",\n",
    "    )\n",
    "    parser.add_argument('--use_flash_attention', action='store_true', help='Use Flash Attention')\n",
    "    parser.add_argument('--output_dir', type=str, default='./output/', help='Output directory')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')\n",
    "    parser.add_argument(\n",
    "        '--batch_size_per_gpu',\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help='Batch size per GPU (adjust this to fit in GPU memory)',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_train_epochs', type=int, default=1, help='Number of training epochs'\n",
    "    )\n",
    "    parser.add_argument('--learning_rate', type=float, default=4.0e-5, help='Learning rate')\n",
    "    parser.add_argument('--wd', type=float, default=0.01, help='Weight decay')\n",
    "    parser.add_argument('--no-tqdm', dest='tqdm', action='store_false', help='Disable tqdm')\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    with accelerator.local_main_process_first():\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model = create_model(\n",
    "            args.model_name_or_path,\n",
    "            use_flash_attention=args.use_flash_attention,\n",
    "        )\n",
    "\n",
    "    model.set_lora_adapter('speech')\n",
    "\n",
    "\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
    "\n",
    "    eval_dataset = CoVoSTDataset(processor,\n",
    "                                 data_dir=\"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en\",\n",
    "                                 split=\"validated[:200]\",\n",
    "                                 lang=args.lang,\n",
    "                                 rank=rank,\n",
    "                                 world_size=world_size)\n",
    "    \n",
    "    train_dataset = CoVoSTDataset(processor,\n",
    "                                  data_dir=\"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en\",\n",
    "                                  split=f'validated[:{_TRAIN_SIZE}]',\n",
    "                                  lang=args.lang)\n",
    "\n",
    "    num_gpus = accelerator.num_processes\n",
    "    print(f'training on {num_gpus} GPUs')\n",
    "    assert (\n",
    "        args.batch_size % (num_gpus * args.batch_size_per_gpu) == 0\n",
    "    ), 'Batch size must be divisible by the number of GPUs'\n",
    "    gradient_accumulation_steps = args.batch_size // (num_gpus * args.batch_size_per_gpu)\n",
    "\n",
    "    if args.use_flash_attention:\n",
    "        fp16 = False\n",
    "        bf16 = True\n",
    "    else:\n",
    "        fp16 = True\n",
    "        bf16 = False\n",
    "\n",
    "    # hard coded training args\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        per_device_train_batch_size=args.batch_size_per_gpu,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        optim='adamw_torch',\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.95,\n",
    "        adam_epsilon=1e-7,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.wd,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type='linear',\n",
    "        warmup_steps=50,\n",
    "        logging_steps=10,\n",
    "        output_dir=args.output_dir,\n",
    "        save_strategy='no',\n",
    "        save_total_limit=10,\n",
    "        save_only_model=True,\n",
    "        bf16=bf16,\n",
    "        fp16=fp16,\n",
    "        remove_unused_columns=False,\n",
    "        report_to='none',\n",
    "        deepspeed=None,\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        dataloader_num_workers=4,\n",
    "        ddp_find_unused_parameters=True,  # for unused SigLIP layers\n",
    "    )\n",
    "\n",
    "    # eval before fine-tuning\n",
    "    out_path = Path(training_args.output_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        save_path=out_path / 'eval_before.json',\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        eval_batch_size=args.batch_size_per_gpu,\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        print(f'BLEU Score before finetuning: {score}')\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=covost_collate_fn,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    if accelerator.is_main_process:\n",
    "        processor.save_pretrained(training_args.output_dir)\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    # eval after fine-tuning (load saved checkpoint)\n",
    "    # first try to clear GPU memory\n",
    "    del model\n",
    "    del trainer\n",
    "    __import__('gc').collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # reload the model for inference\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        training_args.output_dir,\n",
    "        torch_dtype=torch.bfloat16 if args.use_flash_attention else torch.float32,\n",
    "        trust_remote_code=True,\n",
    "        _attn_implementation='flash_attention_2' if args.use_flash_attention else 'sdpa',\n",
    "    ).to('cuda')\n",
    "\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        save_path=out_path / 'eval_after.json',\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        eval_batch_size=args.batch_size_per_gpu,\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        print(f'BLEU Score after finetuning: {score}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4f8b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--model_name_or_path',\n",
    "        type=str,\n",
    "        default='microsoft/Phi-4-multimodal-instruct',\n",
    "        help='Model name or path to load from',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lang\",\n",
    "        type=str,\n",
    "        default=\"en_sl\",\n",
    "        help=\"Language pair for translation.\",\n",
    "    )\n",
    "    parser.add_argument('--output_dir', type=str, default='./output/', help='Output directory')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')\n",
    "    parser.add_argument('--batch_size_per_gpu', type=int, default=32, help='Batch size per GPU')\n",
    "    parser.add_argument('--num_train_epochs', type=int, default=1, help='Number of training epochs')\n",
    "    parser.add_argument('--learning_rate', type=float, default=4.0e-5, help='Learning rate')\n",
    "    parser.add_argument('--wd', type=float, default=0.01, help='Weight decay')\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    with accelerator.local_main_process_first():\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model = create_model(\n",
    "            args.model_name_or_path,\n",
    "        )\n",
    "\n",
    "    model.set_lora_adapter('speech')\n",
    "\n",
    "    # Load the dataset with Pandas and convert to Hugging Face Dataset\n",
    "    validated_path = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/validated.tsv\"\n",
    "    df = pd.read_csv(validated_path, sep=\"\\t\")\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    # Split the dataset into training and evaluation sets\n",
    "    train_size = int(0.8 * len(hf_dataset))\n",
    "    eval_size = len(hf_dataset) - train_size\n",
    "    train_dataset_raw, eval_dataset_raw = hf_dataset.train_test_split(\n",
    "        test_size=eval_size / len(hf_dataset)\n",
    "    ).values()\n",
    "\n",
    "    # Create CoVoSTDataset instances\n",
    "    train_dataset = CoVoSTDataset(processor, train_dataset_raw, training=True, lang=args.lang)\n",
    "    eval_dataset = CoVoSTDataset(processor, eval_dataset_raw, training=False, lang=args.lang)\n",
    "\n",
    "    num_gpus = accelerator.num_processes\n",
    "    print(f'training on {num_gpus} GPUs')\n",
    "    gradient_accumulation_steps = args.batch_size // (num_gpus * args.batch_size_per_gpu)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        per_device_train_batch_size=args.batch_size_per_gpu,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.wd,\n",
    "        output_dir=args.output_dir,\n",
    "        save_strategy='no',\n",
    "        remove_unused_columns=False,\n",
    "        disable_tqdm=True,\n",
    "    )\n",
    "\n",
    "    # Evaluate before fine-tuning\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        eval_batch_size=args.batch_size_per_gpu,\n",
    "    )\n",
    "    print(f'BLEU Score before finetuning: {score}')\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=covost_collate_fn,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "\n",
    "    # Evaluate after fine-tuning\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        eval_batch_size=args.batch_size_per_gpu,\n",
    "    )\n",
    "    print(f'BLEU Score after finetuning: {score}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d61f6",
   "metadata": {},
   "source": [
    "## Loading smaller version of Common Vocie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8324ccf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files and directories in data_dir:\n",
      "['unvalidated_sentences.tsv', 'reported.tsv', 'validated_sentences.tsv', 'clip_durations.tsv', 'invalidated.tsv', 'clips', 'validated.tsv', 'other.tsv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en\"\n",
    "print(\"Files and directories in data_dir:\")\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf25ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_dir = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en\"\n",
    "print(\"Files and directories in data_dir:\")\n",
    "print(os.listdir(data_dir))\n",
    "\n",
    "validated_path = os.path.join(data_dir, \"validated.tsv\")\n",
    "if os.path.exists(validated_path):\n",
    "    df = pd.read_csv(validated_path, sep=\"\\t\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"validated.tsv is missing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89193aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully with 202 examples!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "try:\n",
    "    dataset = load_dataset(\n",
    "         \"csv\",\n",
    "        data_files=\"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/validated.tsv\",\n",
    "        column_names=[\"client_id\", \"file\", \"audio\", \"sentence\", \"translation\", \"id\"],\n",
    "        split=\"train\",    \n",
    "        )\n",
    "    print(f\"Dataset loaded successfully with {len(dataset)} examples!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4d0a755d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:00, ? examples/s]Failed to read file '/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/validated.tsv' with error <class 'pandas.errors.ParserError'>: Error tokenizing data. C error: Expected 4 fields in line 182, saw 5\n",
      "\n",
      "Generating train split: 0 examples [00:00, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading dataset: An error occurred while generating the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "try:\n",
    "    # Load the dataset and let it infer the column names from the header\n",
    "    dataset = load_dataset(\n",
    "        \"csv\",\n",
    "        data_files=\"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/validated.tsv\"\n",
    "    )\n",
    "    \n",
    "    # Print the inferred column names\n",
    "    print(f\"Dataset loaded successfully with {len(dataset)} examples!\")\n",
    "    print(f\"Columns: {dataset.column_names}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "28e57eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully!\n",
      "First 5 rows of the file:\n",
      "                                           client_id  \\\n",
      "0  116398939d6be70fc5fb532924a130c0adf286ac283499...   \n",
      "1  24a4da2e8f053a45a0715849c222a40a4b0da9872efb2e...   \n",
      "2  30849595699bc853c3810a78448acede46888b4e2d0809...   \n",
      "3  42d53f34c1bc50f7a7c4ed1765a8d1ffeaf5cd441513cc...   \n",
      "4  436b9e1f9da710d74eb01209f8f269bee70e93cadf2053...   \n",
      "\n",
      "                           path  \\\n",
      "0  common_voice_en_41923025.mp3   \n",
      "1  common_voice_en_42356358.mp3   \n",
      "2  common_voice_en_42165090.mp3   \n",
      "3  common_voice_en_41921729.mp3   \n",
      "4  common_voice_en_42528393.mp3   \n",
      "\n",
      "                                         sentence_id  \\\n",
      "0  f5a2a431746c5229ab696ba0e1a518fe7b26e208ff3b84...   \n",
      "1  f6f009587d8812c147af1cc05079e1fcd8120c8a98cdf8...   \n",
      "2  f69afa5e77812e8be0085c874d2a9767323c78ffb43ba6...   \n",
      "3  f5739acbefdbd3aac990792966fac4d40dcb39eb8dfa21...   \n",
      "4  f7d35c60d76f025c45a9495757d1ee0e2b7c206317a288...   \n",
      "\n",
      "                                            sentence sentence_domain  \\\n",
      "0  He was born at Wichenford, in Worcestershire, ...             NaN   \n",
      "1  The Portuguese division was overrun and withdr...             NaN   \n",
      "2            Her health by this stage was also poor.             NaN   \n",
      "3  His sporting interests outside of cricket incl...             NaN   \n",
      "4  The following year he was elected to be part o...             NaN   \n",
      "\n",
      "   up_votes  down_votes       age gender                accents  variant  \\\n",
      "0         2           0  thirties    NaN  United States English      NaN   \n",
      "1         2           0     teens    NaN  United States English      NaN   \n",
      "2         2           0       NaN    NaN                    NaN      NaN   \n",
      "3         2           0  nineties    NaN        England English      NaN   \n",
      "4         2           0     teens    NaN  United States English      NaN   \n",
      "\n",
      "  locale  segment  \n",
      "0     en      NaN  \n",
      "1     en      NaN  \n",
      "2     en      NaN  \n",
      "3     en      NaN  \n",
      "4     en      NaN  \n",
      "\n",
      "Columns in the file:\n",
      "['client_id', 'path', 'sentence_id', 'sentence', 'sentence_domain', 'up_votes', 'down_votes', 'age', 'gender', 'accents', 'variant', 'locale', 'segment']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the validated.tsv file\n",
    "validated_path = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/validated.tsv\"\n",
    "\n",
    "# Check if the file exists and load it with Pandas\n",
    "try:\n",
    "    # Load the .tsv file\n",
    "    df = pd.read_csv(validated_path, sep=\"\\t\")  # Use '\\t' as the separator for .tsv files\n",
    "    \n",
    "    # Print the first few rows of the file\n",
    "    print(\"File loaded successfully!\")\n",
    "    print(\"First 5 rows of the file:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Print the column names\n",
    "    print(\"\\nColumns in the file:\")\n",
    "    print(df.columns.tolist())\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {validated_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97c66b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['client_id', 'path', 'sentence_id', 'sentence', 'sentence_domain', 'up_votes', 'down_votes', 'age', 'gender', 'accents', 'variant', 'locale', 'segment'],\n",
      "    num_rows: 249\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the .tsv file with Pandas\n",
    "df = pd.read_csv(validated_path, sep=\"\\t\")\n",
    "\n",
    "# Convert the Pandas DataFrame to a Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Print the first few rows of the Hugging Face Dataset\n",
    "print(hf_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3620e554",
   "metadata": {},
   "source": [
    "## Load smaller version into pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fede092",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (3943408747.py, line 108)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 108\u001b[0;36m\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "#%pip install torchaudio\n",
    "\n",
    "from datasets import Dataset\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "INSTSRUCTION = {\n",
    "    \"en_zh-CN\": \"Translate the audio to Mandarin.\",\n",
    "    \"en_id\": \"Translate the audio to Indonesian.\",\n",
    "    \"en_sl\": \"Translate the audio to Slovenian.\",\n",
    "}\n",
    "TOKENIZER = {\n",
    "    \"en_zh-CN\": \"zh\",\n",
    "    \"en_ja\": \"ja-mecab\",\n",
    "}\n",
    "ANSWER_SUFFIX = \"<|end|><|endoftext|>\"\n",
    "_IGNORE_INDEX = -100\n",
    "_TRAIN_SIZE = 50000\n",
    "_EVAL_SIZE = 200\n",
    "\n",
    "class MultipleTokenBatchStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"Stopping criteria capable of receiving multiple stop-tokens and handling batched inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, stop_tokens: torch.LongTensor, batch_size: int = 1) -> None:\n",
    "        \"\"\"Initialize the multiple token batch stopping criteria.\n",
    "\n",
    "        Args:\n",
    "            stop_tokens: Stop-tokens.\n",
    "            batch_size: Batch size.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.stop_tokens = stop_tokens\n",
    "        self.max_stop_tokens = stop_tokens.shape[-1]\n",
    "        self.stop_tokens_idx = torch.zeros(batch_size, dtype=torch.long, device=stop_tokens.device)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # Only gather the maximum number of inputs compatible with stop tokens\n",
    "        # and checks whether generated inputs are equal to `stop_tokens`\n",
    "        generated_inputs = torch.eq(input_ids[:, -self.max_stop_tokens :].unsqueeze(1), self.stop_tokens)\n",
    "        equal_generated_inputs = torch.all(generated_inputs, dim=2)\n",
    "\n",
    "        # Mark the position where a stop token has been produced for each input in the batch,\n",
    "        # but only if the corresponding entry is not already set\n",
    "        sequence_idx = torch.any(equal_generated_inputs, dim=1)\n",
    "        sequence_set_mask = self.stop_tokens_idx == 0\n",
    "        self.stop_tokens_idx[sequence_idx & sequence_set_mask] = input_ids.shape[-1]\n",
    "\n",
    "        return torch.all(self.stop_tokens_idx)\n",
    "\n",
    "def preprocess_dataset(validated_path, base_audio_dir):\n",
    "    # Load the dataset using Pandas\n",
    "    df = pd.read_csv(validated_path, sep=\"\\t\")\n",
    "\n",
    "    # Print column names to verify structure\n",
    "    print(\"Columns in the dataset:\", df.columns)\n",
    "\n",
    "    # Add audio data to the dataset\n",
    "    def load_audio(row):\n",
    "        try:\n",
    "            # Construct the full path to the audio file\n",
    "            audio_file_path = os.path.join(base_audio_dir, row[\"path\"])\n",
    "            waveform, sampling_rate = torchaudio.load(audio_file_path)\n",
    "            return {\"array\": waveform.squeeze().numpy(), \"sampling_rate\": sampling_rate}\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio for file {row['path']}: {e}\")\n",
    "            return {\"array\": None, \"sampling_rate\": None}\n",
    "\n",
    "    # Ensure the \"path\" column exists\n",
    "    if \"path\" not in df.columns:\n",
    "        raise KeyError(\"The 'path' column is missing in the dataset. Please check the dataset structure.\")\n",
    "\n",
    "    df[\"audio\"] = df.apply(load_audio, axis=1)\n",
    "\n",
    "    # Filter out rows where audio could not be loaded\n",
    "    df = df[df[\"audio\"].apply(lambda x: x[\"array\"] is not None)]\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "    return hf_dataset\n",
    "\n",
    "\n",
    "class CoVoSTDataset(Dataset):\n",
    "    def __init__(self, processor, dataset, training=True, lang=\"en_zh-CN\"):\n",
    "        self.dataset = dataset  # Renamed from self.data to self.dataset\n",
    "        self.training = training\n",
    "        self.processor = processor\n",
    "        self.instruction = INSTSRUCTION[lang]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        user_message = {\n",
    "            'role': 'user',\n",
    "            'content': '<|audio_1|>\\n' + self.instruction,\n",
    "        }\n",
    "        prompt = self.processor.tokenizer.apply_chat_template(\n",
    "            [user_message], tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = self.processor(\n",
    "            text=prompt,\n",
    "            audios=[(data[\"audio\"][\"array\"], data[\"audio\"][\"sampling_rate\"])],\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        answer = f\"{data['translation']}{ANSWER_SUFFIX}\"\n",
    "        answer_ids = self.processor.tokenizer(answer, return_tensors='pt').input_ids\n",
    "        if self.training:\n",
    "            input_ids = torch.cat([inputs.input_ids, answer_ids], dim=1)\n",
    "            labels = torch.full_like(input_ids, _IGNORE_INDEX)\n",
    "            labels[:, -answer_ids.shape[1]:] = answer_ids\n",
    "        else:\n",
    "            input_ids = inputs.input_ids\n",
    "            labels = answer_ids\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'input_audio_embeds': inputs.input_audio_embeds,\n",
    "            'audio_embed_sizes': inputs.audio_embed_sizes,\n",
    "        }\n",
    "\n",
    "def pad_sequence(sequences, padding_side='right', padding_value=0):\n",
    "    \"\"\"\n",
    "    Pad a list of sequences to the same length.\n",
    "    sequences: list of tensors in [seq_len, *] shape\n",
    "    \"\"\"\n",
    "    assert padding_side in ['right', 'left']\n",
    "    max_size = sequences[0].size()\n",
    "    trailing_dims = max_size[1:]\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    batch_size = len(sequences)\n",
    "    output = sequences[0].new_full((batch_size, max_len) + trailing_dims, padding_value)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = seq.size(0)\n",
    "        if padding_side == 'right':\n",
    "            output.data[i, :length] = seq\n",
    "        else:\n",
    "            output.data[i, -length:] = seq\n",
    "    return output\n",
    "\n",
    "\n",
    "def cat_with_pad(tensors, dim, padding_value=0):\n",
    "    \"\"\"\n",
    "    cat along dim, while pad to max for all other dims\n",
    "    \"\"\"\n",
    "    ndim = tensors[0].dim()\n",
    "    assert all(\n",
    "        t.dim() == ndim for t in tensors[1:]\n",
    "    ), 'All tensors must have the same number of dimensions'\n",
    "\n",
    "    out_size = [max(t.shape[i] for t in tensors) for i in range(ndim)]\n",
    "    out_size[dim] = sum(t.shape[dim] for t in tensors)\n",
    "    output = tensors[0].new_full(out_size, padding_value)\n",
    "\n",
    "    index = 0\n",
    "    for t in tensors:\n",
    "        # Create a slice list where every dimension except dim is full slice\n",
    "        slices = [slice(0, t.shape[d]) for d in range(ndim)]\n",
    "        # Update only the concat dimension slice\n",
    "        slices[dim] = slice(index, index + t.shape[dim])\n",
    "\n",
    "        output[slices] = t\n",
    "        index += t.shape[dim]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def covost_collate_fn(batch):\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    input_audio_embeds_list = []\n",
    "    audio_embed_sizes_list = []\n",
    "    audio_attention_mask_list = []\n",
    "    for inputs in batch:\n",
    "        input_ids_list.append(inputs['input_ids'][0])\n",
    "        labels_list.append(inputs['labels'][0])\n",
    "        input_audio_embeds_list.append(inputs['input_audio_embeds'])\n",
    "        audio_embed_sizes_list.append(inputs['audio_embed_sizes'])\n",
    "        audio_attention_mask_list.append(\n",
    "            inputs['input_audio_embeds'].new_full((inputs['input_audio_embeds'].size(1),), True, dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        input_ids = pad_sequence(input_ids_list, padding_side='left', padding_value=0)\n",
    "        labels = pad_sequence(labels_list, padding_side='left', padding_value=0)\n",
    "        audio_attention_mask = (\n",
    "            pad_sequence(audio_attention_mask_list, padding_side='right', padding_value=False)\n",
    "            if len(audio_attention_mask_list) > 1\n",
    "            else None\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(input_ids_list)\n",
    "        print(labels_list)\n",
    "        raise\n",
    "    attention_mask = (input_ids != 0).long()\n",
    "    input_audio_embeds = cat_with_pad(input_audio_embeds_list, dim=0)\n",
    "    audio_embed_sizes = torch.cat(audio_embed_sizes_list)\n",
    "\n",
    "    return BatchFeature(\n",
    "        {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "            'input_audio_embeds': input_audio_embeds,\n",
    "            'audio_embed_sizes': audio_embed_sizes,\n",
    "            'audio_attention_mask': audio_attention_mask,\n",
    "            'input_mode': 2,  # speech mode\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d9665",
   "metadata": {},
   "source": [
    "## Model Training Pipeline using smaller version of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61617241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: Index(['client_id', 'path', 'sentence_id', 'sentence', 'sentence_domain',\n",
      "       'up_votes', 'down_votes', 'age', 'gender', 'accents', 'variant',\n",
      "       'locale', 'segment'],\n",
      "      dtype='object')\n",
      "training on 1 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running eval:   0%|          | 0/2 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 2781, in __getitems__\n    batch = self.__getitem__(keys)\n  File \"/tmp/ipykernel_168869/155230711.py\", line 105, in __getitem__\n    audios=[(data[\"audio\"][\"array\"], data[\"audio\"][\"sampling_rate\"])],\nTypeError: list indices must be integers or slices, not str\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 183\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLEU Score after finetuning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 183\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 154\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m    142\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_train_epochs,\n\u001b[1;32m    143\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size_per_gpu,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m     disable_tqdm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    151\u001b[0m )\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Evaluate before fine-tuning\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size_per_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLEU Score before finetuning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    162\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    163\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    164\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m    165\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mcovost_collate_fn,\n\u001b[1;32m    166\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m    167\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 37\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, processor, eval_dataset, save_path, disable_tqdm, eval_batch_size)\u001b[0m\n\u001b[1;32m     34\u001b[0m stop_tokens_ids \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer(stop_tokens, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongest\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     35\u001b[0m stop_tokens_ids \u001b[38;5;241m=\u001b[39m stop_tokens_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_rank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m     38\u001b[0m     eval_dataloader, disable\u001b[38;5;241m=\u001b[39m(rank \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m disable_tqdm, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrunning eval\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     39\u001b[0m ):\n\u001b[1;32m     40\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mStoppingCriteriaList([MultipleTokenBatchStoppingCriteria(stop_tokens_ids, batch_size\u001b[38;5;241m=\u001b[39minputs\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))])\n\u001b[1;32m     41\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_rank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1480\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1505\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1505\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/_utils.py:733\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 2781, in __getitems__\n    batch = self.__getitem__(keys)\n  File \"/tmp/ipykernel_168869/155230711.py\", line 105, in __getitem__\n    audios=[(data[\"audio\"][\"array\"], data[\"audio\"][\"sampling_rate\"])],\nTypeError: list indices must be integers or slices, not str\n"
     ]
    }
   ],
   "source": [
    "def create_model(model_name_or_path, use_flash_attention=False):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        torch_dtype=torch.bfloat16 if use_flash_attention else torch.float32,\n",
    "        _attn_implementation='flash_attention_2' if use_flash_attention else 'sdpa',\n",
    "        trust_remote_code=True,\n",
    "    ).to('cuda')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model, processor, eval_dataset, save_path=None, disable_tqdm=False, eval_batch_size=1\n",
    "):\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "\n",
    "    model.eval()\n",
    "    all_generated_texts = []\n",
    "    all_labels = []\n",
    "\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=eval_batch_size,\n",
    "        collate_fn=covost_collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=8,\n",
    "        prefetch_factor=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    stop_tokens = [\"<|end|>\", processor.tokenizer.eos_token]\n",
    "    stop_tokens_ids = processor.tokenizer(stop_tokens, add_special_tokens=False, padding=\"longest\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "    stop_tokens_ids = stop_tokens_ids.to(f'cuda:{local_rank}')\n",
    "\n",
    "    for inputs in tqdm(\n",
    "        eval_dataloader, disable=(rank != 0) or disable_tqdm, desc='running eval'\n",
    "    ):\n",
    "        stopping_criteria=StoppingCriteriaList([MultipleTokenBatchStoppingCriteria(stop_tokens_ids, batch_size=inputs.input_ids.size(0))])\n",
    "        inputs = inputs.to(f'cuda:{local_rank}')\n",
    "        generated_ids = model.generate(\n",
    "            **inputs, eos_token_id=processor.tokenizer.eos_token_id, max_new_tokens=64,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "        )\n",
    "\n",
    "        stop_tokens_idx = stopping_criteria[0].stop_tokens_idx.reshape(inputs.input_ids.size(0), -1)[:, 0]\n",
    "\n",
    "        stop_tokens_idx = torch.where(\n",
    "            stop_tokens_idx > 0,\n",
    "            stop_tokens_idx - stop_tokens_ids.shape[-1],\n",
    "            generated_ids.shape[-1],\n",
    "        )\n",
    "        generated_text = [\n",
    "            processor.decode(_pred_ids[inputs[\"input_ids\"].shape[1] : _stop_tokens_idx], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "            for _pred_ids, _stop_tokens_idx in zip(generated_ids, stop_tokens_idx)\n",
    "        ]\n",
    "        all_generated_texts.extend(generated_text)\n",
    "        labels = [processor.decode(_label_ids[_label_ids != 0]).removesuffix(ANSWER_SUFFIX) for _label_ids in inputs[\"labels\"]]\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    all_generated_texts = gather_object(all_generated_texts)\n",
    "    all_labels = gather_object(all_labels)\n",
    "    \n",
    "    if rank == 0:\n",
    "        assert len(all_generated_texts) == len(all_labels)\n",
    "        bleu = sacrebleu.corpus_bleu(all_generated_texts, [all_labels])\n",
    "        print(bleu)\n",
    "        if save_path:\n",
    "            with open(save_path, 'w') as f:\n",
    "                save_dict = {\n",
    "                    'all_generated_texts': all_generated_texts,\n",
    "                    'all_labels': all_labels,\n",
    "                    'score': bleu.score,\n",
    "                }\n",
    "                json.dump(save_dict, f)\n",
    "\n",
    "        return bleu.score\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    import os\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Suppress tokenizer parallelism warning\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--model_name_or_path',\n",
    "        type=str,\n",
    "        default='microsoft/Phi-4-multimodal-instruct',\n",
    "        help='Model name or path to load from',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lang\",\n",
    "        type=str,\n",
    "        default=\"en_sl\",\n",
    "        help=\"Language pair for translation.\",\n",
    "    )\n",
    "    parser.add_argument('--output_dir', type=str, default='./output/', help='Output directory')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')\n",
    "    parser.add_argument('--batch_size_per_gpu', type=int, default=32, help='Batch size per GPU')\n",
    "    parser.add_argument('--num_train_epochs', type=int, default=1, help='Number of training epochs')\n",
    "    parser.add_argument('--learning_rate', type=float, default=4.0e-5, help='Learning rate')\n",
    "    parser.add_argument('--wd', type=float, default=0.01, help='Weight decay')\n",
    "    parser.add_argument('--no-tqdm', dest='tqdm', action='store_false', help='Disable tqdm')\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    with accelerator.local_main_process_first():\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model = create_model(\n",
    "            args.model_name_or_path,\n",
    "        )\n",
    "\n",
    "    model.set_lora_adapter('speech')\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    validated_path = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/validated.tsv\"\n",
    "    base_audio_dir = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/clips\"\n",
    "    hf_dataset = preprocess_dataset(validated_path, base_audio_dir)\n",
    "\n",
    "    # Split the dataset into training and evaluation sets\n",
    "    train_size = int(0.8 * len(hf_dataset))\n",
    "    eval_size = len(hf_dataset) - train_size\n",
    "    train_dataset_raw, eval_dataset_raw = hf_dataset.train_test_split(\n",
    "        test_size=eval_size / len(hf_dataset)\n",
    "    ).values()\n",
    "\n",
    "    # Create CoVoSTDataset instances\n",
    "    train_dataset = CoVoSTDataset(processor, train_dataset_raw, training=True, lang=args.lang)\n",
    "    eval_dataset = CoVoSTDataset(processor, eval_dataset_raw, training=False, lang=args.lang)\n",
    "\n",
    "    num_gpus = accelerator.num_processes\n",
    "    print(f'training on {num_gpus} GPUs')\n",
    "    gradient_accumulation_steps = args.batch_size // (num_gpus * args.batch_size_per_gpu)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        per_device_train_batch_size=args.batch_size_per_gpu,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.wd,\n",
    "        output_dir=args.output_dir,\n",
    "        save_strategy='no',\n",
    "        remove_unused_columns=False,\n",
    "        disable_tqdm=True,\n",
    "    )\n",
    "\n",
    "    # Evaluate before fine-tuning\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        eval_batch_size=args.batch_size_per_gpu,\n",
    "    )\n",
    "    print(f'BLEU Score before finetuning: {score}')\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=covost_collate_fn,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "\n",
    "    # Evaluate after fine-tuning\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        eval_batch_size=args.batch_size_per_gpu,\n",
    "    )\n",
    "    print(f'BLEU Score after finetuning: {score}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa16563",
   "metadata": {},
   "source": [
    "## Verify issues with datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "264634cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio loaded successfully! Sampling rate: 32000\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import os\n",
    "\n",
    "audio_file_path = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/clips/common_voice_en_41923025.mp3\"\n",
    "\n",
    "try:\n",
    "    waveform, sampling_rate = torchaudio.load(audio_file_path)\n",
    "    print(f\"Audio loaded successfully! Sampling rate: {sampling_rate}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading audio: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ee091ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: Index(['client_id', 'path', 'sentence_id', 'sentence', 'sentence_domain',\n",
      "       'up_votes', 'down_votes', 'age', 'gender', 'accents', 'variant',\n",
      "       'locale', 'segment'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "validated_path = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/validated.tsv\"\n",
    "base_audio_dir = \"/home/kelechi/bio_ramp_project/cv-corpus-21.0-delta-2025-03-14/en/clips\"\n",
    "\n",
    "\n",
    "hf_dataset = preprocess_dataset(validated_path, base_audio_dir)\n",
    "for idx, row in enumerate(hf_dataset):\n",
    "    if not isinstance(row[\"audio\"], dict) or \"array\" not in row[\"audio\"] or \"sampling_rate\" not in row[\"audio\"]:\n",
    "        print(f\"Invalid audio field at index {idx}: {row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c001b9f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mprocessor(\n\u001b[1;32m      2\u001b[0m     text\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m      3\u001b[0m     audios\u001b[38;5;241m=\u001b[39m[(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampling_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m])],\n\u001b[1;32m      4\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = self.processor(\n",
    "    text=prompt,\n",
    "    audios=[(data[\"audio\"][\"array\"], data[\"audio\"][\"sampling_rate\"])],\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_ramp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
