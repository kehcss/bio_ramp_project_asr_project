{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bdf3686",
   "metadata": {},
   "source": [
    "## Pip Install Requirment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee43e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirments:\n",
    "# pip install requests\n",
    "# !pip install torch\n",
    "# !pip install sacrebleu\n",
    "# !pip install accelerate\n",
    "# !pip install datasets\n",
    "# !pip install transformers\n",
    "# !pip install scipy torchvision\n",
    "# !pip install peft\n",
    "# !pip install backoff\n",
    "# !pip install soundfile\n",
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487ecb3e",
   "metadata": {},
   "source": [
    "## Load Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca5eeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "finetune Phi-4-multimodal-instruct on an speech task\n",
    "\n",
    "scipy==1.15.1\n",
    "peft==0.13.2\n",
    "backoff==2.2.1\n",
    "transformers==4.46.1\n",
    "accelerate==1.3.0\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import tarfile\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import sacrebleu\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    BatchFeature,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8565c9d",
   "metadata": {},
   "source": [
    "## Download Commonvoice for eval and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f78728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download tar file to the current directory\n",
    "\n",
    "def download_and_extract_tar(url, extract_to='.'):\n",
    "    # Download the tar file\n",
    "    response = requests.get(url, stream=True)\n",
    "    tar_file_path = 'dataset.tar.gz'\n",
    "    \n",
    "    with open(tar_file_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    # Extract the tar file\n",
    "    with tarfile.open(tar_file_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=extract_to)\n",
    "\n",
    "    # Remove the tar file after extraction\n",
    "    os.remove(tar_file_path)\n",
    "# Example usage\n",
    "url = 'https://storage.googleapis.com/common-voice-prod-prod-datasets/cv-corpus-4-2019-12-10/en.tar.gz?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gke-prod%40moz-fx-common-voice-prod.iam.gserviceaccount.com%2F20250424%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250424T000524Z&X-Goog-Expires=43200&X-Goog-SignedHeaders=host&X-Goog-Signature=49076e5a9f1be99b24d7cf2ef7deabd641cca40829c838875c5c6dcf1bea8818a62199915f5ca1e23a82e6b36875430cce84545db101b59c17a9f25a4dc62795d70b6ac17314ba9537156ad43aec47ed365026e966d2c57e0eae8b5c8a64cbcbb1ab2cd3e096da6d03be5ab62abceec469591ae11f2747a619440e87e2029b3f1f18b42d4ec4ee85ba57339bdda9714f7101be14a50fe533785c4ca4ab8923273b889cc1ac577a7b767009fefeed24f4e63e70e2f18397959d137ca36756560874dc14d3ea296efe7a22d7ff45c5b577fa553fd69ff351e14398c018af9873ec585a025078925dac809a4d7198e3086d3c8c1a6c18abe61374b128d5923ab56f'\n",
    "\n",
    "download_and_extract_tar(url, extract_to='CommonVoice/EN')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd604c3",
   "metadata": {},
   "source": [
    "## Evaluate and Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b495cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/transformers/models/auto/image_processing_auto.py:604: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "/home/kelechi/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on 1 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running eval:   0%|          | 0/4828 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_285459/1485723623.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n",
      "/tmp/ipykernel_285459/1485723623.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n",
      "/tmp/ipykernel_285459/1485723623.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n",
      "/tmp/ipykernel_285459/1485723623.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n",
      "/tmp/ipykernel_285459/1485723623.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n",
      "/tmp/ipykernel_285459/1485723623.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n",
      "/tmp/ipykernel_285459/1485723623.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n",
      "/tmp/ipykernel_285459/1485723623.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n",
      "/tmp/ipykernel_285459/1485723623.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n",
      "/tmp/ipykernel_285459/1485723623.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n",
      "running eval:   0%|          | 0/4828 [00:01<?, ?it/s]/tmp/ipykernel_285459/1485723623.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n",
      "/tmp/ipykernel_285459/1485723623.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n",
      "/tmp/ipykernel_285459/1485723623.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n",
      "/tmp/ipykernel_285459/1485723623.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n",
      "/tmp/ipykernel_285459/1485723623.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n",
      "/tmp/ipykernel_285459/1485723623.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n",
      "/tmp/ipykernel_285459/1485723623.py:77: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/librosa/core/audio.py\", line 176, in load\n    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n    context = sf.SoundFile(path)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/soundfile.py\", line 690, in __init__\n    self._file = self._open(file, mode_int, closefd)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/soundfile.py\", line 1265, in _open\n    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\nsoundfile.LibsndfileError: Error opening 'emnlp_nv_dataset/20240321060225-198-3540-1290941-ekwenyere-na-agbilu-na-ewepu-a.wav': System error.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_285459/1485723623.py\", line 77, in __getitem__\n    audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/librosa/core/audio.py\", line 184, in load\n    y, sr_native = __audioread_load(path, offset, duration, dtype)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/decorator.py\", line 235, in fun\n    return caller(func, *(extras + args), **kw)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n    return func(*args, **kwargs)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n    reader = audioread.audio_open(path)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/audioread/__init__.py\", line 127, in audio_open\n    return BackendClass(path)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/audioread/rawread.py\", line 59, in __init__\n    self._fh = open(filename, 'rb')\nFileNotFoundError: [Errno 2] No such file or directory: 'emnlp_nv_dataset/20240321060225-198-3540-1290941-ekwenyere-na-agbilu-na-ewepu-a.wav'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 491\u001b[0m\n\u001b[1;32m    487\u001b[0m     accelerator\u001b[38;5;241m.\u001b[39mwait_for_everyone()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 491\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 404\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# Clear GPU memory before evaluation\u001b[39;00m\n\u001b[1;32m    402\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m--> 404\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval_before.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size_per_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39mis_main_process:\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLEU Score before finetuning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 232\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, processor, eval_dataset, save_path, disable_tqdm, eval_batch_size)\u001b[0m\n\u001b[1;32m    229\u001b[0m stop_tokens_ids \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer(stop_tokens, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongest\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    230\u001b[0m stop_tokens_ids \u001b[38;5;241m=\u001b[39m stop_tokens_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_rank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m    233\u001b[0m     eval_dataloader, disable\u001b[38;5;241m=\u001b[39m(rank \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m disable_tqdm, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrunning eval\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    234\u001b[0m ):\n\u001b[1;32m    235\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mStoppingCriteriaList([MultipleTokenBatchStoppingCriteria(stop_tokens_ids, batch_size\u001b[38;5;241m=\u001b[39minputs\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))])\n\u001b[1;32m    236\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_rank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1480\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1505\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1505\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/_utils.py:733\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/librosa/core/audio.py\", line 176, in load\n    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n    context = sf.SoundFile(path)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/soundfile.py\", line 690, in __init__\n    self._file = self._open(file, mode_int, closefd)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/soundfile.py\", line 1265, in _open\n    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\nsoundfile.LibsndfileError: Error opening 'emnlp_nv_dataset/20240321060225-198-3540-1290941-ekwenyere-na-agbilu-na-ewepu-a.wav': System error.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_285459/1485723623.py\", line 77, in __getitem__\n    audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/librosa/core/audio.py\", line 184, in load\n    y, sr_native = __audioread_load(path, offset, duration, dtype)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/decorator.py\", line 235, in fun\n    return caller(func, *(extras + args), **kw)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n    return func(*args, **kwargs)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n    reader = audioread.audio_open(path)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/audioread/__init__.py\", line 127, in audio_open\n    return BackendClass(path)\n  File \"/home/kelechi/miniconda3/envs/bio_ramp/lib/python3.9/site-packages/audioread/rawread.py\", line 59, in __init__\n    self._fh = open(filename, 'rb')\nFileNotFoundError: [Errno 2] No such file or directory: 'emnlp_nv_dataset/20240321060225-198-3540-1290941-ekwenyere-na-agbilu-na-ewepu-a.wav'\n"
     ]
    }
   ],
   "source": [
    "INSTSRUCTION = {\n",
    "    \"en_zh-CN\": \"Translate the audio to Mandarin.\",\n",
    "    \"en_id\": \"Translate the audio to Indonesian.\",\n",
    "    \"en_sl\": \"Translate the audio to Slovenian.\",\n",
    "}\n",
    "TOKENIZER = {\n",
    "    \"en_zh-CN\": \"zh\",\n",
    "    \"en_ja\": \"ja-mecab\",\n",
    "}\n",
    "ANSWER_SUFFIX = \"<|end|><|endoftext|>\"\n",
    "_IGNORE_INDEX = -100\n",
    "_TRAIN_SIZE = 50000\n",
    "_EVAL_SIZE = 200\n",
    "\n",
    "class MultipleTokenBatchStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"Stopping criteria capable of receiving multiple stop-tokens and handling batched inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, stop_tokens: torch.LongTensor, batch_size: int = 1) -> None:\n",
    "        \"\"\"Initialize the multiple token batch stopping criteria.\n",
    "\n",
    "        Args:\n",
    "            stop_tokens: Stop-tokens.\n",
    "            batch_size: Batch size.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.stop_tokens = stop_tokens\n",
    "        self.max_stop_tokens = stop_tokens.shape[-1]\n",
    "        self.stop_tokens_idx = torch.zeros(batch_size, dtype=torch.long, device=stop_tokens.device)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # Only gather the maximum number of inputs compatible with stop tokens\n",
    "        # and checks whether generated inputs are equal to `stop_tokens`\n",
    "        generated_inputs = torch.eq(input_ids[:, -self.max_stop_tokens :].unsqueeze(1), self.stop_tokens)\n",
    "        equal_generated_inputs = torch.all(generated_inputs, dim=2)\n",
    "\n",
    "        # Mark the position where a stop token has been produced for each input in the batch,\n",
    "        # but only if the corresponding entry is not already set\n",
    "        sequence_idx = torch.any(equal_generated_inputs, dim=1)\n",
    "        sequence_set_mask = self.stop_tokens_idx == 0\n",
    "        self.stop_tokens_idx[sequence_idx & sequence_set_mask] = input_ids.shape[-1]\n",
    "\n",
    "        return torch.all(self.stop_tokens_idx)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, processor, data_dir, split, lang=\"en_zh-CN\", rank=0, world_size=1):\n",
    "        # Load the custom dataset from CSV files\n",
    "        file_path = os.path.join(data_dir, f\"{split}.csv\")\n",
    "        self.data = pd.read_csv(file_path)\n",
    "\n",
    "        self.training = \"train\" in split\n",
    "        self.processor = processor\n",
    "        self.instruction = INSTSRUCTION[lang]\n",
    "\n",
    "        # For distributed training, shard the dataset if needed\n",
    "        if world_size > 1:\n",
    "            self.data = self.data.iloc[rank::world_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data.iloc[idx]\n",
    "        user_message = {\n",
    "            'role': 'user',\n",
    "            'content': '<|audio_1|>\\n' + self.instruction,\n",
    "        }\n",
    "        prompt = self.processor.tokenizer.apply_chat_template(\n",
    "            [user_message], tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        audio_path = data[\"path\"]\n",
    "        # Suppress FutureWarning from librosa\n",
    "        \n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"librosa\")\n",
    "            audio_array, sampling_rate = librosa.load(audio_path, sr=None)\n",
    "            \n",
    "\n",
    "        inputs = self.processor(\n",
    "            text=prompt,\n",
    "            audios=[(audio_array, sampling_rate)],\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        answer = f\"{data['sentence']}{ANSWER_SUFFIX}\"\n",
    "        answer_ids = self.processor.tokenizer(answer, return_tensors='pt').input_ids\n",
    "        if self.training:\n",
    "            input_ids = torch.cat([inputs.input_ids, answer_ids], dim=1)\n",
    "            labels = torch.full_like(input_ids, _IGNORE_INDEX)\n",
    "            labels[:, -answer_ids.shape[1]:] = answer_ids\n",
    "        else:\n",
    "            input_ids = inputs.input_ids\n",
    "            labels = answer_ids\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'input_audio_embeds': inputs.input_audio_embeds,\n",
    "            'audio_embed_sizes': inputs.audio_embed_sizes,\n",
    "        }\n",
    "\n",
    "\n",
    "def pad_sequence(sequences, padding_side='right', padding_value=0):\n",
    "    \"\"\"\n",
    "    Pad a list of sequences to the same length.\n",
    "    sequences: list of tensors in [seq_len, *] shape\n",
    "    \"\"\"\n",
    "    assert padding_side in ['right', 'left']\n",
    "    max_size = sequences[0].size()\n",
    "    trailing_dims = max_size[1:]\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    batch_size = len(sequences)\n",
    "    output = sequences[0].new_full((batch_size, max_len) + trailing_dims, padding_value)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = seq.size(0)\n",
    "        if padding_side == 'right':\n",
    "            output.data[i, :length] = seq\n",
    "        else:\n",
    "            output.data[i, -length:] = seq\n",
    "    return output\n",
    "\n",
    "\n",
    "def cat_with_pad(tensors, dim, padding_value=0):\n",
    "    \"\"\"\n",
    "    cat along dim, while pad to max for all other dims\n",
    "    \"\"\"\n",
    "    ndim = tensors[0].dim()\n",
    "    assert all(\n",
    "        t.dim() == ndim for t in tensors[1:]\n",
    "    ), 'All tensors must have the same number of dimensions'\n",
    "\n",
    "    out_size = [max(t.shape[i] for t in tensors) for i in range(ndim)]\n",
    "    out_size[dim] = sum(t.shape[dim] for t in tensors)\n",
    "    output = tensors[0].new_full(out_size, padding_value)\n",
    "\n",
    "    index = 0\n",
    "    for t in tensors:\n",
    "        # Create a slice list where every dimension except dim is full slice\n",
    "        slices = [slice(0, t.shape[d]) for d in range(ndim)]\n",
    "        # Update only the concat dimension slice\n",
    "        slices[dim] = slice(index, index + t.shape[dim])\n",
    "\n",
    "        output[slices] = t\n",
    "        index += t.shape[dim]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def covost_collate_fn(batch):\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    input_audio_embeds_list = []\n",
    "    audio_embed_sizes_list = []\n",
    "    audio_attention_mask_list = []\n",
    "    for inputs in batch:\n",
    "        input_ids_list.append(inputs['input_ids'][0])\n",
    "        labels_list.append(inputs['labels'][0])\n",
    "        input_audio_embeds_list.append(inputs['input_audio_embeds'])\n",
    "        audio_embed_sizes_list.append(inputs['audio_embed_sizes'])\n",
    "        audio_attention_mask_list.append(\n",
    "            inputs['input_audio_embeds'].new_full((inputs['input_audio_embeds'].size(1),), True, dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        input_ids = pad_sequence(input_ids_list, padding_side='left', padding_value=0)\n",
    "        labels = pad_sequence(labels_list, padding_side='left', padding_value=0)\n",
    "        audio_attention_mask = (\n",
    "            pad_sequence(audio_attention_mask_list, padding_side='right', padding_value=False)\n",
    "            if len(audio_attention_mask_list) > 1\n",
    "            else None\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(input_ids_list)\n",
    "        print(labels_list)\n",
    "        raise\n",
    "    attention_mask = (input_ids != 0).long()\n",
    "    input_audio_embeds = cat_with_pad(input_audio_embeds_list, dim=0)\n",
    "    audio_embed_sizes = torch.cat(audio_embed_sizes_list)\n",
    "\n",
    "    return BatchFeature(\n",
    "        {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "            'input_audio_embeds': input_audio_embeds,\n",
    "            'audio_embed_sizes': audio_embed_sizes,\n",
    "            'audio_attention_mask': audio_attention_mask,\n",
    "            'input_mode': 2,  # speech mode\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def create_model(model_name_or_path, use_flash_attention=False):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        torch_dtype=torch.bfloat16 if use_flash_attention else torch.float32,\n",
    "        _attn_implementation='flash_attention_2' if use_flash_attention else 'sdpa',\n",
    "        trust_remote_code=True,\n",
    "    ).to('cuda')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model, processor, eval_dataset, save_path=None, disable_tqdm=False, eval_batch_size=1\n",
    "):\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "\n",
    "    model.eval()\n",
    "    all_generated_texts = []\n",
    "    all_labels = []\n",
    "\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=eval_batch_size,\n",
    "        collate_fn=covost_collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=8,\n",
    "        prefetch_factor=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    stop_tokens = [\"<|end|>\", processor.tokenizer.eos_token]\n",
    "    stop_tokens_ids = processor.tokenizer(stop_tokens, add_special_tokens=False, padding=\"longest\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "    stop_tokens_ids = stop_tokens_ids.to(f'cuda:{local_rank}')\n",
    "\n",
    "    for inputs in tqdm(\n",
    "        eval_dataloader, disable=(rank != 0) or disable_tqdm, desc='running eval'\n",
    "    ):\n",
    "        stopping_criteria=StoppingCriteriaList([MultipleTokenBatchStoppingCriteria(stop_tokens_ids, batch_size=inputs.input_ids.size(0))])\n",
    "        inputs = inputs.to(f'cuda:{local_rank}')\n",
    "        generated_ids = model.generate(\n",
    "            **inputs, eos_token_id=processor.tokenizer.eos_token_id, max_new_tokens=64,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            num_logits_to_keep=64  # Set an appropriate value\n",
    "        )\n",
    "\n",
    "        stop_tokens_idx = stopping_criteria[0].stop_tokens_idx.reshape(inputs.input_ids.size(0), -1)[:, 0]\n",
    "\n",
    "        stop_tokens_idx = torch.where(\n",
    "            stop_tokens_idx > 0,\n",
    "            stop_tokens_idx - stop_tokens_ids.shape[-1],\n",
    "            generated_ids.shape[-1],\n",
    "        )\n",
    "        generated_text = [\n",
    "            processor.decode(_pred_ids[inputs[\"input_ids\"].shape[1] : _stop_tokens_idx], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "            for _pred_ids, _stop_tokens_idx in zip(generated_ids, stop_tokens_idx)\n",
    "        ]\n",
    "        all_generated_texts.extend(generated_text)\n",
    "        labels = [processor.decode(_label_ids[_label_ids != 0]).removesuffix(ANSWER_SUFFIX) for _label_ids in inputs[\"labels\"]]\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    all_generated_texts = gather_object(all_generated_texts)\n",
    "    all_labels = gather_object(all_labels)\n",
    "    \n",
    "    if rank == 0:\n",
    "        assert len(all_generated_texts) == len(all_labels)\n",
    "        bleu = sacrebleu.corpus_bleu(all_generated_texts, [all_labels])\n",
    "        print(bleu)\n",
    "        if save_path:\n",
    "            with open(save_path, 'w') as f:\n",
    "                save_dict = {\n",
    "                    'all_generated_texts': all_generated_texts,\n",
    "                    'all_labels': all_labels,\n",
    "                    'score': bleu.score,\n",
    "                }\n",
    "                json.dump(save_dict, f)\n",
    "\n",
    "        return bleu.score\n",
    "    return None\n",
    "\n",
    "\n",
    "### EVALUATION AND THEN TRAINING\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--model_name_or_path',\n",
    "        type=str,\n",
    "        default='microsoft/Phi-4-multimodal-instruct',\n",
    "        help='Model name or path to load from',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--common_voice_dir\",\n",
    "        type=str,\n",
    "        default=\"CommonVoice/EN/afrivox\",\n",
    "        help=\"Custom dataset directory\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lang\",\n",
    "        type=str,\n",
    "        default=\"en_sl\",\n",
    "        help=\"Language pair for translation.\",\n",
    "    )\n",
    "    parser.add_argument('--use_flash_attention', action='store_true', help='Use Flash Attention')\n",
    "    parser.add_argument('--output_dir', type=str, default='./output/', help='Output directory')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')\n",
    "    parser.add_argument(\n",
    "        '--batch_size_per_gpu',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='Batch size per GPU (adjust this to fit in GPU memory)',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_train_epochs', type=int, default=1, help='Number of training epochs'\n",
    "    )\n",
    "    parser.add_argument('--learning_rate', type=float, default=4.0e-5, help='Learning rate')\n",
    "    parser.add_argument('--wd', type=float, default=0.01, help='Weight decay')\n",
    "    parser.add_argument('--no-tqdm', dest='tqdm', action='store_false', help='Disable tqdm')\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    with accelerator.local_main_process_first():\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model = create_model(\n",
    "            args.model_name_or_path,\n",
    "            use_flash_attention=args.use_flash_attention,\n",
    "        )\n",
    "\n",
    "    model.set_lora_adapter('speech')\n",
    "\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
    "\n",
    "    train_dataset = CustomDataset(\n",
    "        processor,\n",
    "        data_dir=args.common_voice_dir,\n",
    "        split='train',\n",
    "        lang=args.lang,\n",
    "    )\n",
    "    \n",
    "    eval_dataset = CustomDataset(\n",
    "        processor,\n",
    "        data_dir=args.common_voice_dir,\n",
    "        split='test',\n",
    "        lang=args.lang,\n",
    "        rank=rank,\n",
    "        world_size=world_size,\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    num_gpus = accelerator.num_processes\n",
    "    print(f'training on {num_gpus} GPUs')\n",
    "    assert (\n",
    "        args.batch_size % (num_gpus * args.batch_size_per_gpu) == 0\n",
    "    ), 'Batch size must be divisible by the number of GPUs'\n",
    "    gradient_accumulation_steps = args.batch_size // (num_gpus * args.batch_size_per_gpu)\n",
    "\n",
    "    if args.use_flash_attention:\n",
    "        fp16 = False\n",
    "        bf16 = True\n",
    "    else:\n",
    "        fp16 = True\n",
    "        bf16 = False\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        per_device_train_batch_size=args.batch_size_per_gpu,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        optim='adamw_torch',\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.95,\n",
    "        adam_epsilon=1e-7,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.wd,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type='linear',\n",
    "        warmup_steps=50,\n",
    "        logging_steps=10,\n",
    "        output_dir=args.output_dir,\n",
    "        save_strategy='no',\n",
    "        save_total_limit=10,\n",
    "        save_only_model=True,\n",
    "        bf16=bf16,\n",
    "        fp16=fp16,\n",
    "        remove_unused_columns=False,\n",
    "        report_to='none',\n",
    "        deepspeed=None,\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        dataloader_num_workers=4,\n",
    "        ddp_find_unused_parameters=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    # eval before fine-tuning\n",
    "    out_path = Path(training_args.output_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Clear GPU memory before evaluation\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        save_path=out_path / 'eval_before.json',\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        eval_batch_size=args.batch_size_per_gpu,\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        print(f'BLEU Score before finetuning: {score}')\n",
    "    \n",
    "    # clear GPU memory before training\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=covost_collate_fn,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    if accelerator.is_main_process:\n",
    "        processor.save_pretrained(training_args.output_dir)\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    # eval after fine-tuning (load saved checkpoint)\n",
    "    # first try to clear GPU memory\n",
    "    del model\n",
    "    del trainer\n",
    "    __import__('gc').collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # reload the model for inference\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        training_args.output_dir,\n",
    "        torch_dtype=torch.bfloat16 if args.use_flash_attention else torch.float32,\n",
    "        trust_remote_code=True,\n",
    "        _attn_implementation='flash_attention_2' if args.use_flash_attention else 'sdpa',\n",
    "    ).to('cuda')\n",
    "\n",
    "    # Clear GPU memory before evaluation\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Disable gradient checkpointing during evaluation\n",
    "    model.gradient_checkpointing_disable()\n",
    "\n",
    "    # Evaluate before fine-tuning\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        save_path=out_path / 'eval_before.json',\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        eval_batch_size=1,  # Reduce evaluation batch size\n",
    "    )\n",
    "    print(f'BLEU Score before finetuning: {score}')\n",
    "\n",
    "    # Re-enable gradient checkpointing for training\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Clear GPU memory after evaluation\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        print(f'BLEU Score after finetuning: {score}')\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Skip evaluation and proceed to fine-tuning\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=covost_collate_fn,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    if accelerator.is_main_process:\n",
    "        processor.save_pretrained(training_args.output_dir)\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6f4e29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed nonexistent files from CommonVoice/EN/afrivox/test_copy.csv.\n"
     ]
    }
   ],
   "source": [
    "## Remove csv rows where file does not exist\n",
    "def remove_nonexistent_files(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df[df['path'].apply(lambda x: os.path.exists(x))]\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Removed nonexistent files from {csv_file}.\")\n",
    "# Example usage\n",
    "csv_file = 'CommonVoice/EN/afrivox/test_copy.csv'\n",
    "remove_nonexistent_files(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00bc72c",
   "metadata": {},
   "source": [
    "## Data Zip/Unzip (Custom Naija Voice Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the zip file\n",
    "import zipfile\n",
    "import os\n",
    "def extract_zip(zip_file_path, extract_to='.'):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "# Example usage\n",
    "zip_file_path = '/home/kelechi/bio_ramp_project/afrivox-20250424T174821Z-002.zip'\n",
    "extract_zip(zip_file_path, extract_to='CommonVoice/EN')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe86fbc",
   "metadata": {},
   "source": [
    "## Tran/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv datasets and split into train, test, and validation sets\n",
    "import pandas as pd\n",
    "full_dataset = pd.read_csv('/home/kelechi/bio_ramp_project/CommonVoice/EN/afrivox/emnlp_nv_dataset.csv')\n",
    "#rename speaker_id to client_id\n",
    "full_dataset.rename(columns={'speaker_id': 'client_id'}, inplace=True)\n",
    "full_dataset.rename(columns={'audio_path': 'path'}, inplace=True)\n",
    "full_dataset.rename(columns={'transcription': 'sentence'}, inplace=True)\n",
    "\n",
    "\n",
    "train_dataset = full_dataset.sample(frac=0.8, random_state=42)\n",
    "test_dataset = full_dataset.drop(train_dataset.index)\n",
    "val_dataset = test_dataset.sample(frac=0.5, random_state=42)\n",
    "test_dataset = test_dataset.drop(val_dataset.index)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset.to_csv('/home/kelechi/bio_ramp_project/CommonVoice/EN/afrivox/train.csv', index=False)\n",
    "val_dataset.to_csv('/home/kelechi/bio_ramp_project/CommonVoice/EN/afrivox/validated.csv', index=False)\n",
    "test_dataset.to_csv('/home/kelechi/bio_ramp_project/CommonVoice/EN/afrivox/test.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd22260b",
   "metadata": {},
   "source": [
    "## Working Copy of Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e423c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTSRUCTION = {\n",
    "    \"en_zh-CN\": \"Translate the audio to Mandarin.\",\n",
    "    \"en_id\": \"Translate the audio to Indonesian.\",\n",
    "    \"en_sl\": \"Translate the audio to Slovenian.\",\n",
    "}\n",
    "TOKENIZER = {\n",
    "    \"en_zh-CN\": \"zh\",\n",
    "    \"en_ja\": \"ja-mecab\",\n",
    "}\n",
    "ANSWER_SUFFIX = \"<|end|><|endoftext|>\"\n",
    "_IGNORE_INDEX = -100\n",
    "_TRAIN_SIZE = 50000\n",
    "_EVAL_SIZE = 200\n",
    "\n",
    "class MultipleTokenBatchStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"Stopping criteria capable of receiving multiple stop-tokens and handling batched inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, stop_tokens: torch.LongTensor, batch_size: int = 1) -> None:\n",
    "        \"\"\"Initialize the multiple token batch stopping criteria.\n",
    "\n",
    "        Args:\n",
    "            stop_tokens: Stop-tokens.\n",
    "            batch_size: Batch size.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.stop_tokens = stop_tokens\n",
    "        self.max_stop_tokens = stop_tokens.shape[-1]\n",
    "        self.stop_tokens_idx = torch.zeros(batch_size, dtype=torch.long, device=stop_tokens.device)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # Only gather the maximum number of inputs compatible with stop tokens\n",
    "        # and checks whether generated inputs are equal to `stop_tokens`\n",
    "        generated_inputs = torch.eq(input_ids[:, -self.max_stop_tokens :].unsqueeze(1), self.stop_tokens)\n",
    "        equal_generated_inputs = torch.all(generated_inputs, dim=2)\n",
    "\n",
    "        # Mark the position where a stop token has been produced for each input in the batch,\n",
    "        # but only if the corresponding entry is not already set\n",
    "        sequence_idx = torch.any(equal_generated_inputs, dim=1)\n",
    "        sequence_set_mask = self.stop_tokens_idx == 0\n",
    "        self.stop_tokens_idx[sequence_idx & sequence_set_mask] = input_ids.shape[-1]\n",
    "\n",
    "        return torch.all(self.stop_tokens_idx)\n",
    "\n",
    "class CoVoSTDataset(Dataset):\n",
    "    def __init__(self, processor, data_dir, split, \n",
    "                 lang=\"en_zh-CN\", rank=0, world_size=1):\n",
    "\n",
    "        # Automatically download the dataset from Hugging Face\n",
    "        self.data = load_dataset(\n",
    "            \"facebook/covost2\",  # CoVoST2 dataset\n",
    "            lang,                # e.g., 'en_zh-CN'\n",
    "            data_dir=data_dir, \n",
    "            split=split,\n",
    "            trust_remote_code=True  # Make sure to trust the code from the repository\n",
    "        )\n",
    "        \n",
    "        self.training = \"train\" in split\n",
    "        self.processor = processor\n",
    "        self.instruction = INSTSRUCTION[lang]\n",
    "        \n",
    "        # For distributed training, shard the dataset if needed\n",
    "        if world_size > 1:\n",
    "            self.data = self.data.shard(world_size, rank) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        user_message = {\n",
    "            'role': 'user',\n",
    "            'content': '<|audio_1|>\\n' + self.instruction,\n",
    "        }\n",
    "        prompt = self.processor.tokenizer.apply_chat_template(\n",
    "            [user_message], tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = self.processor(text=prompt, audios=[(data[\"audio\"][\"array\"], data[\"audio\"][\"sampling_rate\"])], return_tensors='pt')\n",
    "        \n",
    "        answer = f\"{data['translation']}{ANSWER_SUFFIX}\"\n",
    "        answer_ids = self.processor.tokenizer(answer, return_tensors='pt').input_ids\n",
    "        if  self.training:\n",
    "            input_ids = torch.cat([inputs.input_ids, answer_ids], dim=1)\n",
    "            labels = torch.full_like(input_ids, _IGNORE_INDEX)\n",
    "            labels[:, -answer_ids.shape[1] :] = answer_ids\n",
    "        else:\n",
    "            input_ids = inputs.input_ids\n",
    "            labels = answer_ids\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'input_audio_embeds': inputs.input_audio_embeds,\n",
    "            'audio_embed_sizes': inputs.audio_embed_sizes,\n",
    "        }\n",
    "\n",
    "def pad_sequence(sequences, padding_side='right', padding_value=0):\n",
    "    \"\"\"\n",
    "    Pad a list of sequences to the same length.\n",
    "    sequences: list of tensors in [seq_len, *] shape\n",
    "    \"\"\"\n",
    "    assert padding_side in ['right', 'left']\n",
    "    max_size = sequences[0].size()\n",
    "    trailing_dims = max_size[1:]\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    batch_size = len(sequences)\n",
    "    output = sequences[0].new_full((batch_size, max_len) + trailing_dims, padding_value)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = seq.size(0)\n",
    "        if padding_side == 'right':\n",
    "            output.data[i, :length] = seq\n",
    "        else:\n",
    "            output.data[i, -length:] = seq\n",
    "    return output\n",
    "\n",
    "\n",
    "def cat_with_pad(tensors, dim, padding_value=0):\n",
    "    \"\"\"\n",
    "    cat along dim, while pad to max for all other dims\n",
    "    \"\"\"\n",
    "    ndim = tensors[0].dim()\n",
    "    assert all(\n",
    "        t.dim() == ndim for t in tensors[1:]\n",
    "    ), 'All tensors must have the same number of dimensions'\n",
    "\n",
    "    out_size = [max(t.shape[i] for t in tensors) for i in range(ndim)]\n",
    "    out_size[dim] = sum(t.shape[dim] for t in tensors)\n",
    "    output = tensors[0].new_full(out_size, padding_value)\n",
    "\n",
    "    index = 0\n",
    "    for t in tensors:\n",
    "        # Create a slice list where every dimension except dim is full slice\n",
    "        slices = [slice(0, t.shape[d]) for d in range(ndim)]\n",
    "        # Update only the concat dimension slice\n",
    "        slices[dim] = slice(index, index + t.shape[dim])\n",
    "\n",
    "        output[slices] = t\n",
    "        index += t.shape[dim]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def covost_collate_fn(batch):\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    input_audio_embeds_list = []\n",
    "    audio_embed_sizes_list = []\n",
    "    audio_attention_mask_list = []\n",
    "    for inputs in batch:\n",
    "        input_ids_list.append(inputs['input_ids'][0])\n",
    "        labels_list.append(inputs['labels'][0])\n",
    "        input_audio_embeds_list.append(inputs['input_audio_embeds'])\n",
    "        audio_embed_sizes_list.append(inputs['audio_embed_sizes'])\n",
    "        audio_attention_mask_list.append(\n",
    "            inputs['input_audio_embeds'].new_full((inputs['input_audio_embeds'].size(1),), True, dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        input_ids = pad_sequence(input_ids_list, padding_side='left', padding_value=0)\n",
    "        labels = pad_sequence(labels_list, padding_side='left', padding_value=0)\n",
    "        audio_attention_mask = (\n",
    "            pad_sequence(audio_attention_mask_list, padding_side='right', padding_value=False)\n",
    "            if len(audio_attention_mask_list) > 1\n",
    "            else None\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(input_ids_list)\n",
    "        print(labels_list)\n",
    "        raise\n",
    "    attention_mask = (input_ids != 0).long()\n",
    "    input_audio_embeds = cat_with_pad(input_audio_embeds_list, dim=0)\n",
    "    audio_embed_sizes = torch.cat(audio_embed_sizes_list)\n",
    "\n",
    "    return BatchFeature(\n",
    "        {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "            'input_audio_embeds': input_audio_embeds,\n",
    "            'audio_embed_sizes': audio_embed_sizes,\n",
    "            'audio_attention_mask': audio_attention_mask,\n",
    "            'input_mode': 2,  # speech mode\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def create_model(model_name_or_path, use_flash_attention=False):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        torch_dtype=torch.bfloat16 if use_flash_attention else torch.float32,\n",
    "        _attn_implementation='flash_attention_2' if use_flash_attention else 'sdpa',\n",
    "        trust_remote_code=True,\n",
    "    ).to('cuda')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model, processor, eval_dataset, save_path=None, disable_tqdm=False, eval_batch_size=1\n",
    "):\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "\n",
    "    model.eval()\n",
    "    all_generated_texts = []\n",
    "    all_labels = []\n",
    "\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=eval_batch_size,\n",
    "        collate_fn=covost_collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=8,\n",
    "        prefetch_factor=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    stop_tokens = [\"<|end|>\", processor.tokenizer.eos_token]\n",
    "    stop_tokens_ids = processor.tokenizer(stop_tokens, add_special_tokens=False, padding=\"longest\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "    stop_tokens_ids = stop_tokens_ids.to(f'cuda:{local_rank}')\n",
    "\n",
    "    for inputs in tqdm(\n",
    "        eval_dataloader, disable=(rank != 0) or disable_tqdm, desc='running eval'\n",
    "    ):\n",
    "        stopping_criteria=StoppingCriteriaList([MultipleTokenBatchStoppingCriteria(stop_tokens_ids, batch_size=inputs.input_ids.size(0))])\n",
    "        inputs = inputs.to(f'cuda:{local_rank}')\n",
    "        generated_ids = model.generate(\n",
    "            **inputs, eos_token_id=processor.tokenizer.eos_token_id, max_new_tokens=64,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            num_logits_to_keep=64  # Set an appropriate value\n",
    "        )\n",
    "\n",
    "        stop_tokens_idx = stopping_criteria[0].stop_tokens_idx.reshape(inputs.input_ids.size(0), -1)[:, 0]\n",
    "\n",
    "        stop_tokens_idx = torch.where(\n",
    "            stop_tokens_idx > 0,\n",
    "            stop_tokens_idx - stop_tokens_ids.shape[-1],\n",
    "            generated_ids.shape[-1],\n",
    "        )\n",
    "        generated_text = [\n",
    "            processor.decode(_pred_ids[inputs[\"input_ids\"].shape[1] : _stop_tokens_idx], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "            for _pred_ids, _stop_tokens_idx in zip(generated_ids, stop_tokens_idx)\n",
    "        ]\n",
    "        all_generated_texts.extend(generated_text)\n",
    "        labels = [processor.decode(_label_ids[_label_ids != 0]).removesuffix(ANSWER_SUFFIX) for _label_ids in inputs[\"labels\"]]\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    all_generated_texts = gather_object(all_generated_texts)\n",
    "    all_labels = gather_object(all_labels)\n",
    "    \n",
    "    if rank == 0:\n",
    "        assert len(all_generated_texts) == len(all_labels)\n",
    "        bleu = sacrebleu.corpus_bleu(all_generated_texts, [all_labels])\n",
    "        print(bleu)\n",
    "        if save_path:\n",
    "            with open(save_path, 'w') as f:\n",
    "                save_dict = {\n",
    "                    'all_generated_texts': all_generated_texts,\n",
    "                    'all_labels': all_labels,\n",
    "                    'score': bleu.score,\n",
    "                }\n",
    "                json.dump(save_dict, f)\n",
    "\n",
    "        return bleu.score\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### ONLY FOR TRAINING\n",
    "# def main():\n",
    "#     import os\n",
    "#     import torch\n",
    "#     from transformers import TrainingArguments, Trainer\n",
    "#     from accelerate import Accelerator\n",
    "\n",
    "#     # Set environment variables for memory optimization\n",
    "#     os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # Reduce fragmentation\n",
    "#     os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Suppress tokenizer parallelism warning\n",
    "\n",
    "#     # Argument parser\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\n",
    "#         '--model_name_or_path',\n",
    "#         type=str,\n",
    "#         default='microsoft/Phi-4-multimodal-instruct',\n",
    "#         help='Model name or path to load from',\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--common_voice_dir\",\n",
    "#         type=str,\n",
    "#         default=\"CommonVoice/EN\",\n",
    "#         help=\"Unzipped Common Voice Audio dataset directory\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--lang\",\n",
    "#         type=str,\n",
    "#         default=\"en_sl\",\n",
    "#         help=\"Language pair for translation.\",\n",
    "#     )\n",
    "#     parser.add_argument('--use_flash_attention', action='store_true', help='Use Flash Attention')\n",
    "#     parser.add_argument('--output_dir', type=str, default='./output/', help='Output directory')\n",
    "#     parser.add_argument('--batch_size', type=int, default=128, help='Batch size')\n",
    "#     parser.add_argument(\n",
    "#         '--batch_size_per_gpu',\n",
    "#         type=int,\n",
    "#         default=1,  # Small batch size to fit in memory\n",
    "#         help='Batch size per GPU (adjust this to fit in GPU memory)',\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         '--num_train_epochs', type=int, default=1, help='Number of training epochs'\n",
    "#     )\n",
    "#     parser.add_argument('--learning_rate', type=float, default=4.0e-5, help='Learning rate')\n",
    "#     parser.add_argument('--wd', type=float, default=0.01, help='Weight decay')\n",
    "#     parser.add_argument('--no-tqdm', dest='tqdm', action='store_false', help='Disable tqdm')\n",
    "#     args = parser.parse_args(args=[])\n",
    "\n",
    "#     # Initialize Accelerator for distributed training and memory optimization\n",
    "#     accelerator = Accelerator()\n",
    "\n",
    "#     # Load model and processor\n",
    "#     with accelerator.local_main_process_first():\n",
    "#         processor = AutoProcessor.from_pretrained(\n",
    "#             args.model_name_or_path,\n",
    "#             trust_remote_code=True,\n",
    "#         )\n",
    "#         model = create_model(\n",
    "#             args.model_name_or_path,\n",
    "#             use_flash_attention=args.use_flash_attention,\n",
    "#         )\n",
    "\n",
    "#     # Enable LoRA adapter for parameter-efficient fine-tuning\n",
    "#     model.set_lora_adapter('speech')\n",
    "\n",
    "#     # Dataset preparation\n",
    "#     train_dataset = CoVoSTDataset(\n",
    "#         processor,\n",
    "#         data_dir=args.common_voice_dir,\n",
    "#         split=f'train[:{_TRAIN_SIZE}]',\n",
    "#         lang=args.lang,\n",
    "#     )\n",
    "\n",
    "#     # GPU memory optimization\n",
    "#     num_gpus = accelerator.num_processes\n",
    "#     print(f'Training on {num_gpus} GPUs')\n",
    "#     assert (\n",
    "#         args.batch_size % (num_gpus * args.batch_size_per_gpu) == 0\n",
    "#     ), 'Batch size must be divisible by the number of GPUs'\n",
    "#     gradient_accumulation_steps = args.batch_size // (num_gpus * args.batch_size_per_gpu)\n",
    "\n",
    "#     # Mixed precision settings\n",
    "#     if args.use_flash_attention:\n",
    "#         fp16 = False\n",
    "#         bf16 = True\n",
    "#     else:\n",
    "#         fp16 = True\n",
    "#         bf16 = False\n",
    "\n",
    "#     # Training arguments with memory optimizations\n",
    "#     training_args = TrainingArguments(\n",
    "#         num_train_epochs=args.num_train_epochs,\n",
    "#         per_device_train_batch_size=args.batch_size_per_gpu,\n",
    "#         gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "#         gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "#         gradient_accumulation_steps=gradient_accumulation_steps,  # Simulate larger batch size\n",
    "#         optim='adamw_torch',\n",
    "#         adam_beta1=0.9,\n",
    "#         adam_beta2=0.95,\n",
    "#         adam_epsilon=1e-7,\n",
    "#         learning_rate=args.learning_rate,\n",
    "#         weight_decay=args.wd,\n",
    "#         max_grad_norm=1.0,\n",
    "#         lr_scheduler_type='linear',\n",
    "#         warmup_steps=50,\n",
    "#         logging_steps=10,\n",
    "#         output_dir=args.output_dir,\n",
    "#         save_strategy='no',\n",
    "#         save_total_limit=10,\n",
    "#         save_only_model=True,\n",
    "#         bf16=bf16,  # Use bfloat16 if supported\n",
    "#         fp16=fp16,  # Use mixed precision\n",
    "#         remove_unused_columns=False,\n",
    "#         report_to='none',\n",
    "#         deepspeed=None,\n",
    "#         disable_tqdm=not args.tqdm,\n",
    "#         dataloader_num_workers=4,\n",
    "#         ddp_find_unused_parameters=True,\n",
    "#     )\n",
    "\n",
    "#     # Clear GPU memory before training\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     # Trainer setup\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         data_collator=covost_collate_fn,\n",
    "#         train_dataset=train_dataset,\n",
    "#     )\n",
    "\n",
    "#     # Start training\n",
    "#     trainer.train()\n",
    "\n",
    "#     # Save the model\n",
    "#     trainer.save_model()\n",
    "#     if accelerator.is_main_process:\n",
    "#         processor.save_pretrained(training_args.output_dir)\n",
    "#     accelerator.wait_for_everyone()\n",
    "\n",
    "#     # Clear GPU memory after training\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### EVALUATION AND THEN TRAINING\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--model_name_or_path',\n",
    "        type=str,\n",
    "        default='microsoft/Phi-4-multimodal-instruct',\n",
    "        help='Model name or path to load from',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--common_voice_dir\",\n",
    "        type=str,\n",
    "        default=\"CommonVoice/EN\",\n",
    "        help=\"Unzipped Common Voice Audio dataset directory\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lang\",\n",
    "        type=str,\n",
    "        default=\"en_sl\",\n",
    "        help=\"Language pair for translation.\",\n",
    "    )\n",
    "    parser.add_argument('--use_flash_attention', action='store_true', help='Use Flash Attention')\n",
    "    parser.add_argument('--output_dir', type=str, default='./output/', help='Output directory')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')\n",
    "    parser.add_argument(\n",
    "        '--batch_size_per_gpu',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='Batch size per GPU (adjust this to fit in GPU memory)',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_train_epochs', type=int, default=1, help='Number of training epochs'\n",
    "    )\n",
    "    parser.add_argument('--learning_rate', type=float, default=4.0e-5, help='Learning rate')\n",
    "    parser.add_argument('--wd', type=float, default=0.01, help='Weight decay')\n",
    "    parser.add_argument('--no-tqdm', dest='tqdm', action='store_false', help='Disable tqdm')\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    with accelerator.local_main_process_first():\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model = create_model(\n",
    "            args.model_name_or_path,\n",
    "            use_flash_attention=args.use_flash_attention,\n",
    "        )\n",
    "\n",
    "    model.set_lora_adapter('speech')\n",
    "\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
    "\n",
    "    train_dataset = CoVoSTDataset(\n",
    "        processor,\n",
    "        data_dir=args.common_voice_dir,\n",
    "        split=f'train[:{_TRAIN_SIZE}]',\n",
    "        lang=args.lang,\n",
    "    )\n",
    "    \n",
    "    eval_dataset = CoVoSTDataset(\n",
    "        processor,\n",
    "        data_dir=args.common_voice_dir,\n",
    "        split=f'test[:{_EVAL_SIZE}]',\n",
    "        lang=args.lang,\n",
    "        rank=rank,\n",
    "        world_size=world_size,\n",
    "    )\n",
    "    \n",
    "\n",
    "    num_gpus = accelerator.num_processes\n",
    "    print(f'training on {num_gpus} GPUs')\n",
    "    assert (\n",
    "        args.batch_size % (num_gpus * args.batch_size_per_gpu) == 0\n",
    "    ), 'Batch size must be divisible by the number of GPUs'\n",
    "    gradient_accumulation_steps = args.batch_size // (num_gpus * args.batch_size_per_gpu)\n",
    "\n",
    "    if args.use_flash_attention:\n",
    "        fp16 = False\n",
    "        bf16 = True\n",
    "    else:\n",
    "        fp16 = True\n",
    "        bf16 = False\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        per_device_train_batch_size=args.batch_size_per_gpu,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        optim='adamw_torch',\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.95,\n",
    "        adam_epsilon=1e-7,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.wd,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type='linear',\n",
    "        warmup_steps=50,\n",
    "        logging_steps=10,\n",
    "        output_dir=args.output_dir,\n",
    "        save_strategy='no',\n",
    "        save_total_limit=10,\n",
    "        save_only_model=True,\n",
    "        bf16=bf16,\n",
    "        fp16=fp16,\n",
    "        remove_unused_columns=False,\n",
    "        report_to='none',\n",
    "        deepspeed=None,\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        dataloader_num_workers=4,\n",
    "        ddp_find_unused_parameters=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    # eval before fine-tuning\n",
    "    out_path = Path(training_args.output_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Clear GPU memory before evaluation\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        save_path=out_path / 'eval_before.json',\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        eval_batch_size=args.batch_size_per_gpu,\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        print(f'BLEU Score before finetuning: {score}')\n",
    "    \n",
    "    # clear GPU memory before training\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=covost_collate_fn,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    if accelerator.is_main_process:\n",
    "        processor.save_pretrained(training_args.output_dir)\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    # eval after fine-tuning (load saved checkpoint)\n",
    "    # first try to clear GPU memory\n",
    "    del model\n",
    "    del trainer\n",
    "    __import__('gc').collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # reload the model for inference\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        training_args.output_dir,\n",
    "        torch_dtype=torch.bfloat16 if args.use_flash_attention else torch.float32,\n",
    "        trust_remote_code=True,\n",
    "        _attn_implementation='flash_attention_2' if args.use_flash_attention else 'sdpa',\n",
    "    ).to('cuda')\n",
    "\n",
    "    # Clear GPU memory before evaluation\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Disable gradient checkpointing during evaluation\n",
    "    model.gradient_checkpointing_disable()\n",
    "\n",
    "    # Evaluate before fine-tuning\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        save_path=out_path / 'eval_before.json',\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        eval_batch_size=1,  # Reduce evaluation batch size\n",
    "    )\n",
    "    print(f'BLEU Score before finetuning: {score}')\n",
    "\n",
    "    # Re-enable gradient checkpointing for training\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Clear GPU memory after evaluation\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        print(f'BLEU Score after finetuning: {score}')\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Skip evaluation and proceed to fine-tuning\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=covost_collate_fn,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    if accelerator.is_main_process:\n",
    "        processor.save_pretrained(training_args.output_dir)\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_ramp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
