{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bdf3686",
   "metadata": {},
   "source": [
    "## Pip Install Requirment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee43e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirments:\n",
    "# pip install requests\n",
    "# !pip install torch\n",
    "# !pip install sacrebleu\n",
    "# !pip install accelerate\n",
    "# !pip install datasets\n",
    "# !pip install transformers\n",
    "# !pip install scipy torchvision\n",
    "# !pip install peft\n",
    "# !pip install backoff\n",
    "# !pip install soundfile\n",
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487ecb3e",
   "metadata": {},
   "source": [
    "## Load Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca5eeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelechi/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "finetune Phi-4-multimodal-instruct on an speech task\n",
    "\n",
    "scipy==1.15.1\n",
    "peft==0.13.2\n",
    "backoff==2.2.1\n",
    "transformers==4.46.1\n",
    "accelerate==1.3.0\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import sacrebleu\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    BatchFeature,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8565c9d",
   "metadata": {},
   "source": [
    "## Download Commonvoice for eval and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f78728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download tar file to the current directory\n",
    "\n",
    "def download_and_extract_tar(url, extract_to='.'):\n",
    "    # Download the tar file\n",
    "    response = requests.get(url, stream=True)\n",
    "    tar_file_path = 'dataset.tar.gz'\n",
    "    \n",
    "    with open(tar_file_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    # Extract the tar file\n",
    "    with tarfile.open(tar_file_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=extract_to)\n",
    "\n",
    "    # Remove the tar file after extraction\n",
    "    os.remove(tar_file_path)\n",
    "# Example usage\n",
    "url = 'https://storage.googleapis.com/common-voice-prod-prod-datasets/cv-corpus-4-2019-12-10/en.tar.gz?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gke-prod%40moz-fx-common-voice-prod.iam.gserviceaccount.com%2F20250424%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250424T000524Z&X-Goog-Expires=43200&X-Goog-SignedHeaders=host&X-Goog-Signature=49076e5a9f1be99b24d7cf2ef7deabd641cca40829c838875c5c6dcf1bea8818a62199915f5ca1e23a82e6b36875430cce84545db101b59c17a9f25a4dc62795d70b6ac17314ba9537156ad43aec47ed365026e966d2c57e0eae8b5c8a64cbcbb1ab2cd3e096da6d03be5ab62abceec469591ae11f2747a619440e87e2029b3f1f18b42d4ec4ee85ba57339bdda9714f7101be14a50fe533785c4ca4ab8923273b889cc1ac577a7b767009fefeed24f4e63e70e2f18397959d137ca36756560874dc14d3ea296efe7a22d7ff45c5b577fa553fd69ff351e14398c018af9873ec585a025078925dac809a4d7198e3086d3c8c1a6c18abe61374b128d5923ab56f'\n",
    "\n",
    "download_and_extract_tar(url, extract_to='CommonVoice/EN')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd604c3",
   "metadata": {},
   "source": [
    "## Evaluate and Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b495cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelechi/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/transformers/models/auto/image_processing_auto.py:604: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "/home/kelechi/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on 1 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running eval:   0%|          | 0/200 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/kelechi/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "running eval: 100%|██████████| 200/200 [02:20<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 2.94 27.0/5.6/1.5/0.3 (BP = 1.000 ratio = 1.135 hyp_len = 2087 ref_len = 1839)\n",
      "BLEU Score before finetuning: 2.9408429105427025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 23.81 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.68 GiB is allocated by PyTorch, and 483.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 642\u001b[0m\n\u001b[1;32m    638\u001b[0m     accelerator\u001b[38;5;241m.\u001b[39mwait_for_everyone()\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 642\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 576\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    567\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    569\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    570\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    571\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m    572\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mcovost_collate_fn,\n\u001b[1;32m    573\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 576\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39mis_main_process:\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/transformers/trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2554\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2556\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2558\u001b[0m )\n\u001b[1;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2566\u001b[0m ):\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/transformers/trainer.py:3782\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3780\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3784\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/accelerate/accelerator.py:2450\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2449\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2450\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/torch/utils/checkpoint.py:1124\u001b[0m, in \u001b[0;36m_checkpoint_hook.__init__.<locals>.unpack_hook\u001b[0;34m(holder)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _recomputation_hook(\n\u001b[1;32m   1122\u001b[0m         weakref\u001b[38;5;241m.\u001b[39mref(frame), gid\n\u001b[1;32m   1123\u001b[0m     ), torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m-> 1124\u001b[0m         \u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecompute_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _StopRecomputationError:\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/torch/utils/checkpoint.py:1518\u001b[0m, in \u001b[0;36m_checkpoint_without_reentrant_generator.<locals>.recompute_fn\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m device_autocast_ctx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\n\u001b[1;32m   1515\u001b[0m     device_type\u001b[38;5;241m=\u001b[39mdevice_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_autocast_kwargs\n\u001b[1;32m   1516\u001b[0m ) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mis_autocast_available(device_type) \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m device_autocast_ctx, torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcpu_autocast_kwargs), recompute_context:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1518\u001b[0m     \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/modeling_phi4mm.py:1473\u001b[0m, in \u001b[0;36mPhi4MMDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m   1472\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m-> 1473\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1474\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_mlp_dropout(hidden_states)\n\u001b[1;32m   1476\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/modeling_phi4mm.py:1037\u001b[0m, in \u001b[0;36mPhi4MMMLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m   1034\u001b[0m gate, up_states \u001b[38;5;241m=\u001b[39m up_states\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1035\u001b[0m up_states \u001b[38;5;241m=\u001b[39m up_states \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(gate)\n\u001b[0;32m-> 1037\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mup_states\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/peft/tuners/lora/layer.py:712\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_layer(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 712\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m     torch_result_dtype \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    715\u001b[0m     lora_A_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_ramp_env/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 23.81 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 22.68 GiB is allocated by PyTorch, and 483.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "INSTSRUCTION = {\n",
    "    \"en_zh-CN\": \"Translate the audio to Mandarin.\",\n",
    "    \"en_id\": \"Translate the audio to Indonesian.\",\n",
    "    \"en_sl\": \"Translate the audio to Slovenian.\",\n",
    "}\n",
    "TOKENIZER = {\n",
    "    \"en_zh-CN\": \"zh\",\n",
    "    \"en_ja\": \"ja-mecab\",\n",
    "}\n",
    "ANSWER_SUFFIX = \"<|end|><|endoftext|>\"\n",
    "_IGNORE_INDEX = -100\n",
    "_TRAIN_SIZE = 50000\n",
    "_EVAL_SIZE = 200\n",
    "\n",
    "class MultipleTokenBatchStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"Stopping criteria capable of receiving multiple stop-tokens and handling batched inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, stop_tokens: torch.LongTensor, batch_size: int = 1) -> None:\n",
    "        \"\"\"Initialize the multiple token batch stopping criteria.\n",
    "\n",
    "        Args:\n",
    "            stop_tokens: Stop-tokens.\n",
    "            batch_size: Batch size.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.stop_tokens = stop_tokens\n",
    "        self.max_stop_tokens = stop_tokens.shape[-1]\n",
    "        self.stop_tokens_idx = torch.zeros(batch_size, dtype=torch.long, device=stop_tokens.device)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # Only gather the maximum number of inputs compatible with stop tokens\n",
    "        # and checks whether generated inputs are equal to `stop_tokens`\n",
    "        generated_inputs = torch.eq(input_ids[:, -self.max_stop_tokens :].unsqueeze(1), self.stop_tokens)\n",
    "        equal_generated_inputs = torch.all(generated_inputs, dim=2)\n",
    "\n",
    "        # Mark the position where a stop token has been produced for each input in the batch,\n",
    "        # but only if the corresponding entry is not already set\n",
    "        sequence_idx = torch.any(equal_generated_inputs, dim=1)\n",
    "        sequence_set_mask = self.stop_tokens_idx == 0\n",
    "        self.stop_tokens_idx[sequence_idx & sequence_set_mask] = input_ids.shape[-1]\n",
    "\n",
    "        return torch.all(self.stop_tokens_idx)\n",
    "\n",
    "class CoVoSTDataset(Dataset):\n",
    "    def __init__(self, processor, data_dir, split, \n",
    "                 lang=\"en_zh-CN\", rank=0, world_size=1):\n",
    "\n",
    "        # Automatically download the dataset from Hugging Face\n",
    "        self.data = load_dataset(\n",
    "            \"facebook/covost2\",  # CoVoST2 dataset\n",
    "            lang,                # e.g., 'en_zh-CN'\n",
    "            data_dir=data_dir, \n",
    "            split=split,\n",
    "            trust_remote_code=True  # Make sure to trust the code from the repository\n",
    "        )\n",
    "        \n",
    "        self.training = \"train\" in split\n",
    "        self.processor = processor\n",
    "        self.instruction = INSTSRUCTION[lang]\n",
    "        \n",
    "        # For distributed training, shard the dataset if needed\n",
    "        if world_size > 1:\n",
    "            self.data = self.data.shard(world_size, rank) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        user_message = {\n",
    "            'role': 'user',\n",
    "            'content': '<|audio_1|>\\n' + self.instruction,\n",
    "        }\n",
    "        prompt = self.processor.tokenizer.apply_chat_template(\n",
    "            [user_message], tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = self.processor(text=prompt, audios=[(data[\"audio\"][\"array\"], data[\"audio\"][\"sampling_rate\"])], return_tensors='pt')\n",
    "        \n",
    "        answer = f\"{data['translation']}{ANSWER_SUFFIX}\"\n",
    "        answer_ids = self.processor.tokenizer(answer, return_tensors='pt').input_ids\n",
    "        if  self.training:\n",
    "            input_ids = torch.cat([inputs.input_ids, answer_ids], dim=1)\n",
    "            labels = torch.full_like(input_ids, _IGNORE_INDEX)\n",
    "            labels[:, -answer_ids.shape[1] :] = answer_ids\n",
    "        else:\n",
    "            input_ids = inputs.input_ids\n",
    "            labels = answer_ids\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'input_audio_embeds': inputs.input_audio_embeds,\n",
    "            'audio_embed_sizes': inputs.audio_embed_sizes,\n",
    "        }\n",
    "\n",
    "def pad_sequence(sequences, padding_side='right', padding_value=0):\n",
    "    \"\"\"\n",
    "    Pad a list of sequences to the same length.\n",
    "    sequences: list of tensors in [seq_len, *] shape\n",
    "    \"\"\"\n",
    "    assert padding_side in ['right', 'left']\n",
    "    max_size = sequences[0].size()\n",
    "    trailing_dims = max_size[1:]\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    batch_size = len(sequences)\n",
    "    output = sequences[0].new_full((batch_size, max_len) + trailing_dims, padding_value)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = seq.size(0)\n",
    "        if padding_side == 'right':\n",
    "            output.data[i, :length] = seq\n",
    "        else:\n",
    "            output.data[i, -length:] = seq\n",
    "    return output\n",
    "\n",
    "\n",
    "def cat_with_pad(tensors, dim, padding_value=0):\n",
    "    \"\"\"\n",
    "    cat along dim, while pad to max for all other dims\n",
    "    \"\"\"\n",
    "    ndim = tensors[0].dim()\n",
    "    assert all(\n",
    "        t.dim() == ndim for t in tensors[1:]\n",
    "    ), 'All tensors must have the same number of dimensions'\n",
    "\n",
    "    out_size = [max(t.shape[i] for t in tensors) for i in range(ndim)]\n",
    "    out_size[dim] = sum(t.shape[dim] for t in tensors)\n",
    "    output = tensors[0].new_full(out_size, padding_value)\n",
    "\n",
    "    index = 0\n",
    "    for t in tensors:\n",
    "        # Create a slice list where every dimension except dim is full slice\n",
    "        slices = [slice(0, t.shape[d]) for d in range(ndim)]\n",
    "        # Update only the concat dimension slice\n",
    "        slices[dim] = slice(index, index + t.shape[dim])\n",
    "\n",
    "        output[slices] = t\n",
    "        index += t.shape[dim]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def covost_collate_fn(batch):\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    input_audio_embeds_list = []\n",
    "    audio_embed_sizes_list = []\n",
    "    audio_attention_mask_list = []\n",
    "    for inputs in batch:\n",
    "        input_ids_list.append(inputs['input_ids'][0])\n",
    "        labels_list.append(inputs['labels'][0])\n",
    "        input_audio_embeds_list.append(inputs['input_audio_embeds'])\n",
    "        audio_embed_sizes_list.append(inputs['audio_embed_sizes'])\n",
    "        audio_attention_mask_list.append(\n",
    "            inputs['input_audio_embeds'].new_full((inputs['input_audio_embeds'].size(1),), True, dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        input_ids = pad_sequence(input_ids_list, padding_side='left', padding_value=0)\n",
    "        labels = pad_sequence(labels_list, padding_side='left', padding_value=0)\n",
    "        audio_attention_mask = (\n",
    "            pad_sequence(audio_attention_mask_list, padding_side='right', padding_value=False)\n",
    "            if len(audio_attention_mask_list) > 1\n",
    "            else None\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(input_ids_list)\n",
    "        print(labels_list)\n",
    "        raise\n",
    "    attention_mask = (input_ids != 0).long()\n",
    "    input_audio_embeds = cat_with_pad(input_audio_embeds_list, dim=0)\n",
    "    audio_embed_sizes = torch.cat(audio_embed_sizes_list)\n",
    "\n",
    "    return BatchFeature(\n",
    "        {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask,\n",
    "            'input_audio_embeds': input_audio_embeds,\n",
    "            'audio_embed_sizes': audio_embed_sizes,\n",
    "            'audio_attention_mask': audio_attention_mask,\n",
    "            'input_mode': 2,  # speech mode\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def create_model(model_name_or_path, use_flash_attention=False):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        torch_dtype=torch.bfloat16 if use_flash_attention else torch.float32,\n",
    "        _attn_implementation='flash_attention_2' if use_flash_attention else 'sdpa',\n",
    "        trust_remote_code=True,\n",
    "    ).to('cuda')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model, processor, eval_dataset, save_path=None, disable_tqdm=False, eval_batch_size=1\n",
    "):\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "\n",
    "    model.eval()\n",
    "    all_generated_texts = []\n",
    "    all_labels = []\n",
    "\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=eval_batch_size,\n",
    "        collate_fn=covost_collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=8,\n",
    "        prefetch_factor=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    stop_tokens = [\"<|end|>\", processor.tokenizer.eos_token]\n",
    "    stop_tokens_ids = processor.tokenizer(stop_tokens, add_special_tokens=False, padding=\"longest\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "    stop_tokens_ids = stop_tokens_ids.to(f'cuda:{local_rank}')\n",
    "\n",
    "    for inputs in tqdm(\n",
    "        eval_dataloader, disable=(rank != 0) or disable_tqdm, desc='running eval'\n",
    "    ):\n",
    "        stopping_criteria=StoppingCriteriaList([MultipleTokenBatchStoppingCriteria(stop_tokens_ids, batch_size=inputs.input_ids.size(0))])\n",
    "        inputs = inputs.to(f'cuda:{local_rank}')\n",
    "        generated_ids = model.generate(\n",
    "            **inputs, eos_token_id=processor.tokenizer.eos_token_id, max_new_tokens=64,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            num_logits_to_keep=64  # Set an appropriate value\n",
    "        )\n",
    "\n",
    "        stop_tokens_idx = stopping_criteria[0].stop_tokens_idx.reshape(inputs.input_ids.size(0), -1)[:, 0]\n",
    "\n",
    "        stop_tokens_idx = torch.where(\n",
    "            stop_tokens_idx > 0,\n",
    "            stop_tokens_idx - stop_tokens_ids.shape[-1],\n",
    "            generated_ids.shape[-1],\n",
    "        )\n",
    "        generated_text = [\n",
    "            processor.decode(_pred_ids[inputs[\"input_ids\"].shape[1] : _stop_tokens_idx], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "            for _pred_ids, _stop_tokens_idx in zip(generated_ids, stop_tokens_idx)\n",
    "        ]\n",
    "        all_generated_texts.extend(generated_text)\n",
    "        labels = [processor.decode(_label_ids[_label_ids != 0]).removesuffix(ANSWER_SUFFIX) for _label_ids in inputs[\"labels\"]]\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    all_generated_texts = gather_object(all_generated_texts)\n",
    "    all_labels = gather_object(all_labels)\n",
    "    \n",
    "    if rank == 0:\n",
    "        assert len(all_generated_texts) == len(all_labels)\n",
    "        bleu = sacrebleu.corpus_bleu(all_generated_texts, [all_labels])\n",
    "        print(bleu)\n",
    "        if save_path:\n",
    "            with open(save_path, 'w') as f:\n",
    "                save_dict = {\n",
    "                    'all_generated_texts': all_generated_texts,\n",
    "                    'all_labels': all_labels,\n",
    "                    'score': bleu.score,\n",
    "                }\n",
    "                json.dump(save_dict, f)\n",
    "\n",
    "        return bleu.score\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### ONLY FOR TRAINING\n",
    "# def main():\n",
    "#     import os\n",
    "#     import torch\n",
    "#     from transformers import TrainingArguments, Trainer\n",
    "#     from accelerate import Accelerator\n",
    "\n",
    "#     # Set environment variables for memory optimization\n",
    "#     os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # Reduce fragmentation\n",
    "#     os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Suppress tokenizer parallelism warning\n",
    "\n",
    "#     # Argument parser\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\n",
    "#         '--model_name_or_path',\n",
    "#         type=str,\n",
    "#         default='microsoft/Phi-4-multimodal-instruct',\n",
    "#         help='Model name or path to load from',\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--common_voice_dir\",\n",
    "#         type=str,\n",
    "#         default=\"CommonVoice/EN\",\n",
    "#         help=\"Unzipped Common Voice Audio dataset directory\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--lang\",\n",
    "#         type=str,\n",
    "#         default=\"en_sl\",\n",
    "#         help=\"Language pair for translation.\",\n",
    "#     )\n",
    "#     parser.add_argument('--use_flash_attention', action='store_true', help='Use Flash Attention')\n",
    "#     parser.add_argument('--output_dir', type=str, default='./output/', help='Output directory')\n",
    "#     parser.add_argument('--batch_size', type=int, default=128, help='Batch size')\n",
    "#     parser.add_argument(\n",
    "#         '--batch_size_per_gpu',\n",
    "#         type=int,\n",
    "#         default=1,  # Small batch size to fit in memory\n",
    "#         help='Batch size per GPU (adjust this to fit in GPU memory)',\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         '--num_train_epochs', type=int, default=1, help='Number of training epochs'\n",
    "#     )\n",
    "#     parser.add_argument('--learning_rate', type=float, default=4.0e-5, help='Learning rate')\n",
    "#     parser.add_argument('--wd', type=float, default=0.01, help='Weight decay')\n",
    "#     parser.add_argument('--no-tqdm', dest='tqdm', action='store_false', help='Disable tqdm')\n",
    "#     args = parser.parse_args(args=[])\n",
    "\n",
    "#     # Initialize Accelerator for distributed training and memory optimization\n",
    "#     accelerator = Accelerator()\n",
    "\n",
    "#     # Load model and processor\n",
    "#     with accelerator.local_main_process_first():\n",
    "#         processor = AutoProcessor.from_pretrained(\n",
    "#             args.model_name_or_path,\n",
    "#             trust_remote_code=True,\n",
    "#         )\n",
    "#         model = create_model(\n",
    "#             args.model_name_or_path,\n",
    "#             use_flash_attention=args.use_flash_attention,\n",
    "#         )\n",
    "\n",
    "#     # Enable LoRA adapter for parameter-efficient fine-tuning\n",
    "#     model.set_lora_adapter('speech')\n",
    "\n",
    "#     # Dataset preparation\n",
    "#     train_dataset = CoVoSTDataset(\n",
    "#         processor,\n",
    "#         data_dir=args.common_voice_dir,\n",
    "#         split=f'train[:{_TRAIN_SIZE}]',\n",
    "#         lang=args.lang,\n",
    "#     )\n",
    "\n",
    "#     # GPU memory optimization\n",
    "#     num_gpus = accelerator.num_processes\n",
    "#     print(f'Training on {num_gpus} GPUs')\n",
    "#     assert (\n",
    "#         args.batch_size % (num_gpus * args.batch_size_per_gpu) == 0\n",
    "#     ), 'Batch size must be divisible by the number of GPUs'\n",
    "#     gradient_accumulation_steps = args.batch_size // (num_gpus * args.batch_size_per_gpu)\n",
    "\n",
    "#     # Mixed precision settings\n",
    "#     if args.use_flash_attention:\n",
    "#         fp16 = False\n",
    "#         bf16 = True\n",
    "#     else:\n",
    "#         fp16 = True\n",
    "#         bf16 = False\n",
    "\n",
    "#     # Training arguments with memory optimizations\n",
    "#     training_args = TrainingArguments(\n",
    "#         num_train_epochs=args.num_train_epochs,\n",
    "#         per_device_train_batch_size=args.batch_size_per_gpu,\n",
    "#         gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "#         gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "#         gradient_accumulation_steps=gradient_accumulation_steps,  # Simulate larger batch size\n",
    "#         optim='adamw_torch',\n",
    "#         adam_beta1=0.9,\n",
    "#         adam_beta2=0.95,\n",
    "#         adam_epsilon=1e-7,\n",
    "#         learning_rate=args.learning_rate,\n",
    "#         weight_decay=args.wd,\n",
    "#         max_grad_norm=1.0,\n",
    "#         lr_scheduler_type='linear',\n",
    "#         warmup_steps=50,\n",
    "#         logging_steps=10,\n",
    "#         output_dir=args.output_dir,\n",
    "#         save_strategy='no',\n",
    "#         save_total_limit=10,\n",
    "#         save_only_model=True,\n",
    "#         bf16=bf16,  # Use bfloat16 if supported\n",
    "#         fp16=fp16,  # Use mixed precision\n",
    "#         remove_unused_columns=False,\n",
    "#         report_to='none',\n",
    "#         deepspeed=None,\n",
    "#         disable_tqdm=not args.tqdm,\n",
    "#         dataloader_num_workers=4,\n",
    "#         ddp_find_unused_parameters=True,\n",
    "#     )\n",
    "\n",
    "#     # Clear GPU memory before training\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     # Trainer setup\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         data_collator=covost_collate_fn,\n",
    "#         train_dataset=train_dataset,\n",
    "#     )\n",
    "\n",
    "#     # Start training\n",
    "#     trainer.train()\n",
    "\n",
    "#     # Save the model\n",
    "#     trainer.save_model()\n",
    "#     if accelerator.is_main_process:\n",
    "#         processor.save_pretrained(training_args.output_dir)\n",
    "#     accelerator.wait_for_everyone()\n",
    "\n",
    "#     # Clear GPU memory after training\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### EVALUATION AND THEN TRAINING\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--model_name_or_path',\n",
    "        type=str,\n",
    "        default='microsoft/Phi-4-multimodal-instruct',\n",
    "        help='Model name or path to load from',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--common_voice_dir\",\n",
    "        type=str,\n",
    "        default=\"CommonVoice/EN\",\n",
    "        help=\"Unzipped Common Voice Audio dataset directory\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lang\",\n",
    "        type=str,\n",
    "        default=\"en_sl\",\n",
    "        help=\"Language pair for translation.\",\n",
    "    )\n",
    "    parser.add_argument('--use_flash_attention', action='store_true', help='Use Flash Attention')\n",
    "    parser.add_argument('--output_dir', type=str, default='./output/', help='Output directory')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')\n",
    "    parser.add_argument(\n",
    "        '--batch_size_per_gpu',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='Batch size per GPU (adjust this to fit in GPU memory)',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_train_epochs', type=int, default=1, help='Number of training epochs'\n",
    "    )\n",
    "    parser.add_argument('--learning_rate', type=float, default=4.0e-5, help='Learning rate')\n",
    "    parser.add_argument('--wd', type=float, default=0.01, help='Weight decay')\n",
    "    parser.add_argument('--no-tqdm', dest='tqdm', action='store_false', help='Disable tqdm')\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    with accelerator.local_main_process_first():\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        model = create_model(\n",
    "            args.model_name_or_path,\n",
    "            use_flash_attention=args.use_flash_attention,\n",
    "        )\n",
    "\n",
    "    model.set_lora_adapter('speech')\n",
    "\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
    "\n",
    "    train_dataset = CoVoSTDataset(\n",
    "        processor,\n",
    "        data_dir=args.common_voice_dir,\n",
    "        split=f'train[:{_TRAIN_SIZE}]',\n",
    "        lang=args.lang,\n",
    "    )\n",
    "    \n",
    "    eval_dataset = CoVoSTDataset(\n",
    "        processor,\n",
    "        data_dir=args.common_voice_dir,\n",
    "        split=f'test[:{_EVAL_SIZE}]',\n",
    "        lang=args.lang,\n",
    "        rank=rank,\n",
    "        world_size=world_size,\n",
    "    )\n",
    "    \n",
    "\n",
    "    num_gpus = accelerator.num_processes\n",
    "    print(f'training on {num_gpus} GPUs')\n",
    "    assert (\n",
    "        args.batch_size % (num_gpus * args.batch_size_per_gpu) == 0\n",
    "    ), 'Batch size must be divisible by the number of GPUs'\n",
    "    gradient_accumulation_steps = args.batch_size // (num_gpus * args.batch_size_per_gpu)\n",
    "\n",
    "    if args.use_flash_attention:\n",
    "        fp16 = False\n",
    "        bf16 = True\n",
    "    else:\n",
    "        fp16 = True\n",
    "        bf16 = False\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        per_device_train_batch_size=args.batch_size_per_gpu,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        optim='adamw_torch',\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.95,\n",
    "        adam_epsilon=1e-7,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.wd,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type='linear',\n",
    "        warmup_steps=50,\n",
    "        logging_steps=10,\n",
    "        output_dir=args.output_dir,\n",
    "        save_strategy='no',\n",
    "        save_total_limit=10,\n",
    "        save_only_model=True,\n",
    "        bf16=bf16,\n",
    "        fp16=fp16,\n",
    "        remove_unused_columns=False,\n",
    "        report_to='none',\n",
    "        deepspeed=None,\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        dataloader_num_workers=4,\n",
    "        ddp_find_unused_parameters=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    # eval before fine-tuning\n",
    "    out_path = Path(training_args.output_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Clear GPU memory before evaluation\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        save_path=out_path / 'eval_before.json',\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        eval_batch_size=args.batch_size_per_gpu,\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        print(f'BLEU Score before finetuning: {score}')\n",
    "    \n",
    "    # clear GPU memory before training\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=covost_collate_fn,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    if accelerator.is_main_process:\n",
    "        processor.save_pretrained(training_args.output_dir)\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    # eval after fine-tuning (load saved checkpoint)\n",
    "    # first try to clear GPU memory\n",
    "    del model\n",
    "    del trainer\n",
    "    __import__('gc').collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # reload the model for inference\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        training_args.output_dir,\n",
    "        torch_dtype=torch.bfloat16 if args.use_flash_attention else torch.float32,\n",
    "        trust_remote_code=True,\n",
    "        _attn_implementation='flash_attention_2' if args.use_flash_attention else 'sdpa',\n",
    "    ).to('cuda')\n",
    "\n",
    "    # Clear GPU memory before evaluation\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Disable gradient checkpointing during evaluation\n",
    "    model.gradient_checkpointing_disable()\n",
    "\n",
    "    # Evaluate before fine-tuning\n",
    "    score = evaluate(\n",
    "        model,\n",
    "        processor,\n",
    "        eval_dataset,\n",
    "        save_path=out_path / 'eval_before.json',\n",
    "        disable_tqdm=not args.tqdm,\n",
    "        eval_batch_size=1,  # Reduce evaluation batch size\n",
    "    )\n",
    "    print(f'BLEU Score before finetuning: {score}')\n",
    "\n",
    "    # Re-enable gradient checkpointing for training\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Clear GPU memory after evaluation\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        print(f'BLEU Score after finetuning: {score}')\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Skip evaluation and proceed to fine-tuning\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=covost_collate_fn,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    if accelerator.is_main_process:\n",
    "        processor.save_pretrained(training_args.output_dir)\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00bc72c",
   "metadata": {},
   "source": [
    "## Data Zip/Unzip (Custom Naija Voice Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the zip file\n",
    "import zipfile\n",
    "import os\n",
    "def extract_zip(zip_file_path, extract_to='.'):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "# Example usage\n",
    "zip_file_path = '/home/kelechi/bio_ramp_project/afrivox-20250424T174821Z-002.zip'\n",
    "extract_zip(zip_file_path, extract_to='CommonVoice/EN')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe86fbc",
   "metadata": {},
   "source": [
    "## Tran/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv datasets and split into train, test, and validation sets\n",
    "import pandas as pd\n",
    "full_dataset = pd.read_csv('/home/kelechi/bio_ramp_project/CommonVoice/EN/afrivox/emnlp_nv_dataset.csv')\n",
    "#rename speaker_id to client_id\n",
    "full_dataset.rename(columns={'speaker_id': 'client_id'}, inplace=True)\n",
    "full_dataset.rename(columns={'audio_path': 'path'}, inplace=True)\n",
    "full_dataset.rename(columns={'transcription': 'sentence'}, inplace=True)\n",
    "\n",
    "\n",
    "train_dataset = full_dataset.sample(frac=0.8, random_state=42)\n",
    "test_dataset = full_dataset.drop(train_dataset.index)\n",
    "val_dataset = test_dataset.sample(frac=0.5, random_state=42)\n",
    "test_dataset = test_dataset.drop(val_dataset.index)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset.to_csv('/home/kelechi/bio_ramp_project/CommonVoice/EN/afrivox/train.csv', index=False)\n",
    "val_dataset.to_csv('/home/kelechi/bio_ramp_project/CommonVoice/EN/afrivox/validated.csv', index=False)\n",
    "test_dataset.to_csv('/home/kelechi/bio_ramp_project/CommonVoice/EN/afrivox/test.csv', index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_ramp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
